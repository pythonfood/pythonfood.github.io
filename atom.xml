<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PythonFood</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://pythonfood.github.io/"/>
  <updated>2018-08-16T09:38:18.415Z</updated>
  <id>http://pythonfood.github.io/</id>
  
  <author>
    <name>Python Food</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy分布式的部署详解</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E9%83%A8%E7%BD%B2%E8%AF%A6%E8%A7%A3/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy分布式的部署详解/</id>
    <published>2018-07-05T13:00:00.000Z</published>
    <updated>2018-08-16T09:38:18.415Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、部署详解"><a href="#一、部署详解" class="headerlink" title="一、部署详解"></a>一、部署详解</h2><h3 id="1、scrapyd组件"><a href="#1、scrapyd组件" class="headerlink" title="1、scrapyd组件"></a>1、scrapyd组件</h3><p>github地址：<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd</a></p><p>官方文档：<a href="http://scrapyd.readthedocs.io/en/stable/" target="_blank" rel="noopener">http://scrapyd.readthedocs.io/en/stable/</a></p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p><code>pip install scrapyd</code></p><p>如果出现和python3不兼容的问题，打开python包管理的网站：<a href="https://pypi.org/" target="_blank" rel="noopener">https://pypi.org/</a> ，搜索scrapyd，找到一个兼容的版本安装。</p><h3 id="3、启动"><a href="#3、启动" class="headerlink" title="3、启动"></a>3、启动</h3><p><code>scrapyd</code></p><p>会回显它是在<a href="http://127.0.0.1:6800/" target="_blank" rel="noopener">http://127.0.0.1:6800/</a> 上监听的。linux上是<a href="http://0.0.0.0:6800/" target="_blank" rel="noopener">http://0.0.0.0:6800/</a></p><h3 id="4、怎样部署项目？"><a href="#4、怎样部署项目？" class="headerlink" title="4、怎样部署项目？"></a>4、怎样部署项目？</h3><p>借助scrapyd-client：</p><p>gitbub地址：<a href="https://github.com/scrapy/scrapyd-client" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd-client</a></p><p>安装<code>pip install scrapyd-client</code></p><p>使用详见官方文档。</p><h3 id="5、修改项目"><a href="#5、修改项目" class="headerlink" title="5、修改项目"></a>5、修改项目</h3><p>修改scrapy.cfg文件[deploy],添加远程主机url = <a href="http://123.206.65.37:6800/addversion.json" target="_blank" rel="noopener">http://123.206.65.37:6800/addversion.json</a> 。</p><p>输入命令<code>scrapyd-deploy</code>帮助我们完成项目的部署。远程部署到主机上了。</p><p>输入命令<code>curl http://123.206.65.37:6800/listprojects.json</code>可以看到已经上传成功了。</p><p>输入命令<code>curl http://123.206.65.37:6800/schedule.json -d project=zhihuuser -d spider=zhihu</code>指定项目和spider远程开启调度进程。</p><p>连续输入三个<code>curl http://123.206.65.37:6800/schedule.json -d project=zhihuuser -d spider=zhihu</code>则开启三个调度进程。</p><p>输入命令<code>curl http://123.206.65.37:6800/listjobs.json\?project\=zhihuuser</code>可以查看当前运行的任务。也可以在浏览器里查看。</p><p>输入命令<code>curl http://123.206.65.37:6800/cancel.json -d project=zhihuuser -d job=35a3bce034hkk2j4kl232l43hjkl</code>指定job代号停止任务。</p><h3 id="6、python-scrapyd-api库"><a href="#6、python-scrapyd-api库" class="headerlink" title="6、python-scrapyd-api库"></a>6、python-scrapyd-api库</h3><p>是对scrapyd相关api做了一个封装。这样可以在python代码中调用scrapyd的api了,就不必用curl接口请求的方式调度了。</p><p>gitbub地址：<a href="https://github.com/djm/python-scrapyd-api" target="_blank" rel="noopener">https://github.com/djm/python-scrapyd-api</a></p><p>安装<code>pip install python-scrapyd-api</code></p><p>命令行里测试下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from scrapyd_api import ScrapydAPI</span><br><span class="line"></span><br><span class="line">scrapyd = ScrapydAPI(&apos;http://123.206.65.37:6800&apos;)</span><br><span class="line">scrapyd.list_projects()</span><br><span class="line">scrapyd.list_spiders(zhihuuser)</span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy分布式架构搭建抓取知乎</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%90%AD%E5%BB%BA%E6%8A%93%E5%8F%96%E7%9F%A5%E4%B9%8E/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy分布式架构搭建抓取知乎/</id>
    <published>2018-07-05T12:00:00.000Z</published>
    <updated>2018-08-21T09:39:45.628Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、在zhihuuser的项目上修改"><a href="#1、在zhihuuser的项目上修改" class="headerlink" title="1、在zhihuuser的项目上修改"></a>1、在zhihuuser的项目上修改</h3><p>首先从github上复制知乎项目到本地<code>git clone https://github.com/Germey/Zhihu.git</code>。</p><p>再在pycharm中打开。</p><p>不能在源代码上修改，需要建一个分支<code>git checkout -b distributed</code>。</p><p>切换到分支<code>git branch</code>。</p><h3 id="2、搭建分布式架构"><a href="#2、搭建分布式架构" class="headerlink" title="2、搭建分布式架构"></a>2、搭建分布式架构</h3><p>本地项目修改settings.py，引入scrapy-redis，这里redis数据库地址设置的一个阿里云服务器。</p><p>zhihu-distributed / scrapy.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = zhihuuser.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line">#url = http://localhost:6800/</span><br><span class="line">url = http://123.206.65.37:6800/addversion.json # 远程主机的url</span><br><span class="line">project = zhihuuser</span><br></pre></td></tr></table></figure></p><p>zhihu-distributed / zhihuuser / items.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    id = Field()</span><br><span class="line">    name = Field()</span><br><span class="line">    avatar_url = Field()</span><br><span class="line">    headline = Field()</span><br><span class="line">    description = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    url_token = Field()</span><br><span class="line">    gender = Field()</span><br><span class="line">    cover_url = Field()</span><br><span class="line">    type = Field()</span><br><span class="line">    badge = Field()</span><br><span class="line"></span><br><span class="line">    answer_count = Field()</span><br><span class="line">    articles_count = Field()</span><br><span class="line">    commercial_question_count = Field()</span><br><span class="line">    favorite_count = Field()</span><br><span class="line">    favorited_count = Field()</span><br><span class="line">    follower_count = Field()</span><br><span class="line">    following_columns_count = Field()</span><br><span class="line">    following_count = Field()</span><br><span class="line">    pins_count = Field()</span><br><span class="line">    question_count = Field()</span><br><span class="line">    thank_from_count = Field()</span><br><span class="line">    thank_to_count = Field()</span><br><span class="line">    thanked_count = Field()</span><br><span class="line">    vote_from_count = Field()</span><br><span class="line">    vote_to_count = Field()</span><br><span class="line">    voteup_count = Field()</span><br><span class="line">    following_favlists_count = Field()</span><br><span class="line">    following_question_count = Field()</span><br><span class="line">    following_topic_count = Field()</span><br><span class="line">    marked_answers_count = Field()</span><br><span class="line">    mutual_followees_count = Field()</span><br><span class="line">    hosted_live_count = Field()</span><br><span class="line">    participated_live_count = Field()</span><br><span class="line"></span><br><span class="line">    locations = Field()</span><br><span class="line">    educations = Field()</span><br><span class="line">    employments = Field()</span><br></pre></td></tr></table></figure></p><p>zhihu-distributed / zhihuuser / middlewares.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpiderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_input</span><span class="params">(response, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">        <span class="comment"># middleware and into the spider.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return None or raise an exception.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_output</span><span class="params">(response, result, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">        <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_exception</span><span class="params">(response, exception, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">        <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return either None or an iterable of Response, dict</span></span><br><span class="line">        <span class="comment"># or Item objects.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_start_requests</span><span class="params">(start_requests, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">        <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">        <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        spider.logger.info(<span class="string">'Spider opened: %s'</span> % spider.name)</span><br></pre></td></tr></table></figure></p><p>zhihu-distributed / zhihuuser / pipelines.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    collection_name = <span class="string">'users'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].update(&#123;<span class="string">'url_token'</span>: item[<span class="string">'url_token'</span>]&#125;, dict(item), <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>zhihu-distributed / zhihuuser / settings.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'zhihuuser'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'zhihuuser.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'zhihuuser.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'authorization'</span>: <span class="string">'oauth c3cef7c66a1843f8b3a9e6a1e3160e20'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'zhihuuser.pipelines.MongoPipeline'</span>: <span class="number">300</span>,  <span class="comment"># 存取本地pipeline</span></span><br><span class="line">    <span class="string">'scrapy_redis.pipelines.RedisPipeline'</span>: <span class="number">301</span>  <span class="comment"># 将爬取结果引入到redis的pipeline里面去</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DATABASE = <span class="string">'zhihu'</span></span><br><span class="line"></span><br><span class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span>  <span class="comment"># 核心调度器换成scrapy_redis调度器</span></span><br><span class="line"></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span>  <span class="comment"># 去重的class</span></span><br><span class="line"></span><br><span class="line">REDIS_URL = <span class="string">'redis://root:redistest@120.27.34.24:6379'</span>  <span class="comment"># redis数据库的连接地址</span></span><br></pre></td></tr></table></figure></p><p>zhihu-distributed / zhihuuser / spiders / zhihu.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> zhihuuser.items <span class="keyword">import</span> UserItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"zhihu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</span><br><span class="line">    user_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;'</span></span><br><span class="line">    follows_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    followers_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    start_user = <span class="string">'tianshansoft'</span></span><br><span class="line">    user_query = <span class="string">'locations,employments,gender,educations,business,voteup_count,thanked_Count,follower_count,following_count,cover_url,following_topic_count,following_question_count,following_favlists_count,following_columns_count,answer_count,articles_count,pins_count,question_count,commercial_question_count,favorite_count,favorited_count,logs_count,marked_answers_count,marked_answers_text,message_thread_token,account_status,is_active,is_force_renamed,is_bind_sina,sina_weibo_url,sina_weibo_name,show_sina_weibo,is_blocking,is_blocked,is_following,is_followed,mutual_followees_count,vote_to_count,vote_from_count,thank_to_count,thank_from_count,thanked_count,description,hosted_live_count,participated_live_count,allow_message,industry_category,org_name,org_homepage,badge[?(type=best_answerer)].topics'</span></span><br><span class="line">    follows_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line">    followers_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user)</span><br><span class="line">        <span class="keyword">yield</span> Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">                      self.parse_follows)</span><br><span class="line">        <span class="keyword">yield</span> Request(self.followers_url.format(user=self.start_user, include=self.followers_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">                      self.parse_followers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        item = UserItem()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> item.fields:</span><br><span class="line">            <span class="keyword">if</span> field <span class="keyword">in</span> result.keys():</span><br><span class="line">                item[field] = result.get(field)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            self.follows_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.follows_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">            self.parse_follows)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            self.followers_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.followers_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">            self.parse_followers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_follows</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        results = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> results.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> results.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.user_query),</span><br><span class="line">                              self.parse_user)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> results.keys() <span class="keyword">and</span> results.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = results.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,</span><br><span class="line">                          self.parse_follows)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_followers</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        results = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> results.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> results.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.user_query),</span><br><span class="line">                              self.parse_user)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> results.keys() <span class="keyword">and</span> results.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = results.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,</span><br><span class="line">                          self.parse_followers)</span><br></pre></td></tr></table></figure></p><h3 id="3、更新代码到github上"><a href="#3、更新代码到github上" class="headerlink" title="3、更新代码到github上"></a>3、更新代码到github上</h3><p><code>git status</code></p><p><code>git add -A</code></p><p><code>git commit -m &#39;add distributed&#39;</code></p><p><code>git push origin distributed</code></p><h3 id="4、在腾讯云上部署scrapy环境"><a href="#4、在腾讯云上部署scrapy环境" class="headerlink" title="4、在腾讯云上部署scrapy环境"></a>4、在腾讯云上部署scrapy环境</h3><p>注意设置mongodb.conf文件，允许远程访问，注释掉bind_ip=127.0.0.1即可允许远程访问了。</p><p>从github上复制代码<code>git clone https://github.com/Germey/Zhihu.git -b distributed</code>。</p><p>运行爬虫<code>scrapy crawl zhihu</code>，相当于在另一台主机上开启了知乎的爬取。<br>scrapy_resis/unils.py  # 一些工具库</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy分布式原理及Scrapy-Redis源码解析</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86%E5%8F%8AScrapy-Redis%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy分布式原理及Scrapy-Redis源码解析/</id>
    <published>2018-07-05T11:00:00.000Z</published>
    <updated>2018-08-16T09:51:36.279Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、原理详解"><a href="#一、原理详解" class="headerlink" title="一、原理详解"></a>一、原理详解</h2><h3 id="1、Scrapy单机架构"><a href="#1、Scrapy单机架构" class="headerlink" title="1、Scrapy单机架构"></a>1、Scrapy单机架构</h3><p><img src="/2018/07/05/Scrapy分布式原理及Scrapy-Redis源码解析/Scrapy单机架构.png" alt="Scrapy单机架构"></p><h3 id="2、-单主机爬虫架构"><a href="#2、-单主机爬虫架构" class="headerlink" title="2、 单主机爬虫架构"></a>2、 单主机爬虫架构</h3><p><img src="/2018/07/05/Scrapy分布式原理及Scrapy-Redis源码解析/单主机爬虫架构.png" alt="单主机爬虫架构"></p><h3 id="3、分布式爬虫架构"><a href="#3、分布式爬虫架构" class="headerlink" title="3、分布式爬虫架构"></a>3、分布式爬虫架构</h3><p><img src="/2018/07/05/Scrapy分布式原理及Scrapy-Redis源码解析/分布式爬虫架构1.png" alt="分布式爬虫架构1"></p><p><img src="/2018/07/05/Scrapy分布式原理及Scrapy-Redis源码解析/分布式爬虫架构2.png" alt="分布式爬虫架构2"></p><h3 id="4、队列用什么维护？"><a href="#4、队列用什么维护？" class="headerlink" title="4、队列用什么维护？"></a>4、队列用什么维护？</h3><p>Redis队列：</p><p>Redis，非关系型数据库，Key-Value形式存储，结构灵活。</p><p>是内存中的数据结构存储系统，处理速度快，性能好。</p><p>提供队列、集合等多种存储结构，⽅方便队列维护。</p><h3 id="5、怎样来去重？"><a href="#5、怎样来去重？" class="headerlink" title="5、怎样来去重？"></a>5、怎样来去重？</h3><p>Redis集合：</p><p>Redis提供集合数据结构，在Redis集合中存储每个Request的指纹。</p><p>在向Request队列中加入Request前首先验证这个Request的指纹是否已经加入集合中。</p><p>如果已存在，则不添加Request到队列，如果不存在，则将Request添加入队列并将指纹加入集合。</p><h3 id="6、怎样防⽌止中断？"><a href="#6、怎样防⽌止中断？" class="headerlink" title="6、怎样防⽌止中断？"></a>6、怎样防⽌止中断？</h3><p>启动判断：</p><p>在每台从机Scrapy启动时都会首先判断当前Redis Request队列是否为空。</p><p>如果不为空，则从队列中取得下一个Request执行爬取。</p><p>如果为空，则重新开始爬取，第一台从机执行爬取向队列中添加Request。</p><h3 id="7、怎样实现该架构？"><a href="#7、怎样实现该架构？" class="headerlink" title="7、怎样实现该架构？"></a>7、怎样实现该架构？</h3><p>Scrapy-Redis：</p><p>Scrapy-Redis库实现了如上架构，改写了Scrapy的调度器，队列等组件。</p><p>利用它可以方便地实现Scrapy分布式架构。</p><p>github地址：<a href="https://github.com/rolando/scrapy-redis" target="_blank" rel="noopener">https://github.com/rolando/scrapy-redis</a></p><p>安装<code>pip install scrapy-redis</code></p><h3 id="8、源码讲解"><a href="#8、源码讲解" class="headerlink" title="8、源码讲解"></a>8、源码讲解</h3><p>scrapy_resis/connection.py  # 连接redis的基本的库</p><p>scrapy_resis/default.py  # 一些默认的变量</p><p>scrapy_resis/dupefilter.py  # 用来去重的一个机制</p><p>scrapy_resis/picklecompat.py  # 和json的load和dump类似</p><p>scrapy_resis/pipelines.py  # 管道，增加了集中存储到resids</p><p>scrapy_resis/queue.py  # 队列</p><p>scrapy_resis/scheduler.py  # 调度器</p><p>scrapy_resis/spiders.py  # 定义了某些spider</p><p>scrapy_resis/unils.py  # 一些工具库</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy+Tushare爬取微博股票数据</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy-Tushare%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E8%82%A1%E7%A5%A8%E6%95%B0%E6%8D%AE/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy-Tushare爬取微博股票数据/</id>
    <published>2018-07-05T10:00:00.000Z</published>
    <updated>2018-08-21T09:45:17.911Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、安装"><a href="#1、安装" class="headerlink" title="1、安装"></a>1、安装</h3><p><code>pip install tushare</code></p><h3 id="2、官网地址"><a href="#2、官网地址" class="headerlink" title="2、官网地址"></a>2、官网地址</h3><p><a href="http://tushare.org/" target="_blank" rel="noopener">http://tushare.org/</a></p><h3 id="3、命令行演示"><a href="#3、命令行演示" class="headerlink" title="3、命令行演示"></a>3、命令行演示</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tushare as ts</span><br><span class="line"></span><br><span class="line">result = ts.get_hs300s()  # 获取沪深300</span><br><span class="line">print(result)</span><br><span class="line">print(type(result))</span><br><span class="line">result[&apos;name&apos;].tolist()  # 获取沪深300股票名称</span><br><span class="line">result[&apos;code&apos;].tolist()  # 获取沪深300股票代号</span><br></pre></td></tr></table></figure><h3 id="4、实战"><a href="#4、实战" class="headerlink" title="4、实战"></a>4、实战</h3><p>weibostock / scrapy.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = weibo.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line">#url = http://localhost:6800/</span><br><span class="line">project = weibo</span><br></pre></td></tr></table></figure></p><p>weibostock / weibo / items.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    table_name = <span class="string">'weibo'</span></span><br><span class="line"></span><br><span class="line">    id = Field()</span><br><span class="line">    content = Field()</span><br><span class="line">    forward_count = Field()</span><br><span class="line">    comment_count = Field()</span><br><span class="line">    like_count = Field()</span><br><span class="line">    posted_at = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    user = Field()</span><br><span class="line">    keyword = Field()  <span class="comment"># 保存到item，就可以进行赋值了</span></span><br><span class="line">    crawled_at = Field()</span><br></pre></td></tr></table></figure></p><p>weibostock / weibo / middlewares.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ConnectionError</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> IgnoreRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookiesMiddleWare</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cookies_pool_url)</span>:</span></span><br><span class="line">        self.logger = logging.getLogger(__name__)</span><br><span class="line">        self.cookies_pool_url = cookies_pool_url</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_random_cookies</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(self.cookies_pool_url)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="keyword">return</span> json.loads(response.text)</span><br><span class="line">        <span class="keyword">except</span> ConnectionError:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        cookies = self._get_random_cookies()</span><br><span class="line">        <span class="keyword">if</span> cookies:</span><br><span class="line">            request.cookies = cookies</span><br><span class="line">            self.logger.debug(<span class="string">'Using Cookies '</span> + json.dumps(cookies))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.logger.debug(<span class="string">'No Valid Cookies'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            cookies_pool_url=crawler.settings.get(<span class="string">'COOKIES_POOL_URL'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> response.status <span class="keyword">in</span> [<span class="number">300</span>, <span class="number">301</span>, <span class="number">302</span>, <span class="number">303</span>]:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                redirect_url = response.headers[<span class="string">'location'</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'login.weibo'</span> <span class="keyword">in</span> redirect_url <span class="keyword">or</span> <span class="string">'login.sina'</span> <span class="keyword">in</span> redirect_url:  <span class="comment"># Cookie失效</span></span><br><span class="line">                    self.logger.warning(<span class="string">'Updating Cookies'</span>)</span><br><span class="line">                <span class="keyword">elif</span> <span class="string">'weibo.cn/security'</span> <span class="keyword">in</span> redirect_url:</span><br><span class="line">                    self.logger.warning(<span class="string">'Now Cookies'</span> + json.dumps(request.cookies))</span><br><span class="line">                    self.logger.warning(<span class="string">'One Account is locked!'</span>)</span><br><span class="line">                request.cookies = self._get_random_cookies()</span><br><span class="line">                self.logger.debug(<span class="string">'Using Cookies'</span> + json.dumps(request.cookies))</span><br><span class="line">                <span class="keyword">return</span> request</span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">raise</span> IgnoreRequest</span><br><span class="line">        <span class="keyword">elif</span> response.status <span class="keyword">in</span> [<span class="number">414</span>]:</span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure></p><p>weibostock / weibo / pipelines.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> weibo.items <span class="keyword">import</span> WeiboItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_time</span><span class="params">(self, datetime)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'\d+月\d+日'</span>, datetime):</span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年'</span>, time.localtime()) + datetime</span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'\d+分钟前'</span>, datetime):</span><br><span class="line">            minute = re.match(<span class="string">'(\d+)'</span>, datetime).group(<span class="number">1</span>)</span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年%m月%d日 %H:%M'</span>, time.localtime(time.time() - float(minute) * <span class="number">60</span>))</span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'今天.*'</span>, datetime):</span><br><span class="line">            datetime = re.match(<span class="string">'今天(.*)'</span>, datetime).group(<span class="number">1</span>).strip()</span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年%m月%d日'</span>, time.localtime()) + <span class="string">' '</span> + datetime</span><br><span class="line">        <span class="keyword">return</span> datetime</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, WeiboItem):</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'content'</span>):</span><br><span class="line">                item[<span class="string">'content'</span>] = item[<span class="string">'content'</span>].lstrip(<span class="string">':'</span>).strip()</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'posted_at'</span>):</span><br><span class="line">                item[<span class="string">'posted_at'</span>] = item[<span class="string">'posted_at'</span>].strip()</span><br><span class="line">                item[<span class="string">'posted_at'</span>] = self.parse_time(item.get(<span class="string">'posted_at'</span>))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[item.table_name].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;, &#123;<span class="string">'$set'</span>: dict(item)&#125;, <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>weibostock / weibo / settings.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'weibo'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'weibo.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'weibo.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'weibo.middlewares.CookiesMiddleWare'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'weibo.pipelines.WeiboPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'weibo.pipelines.MongoPipeline'</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">COOKIES_POOL_URL = <span class="string">'http://localhost:5000/weibo/random'</span></span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DATABASE = <span class="string">'weibo'</span></span><br></pre></td></tr></table></figure></p><p>weibostock / weibo / spiders / search.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request, FormRequest</span><br><span class="line"><span class="keyword">from</span> weibo.items <span class="keyword">import</span> WeiboItem</span><br><span class="line"><span class="keyword">import</span> tushare <span class="keyword">as</span> ts</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"search"</span></span><br><span class="line">    allowed_domains = [<span class="string">"weibo.cn"</span>]</span><br><span class="line">    search_url = <span class="string">'http://weibo.cn/search/mblog'</span></span><br><span class="line">    max_page = <span class="number">200</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        result = ts.get_sz50s()  <span class="comment"># 深圳50股票</span></span><br><span class="line">        <span class="comment"># result = ts.get_zz500s()</span></span><br><span class="line">        <span class="comment"># result = ts.get_hs300s()</span></span><br><span class="line">        self.keywords = result[<span class="string">'code'</span>].tolist()  <span class="comment"># 股票代号生成列表</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.keywords:</span><br><span class="line">            url = <span class="string">'&#123;url&#125;?keyword=&#123;keyword&#125;'</span>.format(url=self.search_url, keyword=keyword)</span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> range(self.max_page + <span class="number">1</span>):</span><br><span class="line">                data = &#123;</span><br><span class="line">                    <span class="string">'mp'</span>: str(self.max_page),</span><br><span class="line">                    <span class="string">'page'</span>: str(page)</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">yield</span> FormRequest(url, callback=self.parse_index, formdata=data, meta=&#123;<span class="string">'keyword'</span>: keyword&#125;)  <span class="comment"># meta参数传递数据保存item</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        weibos = response.xpath(<span class="string">'//div[@class="c" and contains(@id, "M_")]'</span>)</span><br><span class="line">        <span class="keyword">for</span> weibo <span class="keyword">in</span> weibos:</span><br><span class="line">            is_forward = bool(weibo.xpath(<span class="string">'.//span[@class="cmt"]'</span>).extract_first())</span><br><span class="line">            <span class="keyword">if</span> is_forward:</span><br><span class="line">                detail_url = weibo.xpath(<span class="string">'.//a[contains(., "原文评论[")]//@href'</span>).extract_first()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                detail_url = weibo.xpath(<span class="string">'(.//a[contains(., "评论[")]/@href)'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> Request(detail_url, callback=self.parse_detail, meta=&#123;<span class="string">'keyword'</span>: response.meta.get(<span class="string">'keyword'</span>)&#125;)  <span class="comment"># meta参数传递数据保存item</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        url = response.url</span><br><span class="line">        content = <span class="string">''</span>.join(response.xpath(<span class="string">'//div[@id="M_"]//span[@class="ctt"]//text()'</span>).extract())</span><br><span class="line">        id = re.search(<span class="string">'comment\/(.*?)\?'</span>, response.url).group(<span class="number">1</span>)</span><br><span class="line">        comment_count = response.xpath(<span class="string">'//span[@class="pms"]//text()'</span>).re_first(<span class="string">'评论\[(.*?)\]'</span>)</span><br><span class="line">        forward_count = response.xpath(<span class="string">'//a[contains(., "转发[")]//text()'</span>).re_first(<span class="string">'转发\[(.*?)\]'</span>)</span><br><span class="line">        like_count = response.xpath(<span class="string">'//a[contains(., "赞[")]//text()'</span>).re_first(<span class="string">'赞\[(.*?)\]'</span>)</span><br><span class="line">        posted_at = response.xpath(<span class="string">'//div[@id="M_"]//span[@class="ct"]//text()'</span>).extract_first(default=<span class="keyword">None</span>)</span><br><span class="line">        user = response.xpath(<span class="string">'//div[@id="M_"]/div[1]/a/text()'</span>).extract_first()</span><br><span class="line">        keyword = response.meta.get(<span class="string">'keyword'</span>)  <span class="comment"># 接收的时候用response.meta.get()获得关键字</span></span><br><span class="line">        weibo_item = WeiboItem()</span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> weibo_item.fields:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                weibo_item[field] = eval(field)</span><br><span class="line">            <span class="keyword">except</span> NameError:</span><br><span class="line">                print(<span class="string">'Field is Not Defined'</span>, field)</span><br><span class="line">        <span class="keyword">yield</span> weibo_item</span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy+Cookies池抓取新浪微博</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy-Cookies%E6%B1%A0%E6%8A%93%E5%8F%96%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy-Cookies池抓取新浪微博/</id>
    <published>2018-07-05T09:00:00.000Z</published>
    <updated>2018-08-21T09:44:48.051Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、抓取移动端微博"><a href="#1、抓取移动端微博" class="headerlink" title="1、抓取移动端微博"></a>1、抓取移动端微博</h3><p><a href="http://weibo.cn/search/mblog" target="_blank" rel="noopener">http://weibo.cn/search/mblog</a></p><h3 id="2、启动cookies池"><a href="#2、启动cookies池" class="headerlink" title="2、启动cookies池"></a>2、启动cookies池</h3><p>(1)启动redis服务。</p><p>(2)cmd输入命令<code>python import.py</code> ,再输入微博账号密码，导入数据库。</p><p>(3)cmd输入命令<code>python run.py</code>,启动cookies池服务，地址localhost:5000</p><h3 id="3、实战"><a href="#3、实战" class="headerlink" title="3、实战"></a>3、实战</h3><p>weibosearch / scrapy.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = weibo.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line">#url = http://localhost:6800/</span><br><span class="line">project = weibo</span><br></pre></td></tr></table></figure></p><p>weibosearch / weibo / items.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    table_name = <span class="string">'weibo'</span>  <span class="comment"># 数据表名称</span></span><br><span class="line"></span><br><span class="line">    id = Field()</span><br><span class="line">    content = Field()</span><br><span class="line">    forward_count = Field()</span><br><span class="line">    comment_count = Field()</span><br><span class="line">    like_count = Field()</span><br><span class="line">    posted_at = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    user = Field()</span><br><span class="line">    crawled_at = Field()  <span class="comment"># 爬取时间</span></span><br></pre></td></tr></table></figure></p><p>weibosearch / weibo / middlewares.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ConnectionError</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> IgnoreRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CookiesMiddleWare</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cookies_pool_url)</span>:</span></span><br><span class="line">        self.logger = logging.getLogger(__name__)</span><br><span class="line">        self.cookies_pool_url = cookies_pool_url</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_random_cookies</span><span class="params">(self)</span>:</span>  <span class="comment"># 从cookie池读取cookie</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            response = requests.get(self.cookies_pool_url)</span><br><span class="line">            <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">                <span class="keyword">return</span> json.loads(response.text)</span><br><span class="line">        <span class="keyword">except</span> ConnectionError:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span>  <span class="comment"># 请求request设置cookies</span></span><br><span class="line">        cookies = self._get_random_cookies()</span><br><span class="line">        <span class="keyword">if</span> cookies:</span><br><span class="line">            request.cookies = cookies</span><br><span class="line">            self.logger.debug(<span class="string">'Using Cookies '</span> + json.dumps(cookies))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.logger.debug(<span class="string">'No Valid Cookies'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span>  <span class="comment"># 获取settings配置cookie池信息</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            cookies_pool_url=crawler.settings.get(<span class="string">'COOKIES_POOL_URL'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span>  <span class="comment"># 返回response,状态码为300重新获取cookies</span></span><br><span class="line">        <span class="keyword">if</span> response.status <span class="keyword">in</span> [<span class="number">300</span>, <span class="number">301</span>, <span class="number">302</span>, <span class="number">303</span>]:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                redirect_url = response.headers[<span class="string">'location'</span>]  <span class="comment"># 获得重定向url</span></span><br><span class="line">                <span class="keyword">if</span> <span class="string">'login.weibo'</span> <span class="keyword">in</span> redirect_url <span class="keyword">or</span> <span class="string">'login.sina'</span> <span class="keyword">in</span> redirect_url:  <span class="comment"># 如果登录字段在重定向url,则Cookie失效</span></span><br><span class="line">                    self.logger.warning(<span class="string">'Updating Cookies'</span>)</span><br><span class="line">                <span class="keyword">elif</span> <span class="string">'weibo.cn/security'</span> <span class="keyword">in</span> redirect_url:  <span class="comment"># 如果安全提示字段在重定向url，账号被锁</span></span><br><span class="line">                    self.logger.warning(<span class="string">'Now Cookies'</span> + json.dumps(request.cookies))</span><br><span class="line">                    self.logger.warning(<span class="string">'One Account is locked!'</span>)</span><br><span class="line">                request.cookies = self._get_random_cookies()  <span class="comment"># request需要重新获取cookies</span></span><br><span class="line">                self.logger.debug(<span class="string">'Using Cookies'</span> + json.dumps(request.cookies))</span><br><span class="line">                <span class="keyword">return</span> request  <span class="comment"># 重新返回request</span></span><br><span class="line">            <span class="keyword">except</span> Exception:</span><br><span class="line">                <span class="keyword">raise</span> IgnoreRequest</span><br><span class="line">        <span class="keyword">elif</span> response.status <span class="keyword">in</span> [<span class="number">414</span>]:  <span class="comment"># 状态码414返回request</span></span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> response  <span class="comment"># 正常情况返回response</span></span><br></pre></td></tr></table></figure></p><p>weibosearch / weibo / pipelines.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> weibo.items <span class="keyword">import</span> WeiboItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WeiboPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_time</span><span class="params">(self, datetime)</span>:</span>  <span class="comment"># 清洗发布时间显示，都转换为：x年x月x日 x时:x分</span></span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'\d+月\d+日'</span>, datetime):  <span class="comment"># x月x日 x时:x分</span></span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年'</span>, time.localtime()) + datetime</span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'\d+分钟前'</span>, datetime):  <span class="comment"># x分中前</span></span><br><span class="line">            minute = re.match(<span class="string">'(\d+)'</span>, datetime).group(<span class="number">1</span>)</span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年%m月%d日 %H:%M'</span>, time.localtime(time.time() - float(minute) * <span class="number">60</span>))</span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">'今天.*'</span>, datetime):  <span class="comment"># 今天 x时:x分</span></span><br><span class="line">            datetime = re.match(<span class="string">'今天(.*)'</span>, datetime).group(<span class="number">1</span>).strip()</span><br><span class="line">            datetime = time.strftime(<span class="string">'%Y年%m月%d日'</span>, time.localtime()) + <span class="string">' '</span> + datetime</span><br><span class="line">        <span class="keyword">return</span> datetime</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, WeiboItem):  <span class="comment"># 如果item是WeiboItem</span></span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'content'</span>):</span><br><span class="line">                item[<span class="string">'content'</span>] = item[<span class="string">'content'</span>].lstrip(<span class="string">':'</span>).strip()  <span class="comment"># 内容处理</span></span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'posted_at'</span>):</span><br><span class="line">                item[<span class="string">'posted_at'</span>] = item[<span class="string">'posted_at'</span>].strip()</span><br><span class="line">                item[<span class="string">'posted_at'</span>] = self.parse_time(item.get(<span class="string">'posted_at'</span>))  <span class="comment"># 时间处理</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span>  <span class="comment"># 获取settings配置信息</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span>  <span class="comment"># 开启spider时连接数据库</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span>  <span class="comment"># 关闭spider时关闭数据库</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span>  <span class="comment"># 主程序，往数据表跟新或插入数据，同时去重</span></span><br><span class="line">        self.db[item.table_name].update(&#123;<span class="string">'id'</span>: item.get(<span class="string">'id'</span>)&#125;, &#123;<span class="string">'$set'</span>: dict(item)&#125;, <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>weibosearch / weibo / settings.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'weibo'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'weibo.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'weibo.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'weibo.middlewares.CookiesMiddleWare'</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'weibo.pipelines.WeiboPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">'weibo.pipelines.MongoPipeline'</span>: <span class="number">301</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">COOKIES_POOL_URL = <span class="string">'http://localhost:5000/weibo/random'</span>  <span class="comment"># cookie池服务地址</span></span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DATABASE = <span class="string">'weibo'</span></span><br></pre></td></tr></table></figure></p><p>weibosearch / weibo / spiders / search.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request, FormRequest</span><br><span class="line"><span class="keyword">from</span> weibo.items <span class="keyword">import</span> WeiboItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"search"</span></span><br><span class="line">    allowed_domains = [<span class="string">"weibo.cn"</span>]</span><br><span class="line">    search_url = <span class="string">'http://weibo.cn/search/mblog'</span></span><br><span class="line">    max_page = <span class="number">200</span>  <span class="comment"># 最大请求页为200</span></span><br><span class="line">    keywords = [<span class="string">'000001'</span>]  <span class="comment"># 搜索关键字列表，示例是一个股票代码</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span>  <span class="comment"># 初始请求</span></span><br><span class="line">        <span class="keyword">for</span> keyword <span class="keyword">in</span> self.keywords:</span><br><span class="line">            url = <span class="string">'&#123;url&#125;?keyword=&#123;keyword&#125;'</span>.format(url=self.search_url, keyword=keyword)  <span class="comment"># 格式化请求的url</span></span><br><span class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> range(self.max_page + <span class="number">1</span>):</span><br><span class="line">                data = &#123;</span><br><span class="line">                    <span class="string">'mp'</span>: str(self.max_page),  <span class="comment"># 参数mp最大请求页</span></span><br><span class="line">                    <span class="string">'page'</span>: str(page)  <span class="comment"># 请求页</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">yield</span> FormRequest(url, callback=self.parse_index, formdata=data)  <span class="comment"># 生成器生成FormRequest,回调parse_index</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(self, response)</span>:</span>  <span class="comment"># 解析索引页</span></span><br><span class="line">        weibos = response.xpath(<span class="string">'//div[@class="c" and contains(@id, "M_")]'</span>)  <span class="comment"># 获得微博列表</span></span><br><span class="line">        print(len(weibos), weibos)</span><br><span class="line">        <span class="keyword">for</span> weibo <span class="keyword">in</span> weibos:  <span class="comment"># 循环微博列表</span></span><br><span class="line">            is_forward = bool(weibo.xpath(<span class="string">'.//span[@class="cmt"]'</span>).extract_first())  <span class="comment"># 是否转发</span></span><br><span class="line">            <span class="keyword">if</span> is_forward:</span><br><span class="line">                detail_url = weibo.xpath(<span class="string">'.//a[contains(., "原文评论[")]//@href'</span>).extract_first()  <span class="comment"># 如果转发的，获取原文评论</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                detail_url = weibo.xpath(<span class="string">'(.//a[contains(., "评论[")]/@href)'</span>).extract_first()  <span class="comment"># 如果不是转发的，获取评论</span></span><br><span class="line">            <span class="keyword">yield</span> Request(detail_url, callback=self.parse_detail)  <span class="comment"># 生成request，回调parse_detail</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span>  <span class="comment"># 解析详情页</span></span><br><span class="line">        url = response.url</span><br><span class="line">        content = <span class="string">''</span>.join(response.xpath(<span class="string">'//div[@id="M_"]//span[@class="ctt"]//text()'</span>).extract())  <span class="comment"># 微博正文需要拼接</span></span><br><span class="line">        id = re.search(<span class="string">'comment\/(.*?)\?'</span>, response.url).group(<span class="number">1</span>)</span><br><span class="line">        comment_count = response.xpath(<span class="string">'//span[@class="pms"]//text()'</span>).re_first(<span class="string">'评论\[(.*?)\]'</span>)</span><br><span class="line">        forward_count = response.xpath(<span class="string">'//a[contains(., "转发[")]//text()'</span>).re_first(<span class="string">'转发\[(.*?)\]'</span>)</span><br><span class="line">        like_count = response.xpath(<span class="string">'//a[contains(., "赞[")]//text()'</span>).re_first(<span class="string">'赞\[(.*?)\]'</span>)</span><br><span class="line">        posted_at = response.xpath(<span class="string">'//div[@id="M_"]//span[@class="ct"]//text()'</span>).extract_first(default=<span class="keyword">None</span>)</span><br><span class="line">        user = response.xpath(<span class="string">'//div[@id="M_"]/div[1]/a/text()'</span>).extract_first()</span><br><span class="line">        weibo_item = WeiboItem()  <span class="comment"># 实例化weiboitem</span></span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> weibo_item.fields:  <span class="comment"># 遍历weiboitem的字段</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                weibo_item[field] = eval(field)  <span class="comment"># eval可以动态的获取field进行赋值</span></span><br><span class="line">            <span class="keyword">except</span> NameError:</span><br><span class="line">                print(<span class="string">'Field is Not Defined'</span>, field)</span><br><span class="line">        <span class="keyword">yield</span> weibo_item</span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy爬取知乎用户信息实战</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%E7%94%A8%E6%88%B7%E4%BF%A1%E6%81%AF%E5%AE%9E%E6%88%98/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy爬取知乎用户信息实战/</id>
    <published>2018-07-05T08:00:00.000Z</published>
    <updated>2018-08-21T09:39:50.251Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、爬取知乎用户信息"><a href="#1、爬取知乎用户信息" class="headerlink" title="1、爬取知乎用户信息"></a>1、爬取知乎用户信息</h3><p>假设一个用户有三个粉丝，三个粉丝又各自有三个粉丝，以此类推，粉丝数越来越多。我们可以采取递归方式的抓取方式，只要是有粉丝有关注的用户都可以适用这种抓取方式。那些零关注零粉丝的⼤大虾就放过他们吧。</p><h3 id="2、思路"><a href="#2、思路" class="headerlink" title="2、思路"></a>2、思路</h3><p>(1)选定起始人</p><p>选定一位关注数或粉丝数多的大V作为爬取起始点。</p><p>(2)获取粉丝和关注列表</p><p>通过知乎接口获得该大V的粉丝列表和关注列表。</p><p>(3)获取列表用户信息</p><p>通过知乎接口获得列表中每位用户的详细信息。</p><p>(4)获取每位用户粉丝和关注</p><p>进一步对列表中的每一个用户，获取他们的粉丝和关注列表，实现递归爬取。</p><h3 id="3、实战"><a href="#3、实战" class="headerlink" title="3、实战"></a>3、实战</h3><p>以大V轮子哥为起始人：<a href="https://www.zhihu.com/people/excited-vczh/activities" target="_blank" rel="noopener">https://www.zhihu.com/people/excited-vczh/activities</a></p><p>从关注列表和粉丝列表两大递归方向获取每个用户的信息。</p><p>zhihuuser / scrapy.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = zhihuuser.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line">#url = http://localhost:6800/</span><br><span class="line">project = zhihuuser</span><br></pre></td></tr></table></figure></p><p>zhihuuser / zhihuuser / items.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item, Field</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserItem</span><span class="params">(Item)</span>:</span>  <span class="comment"># 定义保存数据的数据结构</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    id = Field()</span><br><span class="line">    name = Field()</span><br><span class="line">    avatar_url = Field()</span><br><span class="line">    headline = Field()</span><br><span class="line">    description = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    url_token = Field()</span><br><span class="line">    gender = Field()</span><br><span class="line">    cover_url = Field()</span><br><span class="line">    type = Field()</span><br><span class="line">    badge = Field()</span><br><span class="line"></span><br><span class="line">    answer_count = Field()</span><br><span class="line">    articles_count = Field()</span><br><span class="line">    commercial_question_count = Field()</span><br><span class="line">    favorite_count = Field()</span><br><span class="line">    favorited_count = Field()</span><br><span class="line">    follower_count = Field()</span><br><span class="line">    following_columns_count = Field()</span><br><span class="line">    following_count = Field()</span><br><span class="line">    pins_count = Field()</span><br><span class="line">    question_count = Field()</span><br><span class="line">    thank_from_count = Field()</span><br><span class="line">    thank_to_count = Field()</span><br><span class="line">    thanked_count = Field()</span><br><span class="line">    vote_from_count = Field()</span><br><span class="line">    vote_to_count = Field()</span><br><span class="line">    voteup_count = Field()</span><br><span class="line">    following_favlists_count = Field()</span><br><span class="line">    following_question_count = Field()</span><br><span class="line">    following_topic_count = Field()</span><br><span class="line">    marked_answers_count = Field()</span><br><span class="line">    mutual_followees_count = Field()</span><br><span class="line">    hosted_live_count = Field()</span><br><span class="line">    participated_live_count = Field()</span><br><span class="line"></span><br><span class="line">    locations = Field()</span><br><span class="line">    educations = Field()</span><br><span class="line">    employments = Field()</span><br></pre></td></tr></table></figure></p><p>zhihuuser / zhihuuser / middlewares.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpiderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_input</span><span class="params">(response, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">        <span class="comment"># middleware and into the spider.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return None or raise an exception.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_output</span><span class="params">(response, result, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">        <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_exception</span><span class="params">(response, exception, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">        <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return either None or an iterable of Response, dict</span></span><br><span class="line">        <span class="comment"># or Item objects.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_start_requests</span><span class="params">(start_requests, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">        <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">        <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        spider.logger.info(<span class="string">'Spider opened: %s'</span> % spider.name)</span><br></pre></td></tr></table></figure></p><p>zhihuuser / zhihuuser / pipelines.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span>  <span class="comment"># 保存数据到mongodb</span></span><br><span class="line">    collection_name = <span class="string">'users'</span>  <span class="comment"># 定义数据库集合名称</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span>  <span class="comment"># 获取settings中的MONGO_URI和MONGO_DATABASE</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span>  <span class="comment"># 启动spider时连接mongo_db</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span>  <span class="comment"># 关闭spider时关闭数据库</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span>  <span class="comment"># 项目管道主程序，update()有去重、插入、更新多重作用。</span></span><br><span class="line">        <span class="comment"># 第一个参数是查询条件，第二个传入的参数，第三个参数设置为True表示按照查询条件，如果查询到了进行更行，没查找到则进行插入</span></span><br><span class="line">        self.db[self.collection_name].update(&#123;<span class="string">'url_token'</span>: item[<span class="string">'url_token'</span>]&#125;, dict(item), <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p>zhihuuser / zhihuuser / settings.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'zhihuuser'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'zhihuuser.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'zhihuuser.spiders'</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">False</span>  <span class="comment"># 设置为False，表示不遵守robotstxt协议，防止部分内容不能爬取</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;  <span class="comment"># 知乎这两个headers字段必须设置，不然不能正常访问</span></span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>,</span><br><span class="line">    <span class="string">'authorization'</span>: <span class="string">'oauth c3cef7c66a1843f8b3a9e6a1e3160e20'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'zhihuuser.pipelines.MongoPipeline'</span>: <span class="number">300</span>,  <span class="comment"># 设置启动MongoPipeline</span></span><br><span class="line">    <span class="comment"># 'scrapy_redis.pipelines.RedisPipeline': 301</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span>  <span class="comment"># 设置MongoDB的host</span></span><br><span class="line">MONGO_DATABASE = <span class="string">'zhihu'</span>  <span class="comment"># 设置MongoDB的数据库名称</span></span><br></pre></td></tr></table></figure></p><p>zhihuuser / zhihuuser / spiders / zhihu.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Spider, Request</span><br><span class="line"><span class="keyword">from</span> zhihuuser.items <span class="keyword">import</span> UserItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"zhihu"</span></span><br><span class="line">    allowed_domains = [<span class="string">"www.zhihu.com"</span>]</span><br><span class="line">    user_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;'</span></span><br><span class="line">    follows_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    followers_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    start_user = <span class="string">'excited-vczh'</span></span><br><span class="line">    user_query = <span class="string">'locations,employments,gender,educations,business,voteup_count,thanked_Count,follower_count,following_count,cover_url,following_topic_count,following_question_count,following_favlists_count,following_columns_count,answer_count,articles_count,pins_count,question_count,commercial_question_count,favorite_count,favorited_count,logs_count,marked_answers_count,marked_answers_text,message_thread_token,account_status,is_active,is_force_renamed,is_bind_sina,sina_weibo_url,sina_weibo_name,show_sina_weibo,is_blocking,is_blocked,is_following,is_followed,mutual_followees_count,vote_to_count,vote_from_count,thank_to_count,thank_from_count,thanked_count,description,hosted_live_count,participated_live_count,allow_message,industry_category,org_name,org_homepage,badge[?(type=best_answerer)].topics'</span></span><br><span class="line">    follows_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line">    followers_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span>  <span class="comment"># 起始请求</span></span><br><span class="line">        <span class="comment"># 请求起始人，回调用户解析</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.user_url.format(user=self.start_user, include=self.user_query), self.parse_user)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 请求关注列表，回调关注列表解析</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.follows_url.format(user=self.start_user, include=self.follows_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">                      self.parse_follows)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 请求粉丝列表，回调粉丝列表解析</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.followers_url.format(user=self.start_user, include=self.followers_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">                      self.parse_followers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(self, response)</span>:</span>  <span class="comment"># 用户解析</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        item = UserItem()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> item.fields:  <span class="comment"># 返回结果生成item</span></span><br><span class="line">            <span class="keyword">if</span> field <span class="keyword">in</span> result.keys():</span><br><span class="line">                item[field] = result.get(field)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 请求关注列表，回调关注列表解析</span></span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            self.follows_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.follows_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">            self.parse_follows)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 请求粉丝列表，回调粉丝列表解析</span></span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            self.followers_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.followers_query, limit=<span class="number">20</span>, offset=<span class="number">0</span>),</span><br><span class="line">            self.parse_followers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_follows</span><span class="params">(self, response)</span>:</span>  <span class="comment"># 关注列表解析</span></span><br><span class="line">        results = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> results.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> results.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="comment"># 请求用户界面，回调用户解析</span></span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.user_query),</span><br><span class="line">                              self.parse_user)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> results.keys() <span class="keyword">and</span> results.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = results.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="comment"># 请求下一页，回调关注列表解析</span></span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,</span><br><span class="line">                          self.parse_follows)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_followers</span><span class="params">(self, response)</span>:</span>  <span class="comment"># 粉丝列表解析</span></span><br><span class="line">        results = json.loads(response.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> results.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> results.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="comment"># 请求用户界面，回调用户解析</span></span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.user_query),</span><br><span class="line">                              self.parse_user)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> results.keys() <span class="keyword">and</span> results.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = results.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="comment"># 请求下一页，回调粉丝列表解析</span></span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,</span><br><span class="line">                          self.parse_followers)</span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中Download Middleware的用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADDownload-Middleware%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Download-Middleware的用法/</id>
    <published>2018-07-05T07:00:00.000Z</published>
    <updated>2018-08-16T05:02:51.753Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、Downloader-Middleware中文文档0-25-0"><a href="#1、Downloader-Middleware中文文档0-25-0" class="headerlink" title="1、Downloader Middleware中文文档0.25.0"></a>1、Downloader Middleware中文文档0.25.0</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/downloader-middleware.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/downloader-middleware.html</a></p><h3 id="2、Downloader-Middleware下载器中间件"><a href="#2、Downloader-Middleware下载器中间件" class="headerlink" title="2、Downloader Middleware下载器中间件"></a>2、Downloader Middleware下载器中间件</h3><p>下载器中间件是介于Scrapy的request/response处理的钩子框架。 是用于全局修改Scrapy request和response的一个轻量、底层的系统。</p><h3 id="3、激活下载器中间件"><a href="#3、激活下载器中间件" class="headerlink" title="3、激活下载器中间件"></a>3、激活下载器中间件</h3><ul><li>settings中设置DOWNLOADER_MIDDLEWARES选项。 设置一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &apos;myproject.middlewares.CustomDownloaderMiddleware&apos;: 543,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>DOWNLOADER_MIDDLEWARES设置会与Scrapy定义的DOWNLOADER_MIDDLEWARES_BASE 设置合并(但不是覆盖)， 而后根据顺序(order)进行排序，最后得到启用中间件的有序列表: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。</p></li><li><p>如果想禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件， 必须在项目的 DOWNLOADER_MIDDLEWARES 设置中定义该中间件，并将其值赋为 None 。 例如，关闭user-agent中间件:  </p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &apos;myproject.middlewares.CustomDownloaderMiddleware&apos;: 543,</span><br><span class="line">    &apos;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&apos;: None,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>获得内置的DOWNLOADER_MIDDLEWARES_BASE中间件，命令行输入命令<code>scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE</code>，回显例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;: 100, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;: 300, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;: 350,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;: 400,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: 500,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;: 550, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;: 560, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;: 580, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 590, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;: 600, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;: 700, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;: 750, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;: 850, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;: 900&#125;</span><br></pre></td></tr></table></figure><h3 id="4、编写下载器中间件"><a href="#4、编写下载器中间件" class="headerlink" title="4、编写下载器中间件"></a>4、编写下载器中间件</h3><ul><li><p>process_request(request, spider)</p></li><li><p>process_response(request, response, spider)</p></li><li><p>process_exception(request, exception, spider)</p></li></ul><p>详见官方文档</p><h3 id="5、其他内置的MIDDLEWARE"><a href="#5、其他内置的MIDDLEWARE" class="headerlink" title="5、其他内置的MIDDLEWARE"></a>5、其他内置的MIDDLEWARE</h3><p>详见官方文档</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中Item Pipeline的用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADItem-Pipeline%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Item-Pipeline的用法/</id>
    <published>2018-07-05T06:00:00.000Z</published>
    <updated>2018-08-16T05:05:49.875Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、Item-Pipeline中文文档0-24-0"><a href="#1、Item-Pipeline中文文档0-24-0" class="headerlink" title="1、Item Pipeline中文文档0.24.0"></a>1、Item Pipeline中文文档0.24.0</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html</a></p><h3 id="2、Item-Pipeline的一些典型应用"><a href="#2、Item-Pipeline的一些典型应用" class="headerlink" title="2、Item Pipeline的一些典型应用"></a>2、Item Pipeline的一些典型应用</h3><ul><li><p>清理HTML数据</p></li><li><p>验证爬取的数据(检查item包含某些字段)</p></li><li><p>查重(并丢弃)</p></li><li><p>将爬取结果保存到数据库中</p></li></ul><h3 id="3、Item-Pipeline"><a href="#3、Item-Pipeline" class="headerlink" title="3、Item Pipeline"></a>3、Item Pipeline</h3><ul><li><p>process_item() 每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem 异常。</p></li><li><p>open_spider() 当spider被开启时，这个方法被调用。</p></li><li><p>close_spider() 当spider被关闭时，这个方法被调用。</p></li><li><p>from_crawler() 可以获取项目settings中的一些配置信息。</p></li></ul><h3 id="4、Item-Pipeline样例"><a href="#4、Item-Pipeline样例" class="headerlink" title="4、Item Pipeline样例"></a>4、Item Pipeline样例</h3><ul><li>将item写入JSON文件：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">class JsonWriterPipeline(object):</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.file = open(&apos;items.jl&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item)) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>将item写入MongoDB数据库：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">class MongoPipeline(object):</span><br><span class="line"></span><br><span class="line">    collection_name = &apos;scrapy_items&apos;</span><br><span class="line"></span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>截取item网页截图：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">import hashlib</span><br><span class="line">from urllib.parse import quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ScreenshotPipeline(object):</span><br><span class="line">    &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span><br><span class="line">    every Scrapy item.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    SPLASH_URL = &quot;http://localhost:8050/render.png?url=&#123;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        encoded_item_url = quote(item[&quot;url&quot;])</span><br><span class="line">        screenshot_url = self.SPLASH_URL.format(encoded_item_url)</span><br><span class="line">        request = scrapy.Request(screenshot_url)</span><br><span class="line">        dfd = spider.crawler.engine.download(request, spider)</span><br><span class="line">        dfd.addBoth(self.return_item, item)</span><br><span class="line">        return dfd</span><br><span class="line"></span><br><span class="line">    def return_item(self, response, item):</span><br><span class="line">        if response.status != 200:</span><br><span class="line">            # Error happened, return item.</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">        # Save screenshot to file, filename will be hash of url.</span><br><span class="line">        url = item[&quot;url&quot;]</span><br><span class="line">        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()</span><br><span class="line">        filename = &quot;&#123;&#125;.png&quot;.format(url_hash)</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line"></span><br><span class="line">        # Store filename in item.</span><br><span class="line">        item[&quot;screenshot_filename&quot;] = filename</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>去重：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line">class DuplicatesPipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&apos;id&apos;] in self.ids_seen:</span><br><span class="line">            raise DropItem(&quot;Duplicate item found: %s&quot; % item)</span><br><span class="line">        else:</span><br><span class="line">            self.ids_seen.add(item[&apos;id&apos;])</span><br><span class="line">            return item</span><br></pre></td></tr></table></figure><h3 id="5、启用一个Item-Pipeline组件："><a href="#5、启用一个Item-Pipeline组件：" class="headerlink" title="5、启用一个Item Pipeline组件："></a>5、启用一个Item Pipeline组件：</h3><p>在settings的ITEM_PIPELINES选项里填写：pipline名称:数字优先级。通常将这些数字定义在0-1000范围内，数字越小优先级越高。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &apos;myproject.pipelines.PricePipeline&apos;: 300,</span><br><span class="line">    &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中Spiders用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADSpiders%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Spiders用法/</id>
    <published>2018-07-05T05:00:00.000Z</published>
    <updated>2018-08-16T05:05:00.989Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、scrapy中文文档"><a href="#1、scrapy中文文档" class="headerlink" title="1、scrapy中文文档"></a>1、scrapy中文文档</h3><p>文档地址：<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest</a></p><p>其中spider部分：<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html</a></p><h3 id="2、Spider中文文档0-25-0里未翻译的部分简介"><a href="#2、Spider中文文档0-25-0里未翻译的部分简介" class="headerlink" title="2、Spider中文文档0.25.0里未翻译的部分简介"></a>2、Spider中文文档0.25.0里未翻译的部分简介</h3><ul><li>custom_settings 类里面的一个特定设置，可以覆盖settings里面的全局设置。以字典形式覆盖即可,例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">     DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">       &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</span><br><span class="line">       &apos;Accept-Language&apos;: &apos;en&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>crawler 是Crawler类的一个实例对象，扩展访问scrapy核心组件和挂载scrapy的唯一途径。</p></li><li><p>settings 是Settings类的一个实例对象，可以控制包括核心插件、pipeline、其他spider组件的相应的配置。</p></li><li><p>from_crawler() 是一个类方法，在类里面定义这个方法，可以通过这个类拿到全局的settings的一些设置。例如：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, mongo_uri, mongo_db, *args, **kwargs):</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">@classmethod</span><br><span class="line">def from_crawler(cls, crawler): </span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DB&apos;)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><h3 id="3、Spider英文文档1-5-0中增加部分简介"><a href="#3、Spider英文文档1-5-0中增加部分简介" class="headerlink" title="3、Spider英文文档1.5.0中增加部分简介"></a>3、Spider英文文档1.5.0中增加部分简介</h3><ul><li>logger 可以直接调用logger输出日志，例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.logger.info(response.status)</span><br></pre></td></tr></table></figure><ul><li>Spider arguments 可以通过命令行<code>scrapy crawl myspider -a category=electronics</code>，指定category传入一些相应的额外参数。例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;myspider&apos;</span><br><span class="line"></span><br><span class="line">    def __init__(self, category=None, *args, **kwargs):</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [&apos;http://www.example.com/categories/%s&apos; % category]</span><br><span class="line">        # ...</span><br></pre></td></tr></table></figure><h3 id="4、特定的spider"><a href="#4、特定的spider" class="headerlink" title="4、特定的spider"></a>4、特定的spider</h3><p>后面还提供了一些特定的spider，应用于一些定制化的需求。例如：CrawlSpider、XMLFeedSpider、CSVFeedSpider、SitemapSpider。</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中选择器用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%AD%E9%80%89%E6%8B%A9%E5%99%A8%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中选择器用法/</id>
    <published>2018-07-05T04:00:00.000Z</published>
    <updated>2018-08-16T05:02:11.340Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、选择器是scrapy内置的一个selector对象"><a href="#1、选择器是scrapy内置的一个selector对象" class="headerlink" title="1、选择器是scrapy内置的一个selector对象"></a>1、选择器是scrapy内置的一个selector对象</h3><p>测试的网址：<a href="https://doc.scrapy.org/en/latest/_static/selectors-sample1.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a></p><h3 id="2、命令行模式测试"><a href="#2、命令行模式测试" class="headerlink" title="2、命令行模式测试"></a>2、命令行模式测试</h3><p><code>scrapy shell https://doc.scrapy.org/en/latest/_static/selectors-sample1.html</code>  # 进入交互模式</p><p><code>response.selector</code>  # scrapy内置的选择器的类</p><p><code>response.selector.xpath(&#39;//title/text()&#39;)</code>  # xpath()返回一个Selector的list</p><p><code>response.selector.css(&#39;title::text&#39;)</code>  # css()返回一个Selector的list</p><p><code>response.selector.xpath(&#39;//title/text()&#39;).extract_first()</code>  # extract_first()可以获取具体的内容</p><p><code>response.selector.css(&#39;title::text&#39;).extract()</code>  # extract可以获取具体内容的列表</p><p><code>response.xpath(&#39;//title/text()&#39;</code>  # 直接用.xpath简写模式，可以省去.selector</p><p><code>response.css(&#39;title::text&#39;)</code>  # .css简写模式</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;)</code>  # 1.首先用xpath定位到这个div标签</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img&#39;)</code>  # 2.然后用css获取img标签的list</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;)</code>  # 3.接着用::attr获取src属性的列表</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;).extract()</code>  # 4.最后用extract获取具体的属性值列表</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;).extract_first(defaut=&#39;&#39;)</code>  #用default设置默认值，查找不到的话用default代替</p><p><code>response.xpath(&#39;//a/@href&#39;).extract()</code>  # xpath获取属性值</p><p><code>response.css(&#39;a::attr(href)&#39;).extract()</code>  # css获取属性值</p><p><code>response.xpath(&#39;//a/text()&#39;).extract()</code>  # xpath获取内容值</p><p><code>response.css(&#39;a::text&#39;).extract()</code>  # css获取内容值</p><p><code>response.xpath(&#39;//a[contains(@href,&quot;image&quot;)]/@href&#39;).extract()</code>  # xpath查找所有a标签下href属性名称包含”image”的超链接值</p><p><code>response.css(&#39;a[href*=image]::attr(href)&#39;).extract()</code>  #  css查找所有a标签下href属性名称包含”image”的超链接值</p><p><code>response.xpath(&#39;//a[contains(@href,&quot;image&quot;)]/img/@src&#39;).extract()</code>  # xpath查找所有a标签(href属性名称包含”image”)下img标签下src属性值</p><p><code>response.css(&#39;a[href*=image] img::attr(src)&#39;).extract()</code>  #  css查找所有a标签(href属性名称包含”image”)下img标签下src属性值</p><p><code>response.css(&#39;a::text&#39;).re(&#39;Name\:(.*)&#39;)</code>  # 通过re正则匹配，返回的是（.*）里的内容，‘Name\:’是匹配的起始位置</p><p><code>response.css(&#39;a::text&#39;).re_first(&#39;Name\:(.*)&#39;).strip()</code>  # rere_first返回正则匹配的第一个值，strip()去除前后的空格          </p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy命令行详解</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AF%A6%E8%A7%A3/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy命令行详解/</id>
    <published>2018-07-05T03:00:00.000Z</published>
    <updated>2018-08-16T02:58:55.552Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、命令行详解"><a href="#一、命令行详解" class="headerlink" title="一、命令行详解"></a>一、命令行详解</h2><h3 id="1、Scrapy官方文档"><a href="#1、Scrapy官方文档" class="headerlink" title="1、Scrapy官方文档"></a>1、Scrapy官方文档</h3><p><a href="https://doc.scrapy.org/en/latest/" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/</a></p><h3 id="2、命令行详解"><a href="#2、命令行详解" class="headerlink" title="2、命令行详解"></a>2、命令行详解</h3><p><code>scrapy startproject testproject</code>  # 新建项目</p><p><code>cd testproject</code></p><p><code>scrapy genspider baidu www.baidu.com</code>  # 生成spider</p><p><code>cd testproject</code></p><p><code>cd spiders</code></p><p><code>scrapy genspider -l</code>  # 列出所有的模板</p><p><code>scrapy genspider -t crawl zhihu www.zhihu.com</code>  # 指定crwal模板，生成spider</p><p><code>scrapy crawl zhihu</code>  # 运行爬虫</p><p><code>scrapy crawl baidu</code></p><p><code>scrapy check</code>  # 检查代码错误</p><p><code>scrapy list</code>  # 列出项目所有的spider</p><p><code>scrapy edit zhihu</code>  # 在命令行下编辑spider</p><p><code>cd ../../../</code></p><p><code>scrapy fetch http://www.baidu.com</code>  # download一个网页，返回网页源代码</p><p><code>scrapy fetch --nolog http://www.baidu.com</code>  # 返回网页源代码时，不会显示日志信息</p><p><code>scrapy fetch --headers http://www.baidu.com</code>  # 返回网页源代码时，返回headers信息    </p><p><code>scrapy fetch --no-redirect http://www.baidu.com</code>  # 请求网页时，不能重定向</p><p><code>scrapy view http://www.baidu.com</code>  # 请求网页，生成document下载下来，并用浏览器自动打开</p><p><code>scrapy view http://www.taobao.com</code>  # 会发现淘宝的网页很多内容不显示，因为都是用ajax加载的</p><p><code>scrapy shell https://www.baidu.com</code>  # 进入命令行交互模式</p><ul><li><p><code>request</code>  # 交互模式下进行操作</p></li><li><p><code>reponse.text</code>  # 交互模式下进行操作</p></li><li><p><code>reponse.headers</code>  # 交互模式下进行操作</p></li><li><p><code>response.css(&#39;title::text&#39;).extract_first()</code>  # 交互模式下进行操作</p></li><li><p><code>view(response)</code>  # 交互模式下进行操作</p></li><li><p><code>exit()</code>  # 退出交互模式</p></li></ul><p><code>cd D:\PycharmProjects\quotetutroial</code></p><p><code>scrapy parse http://quotes.toscrape.com --callback parse</code>  # parse方法，传入url，指定参数，看下解析结果</p><p><code>scrapy settings --get MONGO_URI</code>  # settings方法，获取一些配置信息</p><p><code>scrapy settings -h</code>  # 获取一些帮助信息</p><p><code>scrapy settings --getbool=ROBOTSTXT_OBEY</code>  # 验证下是否遵循ROBOTSTXT_OBEY这个规则</p><p><code>scrapy runspider quotes.py</code>  # 运行spider，注意文件名带.py</p><p><code>scrapy version</code>  # 输出scrapy版本</p><p><code>scrapy version -v</code>  # 输出一些依赖库的版本</p><p><code>scrapy bench</code>  # 测试下当前的爬行速度</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架基本使用</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy框架基本使用/</id>
    <published>2018-07-05T02:00:00.000Z</published>
    <updated>2018-08-21T09:48:51.757Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、目标站点"><a href="#1、目标站点" class="headerlink" title="1、目标站点"></a>1、目标站点</h3><p><a href="http://quotes.toscrape.com/" target="_blank" rel="noopener">http://quotes.toscrape.com/</a></p><h3 id="2、流程框架"><a href="#2、流程框架" class="headerlink" title="2、流程框架"></a>2、流程框架</h3><p>(1)抓取第一页</p><p>请求第一页URL并得到源代码，进行下一步分析。</p><p>(2)获取内容和下一页链接</p><p>分析源代码，获取首页内容，获取下一页链接等待进一步爬取。</p><p>(3)翻页爬取</p><p>请求下一页信息，分析内容并请求再下一页链接。</p><p>(4)保存爬取结果</p><p>将爬取结果保存为特定格式如文本、数据库。</p><h3 id="3、爬虫实战"><a href="#3、爬虫实战" class="headerlink" title="3、爬虫实战"></a>3、爬虫实战</h3><p>命令行输入：</p><p>新建工程：<code>scrapy startproject quotetutroial</code></p><p>进入目录：<code>cd quotetutroial</code></p><p>生成爬虫：<code>scrapy genspider quotes quotes.toscrape.com</code></p><blockquote><p>项目结构：</p><ul><li><p>scrapy.cfg 配置文件</p></li><li><p>items.py 保存数据的数据结构</p></li><li><p>middlewares.py 爬取过程中定义的一些中间件</p></li><li><p>pipelines.py 项目管道，可以输出一些items</p></li><li><p>settings.py 定义的一些配置信息</p></li><li><p>spider/quotes.py 主要的运行代码</p></li></ul></blockquote><p>运行爬虫：<code>scrapy crawl quotes</code></p><h3 id="4、项目代码"><a href="#4、项目代码" class="headerlink" title="4、项目代码"></a>4、项目代码</h3><p>quotetutroial / scrapy.cfg<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Automatically created by: scrapy startproject</span><br><span class="line">#</span><br><span class="line"># For more information about the [deploy] section see:</span><br><span class="line"># https://scrapyd.readthedocs.io/en/latest/deploy.html</span><br><span class="line"></span><br><span class="line">[settings]</span><br><span class="line">default = quotetutroial.settings</span><br><span class="line"></span><br><span class="line">[deploy]</span><br><span class="line">#url = http://localhost:6800/</span><br><span class="line">project = quotetutroial</span><br></pre></td></tr></table></figure></p><p>quotetutroial / quotetutroial / items.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="comment"># 爬取完后，一个一个进行赋值，生成一个整体</span></span><br><span class="line">    text = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br></pre></td></tr></table></figure></p><p>quotetutroial / quotetutroial / middlewares.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your spider middleware</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotetutroialSpiderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_input</span><span class="params">(self, response, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">        <span class="comment"># middleware and into the spider.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return None or raise an exception.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_output</span><span class="params">(self, response, result, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">        <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_exception</span><span class="params">(self, response, exception, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">        <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return either None or an iterable of Response, dict</span></span><br><span class="line">        <span class="comment"># or Item objects.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_start_requests</span><span class="params">(self, start_requests, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">        <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">        <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        spider.logger.info(<span class="string">'Spider opened: %s'</span> % spider.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotetutroialDownloaderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called for each request that goes through the downloader</span></span><br><span class="line">        <span class="comment"># middleware.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this request</span></span><br><span class="line">        <span class="comment"># - or return a Response object</span></span><br><span class="line">        <span class="comment"># - or return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest: process_exception() methods of</span></span><br><span class="line">        <span class="comment">#   installed downloader middleware will be called</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called with the response returned from the downloader.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either;</span></span><br><span class="line">        <span class="comment"># - return a Response object</span></span><br><span class="line">        <span class="comment"># - return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span></span><br><span class="line">        <span class="comment"># Called when a download handler or a process_request()</span></span><br><span class="line">        <span class="comment"># (from other downloader middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this exception</span></span><br><span class="line">        <span class="comment"># - return a Response object: stops process_exception() chain</span></span><br><span class="line">        <span class="comment"># - return a Request object: stops process_exception() chain</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        spider.logger.info(<span class="string">'Spider opened: %s'</span> % spider.name)</span><br></pre></td></tr></table></figure></p><p>quotetutroial / quotetutroial / pipelines.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.limit = <span class="number">50</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'text'</span>]:</span><br><span class="line">            <span class="keyword">if</span> len(item[<span class="string">'text'</span>]) &gt; self.limit:</span><br><span class="line">                item[<span class="string">'text'</span>] = item[<span class="string">'text'</span>][<span class="number">0</span>:self.limit].rstrip() + <span class="string">'...'</span></span><br><span class="line">                <span class="keyword">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> DropItem(<span class="string">'Missing Text'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span>  <span class="comment"># from_crawler方法可以从setting.py里拿到相应的配置信息</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )  <span class="comment"># 这是一个类方法，需要返回一个class对象</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span>  <span class="comment"># open_spider方法定义爬虫将要启动时的操作</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__  <span class="comment"># 将item的名称传给下边db表名</span></span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line">    <span class="comment"># 要想pipeline生效，需要在settings.py里指定ITEM_PIPELINES,序号越小优先级越高</span></span><br><span class="line">    <span class="comment"># 这里指定了两个pipeline，假设删掉一个pipline，那么只会执行另外一个pipline</span></span><br></pre></td></tr></table></figure></p><p>quotetutroial / quotetutroial / settings.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scrapy settings for quotetutroial project</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     https://doc.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="comment">#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'quotetutroial'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'quotetutroial.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'quotetutroial.spiders'</span></span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'quotestutorial'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line"><span class="comment">#USER_AGENT = 'quotetutroial (+http://www.yourdomain.com)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Disable Telnet Console (enabled by default)</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></span><br><span class="line"><span class="comment">#   'Accept-Language': 'en',</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    'quotetutroial.middlewares.QuotetutroialSpiderMiddleware': 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    'quotetutroial.middlewares.QuotetutroialDownloaderMiddleware': 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment">#'quotetutroial.pipelines.TextPipeline': 300,</span></span><br><span class="line">    <span class="string">'quotetutroial.pipelines.MongoPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_ENABLED = True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_START_DELAY = 5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_MAX_DELAY = 60</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="comment"># each remote server</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line"><span class="comment">#AUTOTHROTTLE_DEBUG = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></span><br><span class="line"><span class="comment">#HTTPCACHE_ENABLED = True</span></span><br><span class="line"><span class="comment">#HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"><span class="comment">#HTTPCACHE_DIR = 'httpcache'</span></span><br><span class="line"><span class="comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"><span class="comment">#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></span><br></pre></td></tr></table></figure></p><p>quotetutroial / quotetutroial / spiders / quotes.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> quotetutroial.items <span class="keyword">import</span> QuoteItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'quotes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'quotes.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        quotes = response.css(<span class="string">'.quote'</span>)</span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">            item = QuoteItem()</span><br><span class="line">            text = quotes.css(<span class="string">'.text::text'</span>).extract_first()  <span class="comment"># ::text是scrapy的语法格式，表示获取文本</span></span><br><span class="line">            author = quotes.css(<span class="string">'.author::text'</span>).extract_first()  <span class="comment"># extract_first()表示获取第一个结果</span></span><br><span class="line">            tags = quotes.css(<span class="string">'.tags .tag::text'</span>).extract()  <span class="comment"># extract()表示获取所有结果</span></span><br><span class="line">            <span class="comment"># 这里可以在终端里使用命令scrapy shell quotes.toscrape.com进入交互模式！！！</span></span><br><span class="line">            <span class="comment"># 输入exit()退出交互模式</span></span><br><span class="line">            item[<span class="string">'text'</span>] = text</span><br><span class="line">            item[<span class="string">'author'</span>] = author</span><br><span class="line">            item[<span class="string">'tags'</span>] = tags</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        next = response.css(<span class="string">'.pager .next a::attr(href)'</span>).extract_first()</span><br><span class="line">        url = response.urljoin(next)  <span class="comment"># 因为next是不完全的url，所以需要调用urljoin生成完整的url</span></span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)  <span class="comment"># 回调自身parse，实现翻页</span></span><br><span class="line">        <span class="comment"># 此时可以命令行输入命令scrapy crawl quotes -o quotes.json保存数据</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o quotes.jl保存数据成json line格式</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o quotes.csv保存数据成csv格式</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o quotes.xml保存数据成xml格式</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o quotes.pickle保存数据成pickle格式</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o quotes.marshal保存数据成marshal格式</span></span><br><span class="line">        <span class="comment"># 还可以输入命令scrapy crawl quotes -o ftp://user:pass@ftp.example.com/path/quotes.csv保存数据到ftp服务器</span></span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架安装</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E6%A1%86%E6%9E%B6%E5%AE%89%E8%A3%85/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy框架安装/</id>
    <published>2018-07-05T01:00:00.000Z</published>
    <updated>2018-08-16T02:58:28.473Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、框架安装"><a href="#一、框架安装" class="headerlink" title="一、框架安装"></a>一、框架安装</h2><h3 id="1、Scrapy安装"><a href="#1、Scrapy安装" class="headerlink" title="1、Scrapy安装"></a>1、Scrapy安装</h3><p>scrapy依赖的库比较多，至少需要依赖库有pywin32、Twisted、pyOpenSSL、lxml。而在不同平台环境又各不相同，所以安装之前最好确保依赖库安装好，尤其是windows。</p><h3 id="2、windows"><a href="#2、windows" class="headerlink" title="2、windows"></a>2、windows</h3><p>最好的安装方式是通过wheel文件来安装。首先需要安装wheel库：</p><p><code>pip install wheel</code></p><p>(1)安装lxml</p><p>进入<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\lxml‑3.7.2‑cp36‑cp36m‑win32.whl</code></p><p>(2)安装OpenSSL</p><p>进入<a href="https://pypi.python.org/pypi/pyOpenSSL#downloads" target="_blank" rel="noopener">https://pypi.python.org/pypi/pyOpenSSL#downloads</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\pyOpenSSL-16.2.0-py2.py3-none-any.whl</code></p><p>(3)安装Twisted</p><p>进入<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\Twisted‑17.1.0‑cp36‑cp36m‑win32.whl</code></p><p>(4)安装pywin32</p><p>进入<a href="https://sourceforge.net/projects/pywin32/files/pywin32/Build%20220/" target="_blank" rel="noopener">https://sourceforge.net/projects/pywin32/files/pywin32/Build%20220/</a> ，找到与python版本和系统版本对应的安装包，运行安装即可。</p><p>(5)安装Scrapy</p><p>最后安装Scrapy，pip安装即可：</p><p><code>pip install Scrapy</code></p><h3 id="3、Anaconda安装"><a href="#3、Anaconda安装" class="headerlink" title="3、Anaconda安装"></a>3、Anaconda安装</h3><p>可以轻松的使用conda命令安装Scrapy：</p><p><code>conda install Scrapy</code></p><h3 id="4、安装验证"><a href="#4、安装验证" class="headerlink" title="4、安装验证"></a>4、安装验证</h3><p>安装完，cmd输入命令<code>scarpy</code>可以验证。</p><p>进一步创建一个工程：</p><p><code>scrapy startproject hello</code></p><p><code>cd hello</code></p><p><code>scrapy genspider baidu www.baidu.com</code></p><p><code>scrapy crawl baidu</code></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Pyspider架构概述及用法详解</title>
    <link href="http://pythonfood.github.io/2018/07/04/Pyspider%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0%E5%8F%8A%E7%94%A8%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>http://pythonfood.github.io/2018/07/04/Pyspider架构概述及用法详解/</id>
    <published>2018-07-04T02:00:00.000Z</published>
    <updated>2018-08-16T02:27:23.585Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、官方详解"><a href="#一、官方详解" class="headerlink" title="一、官方详解"></a>一、官方详解</h2><h3 id="1、官方文档"><a href="#1、官方文档" class="headerlink" title="1、官方文档"></a>1、官方文档</h3><p><a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/</a></p><h3 id="2、中文翻译"><a href="#2、中文翻译" class="headerlink" title="2、中文翻译"></a>2、中文翻译</h3><p><a href="http://www.pyspider.cn/book/pyspider/pyspider-Quickstart-2.html" target="_blank" rel="noopener">http://www.pyspider.cn/book/pyspider/pyspider-Quickstart-2.html</a></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>PySpider框架基本使用及抓取TripAdvisor实战</title>
    <link href="http://pythonfood.github.io/2018/07/04/PySpider%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%8A%93%E5%8F%96TripAdvisor%E5%AE%9E%E6%88%98/"/>
    <id>http://pythonfood.github.io/2018/07/04/PySpider框架基本使用及抓取TripAdvisor实战/</id>
    <published>2018-07-04T01:00:00.000Z</published>
    <updated>2018-08-16T02:26:36.285Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、PySpider框架"><a href="#1、PySpider框架" class="headerlink" title="1、PySpider框架"></a>1、PySpider框架</h3><ul><li>去重处理</li><li>结果监控</li><li>多进程处理</li><li>PyQuery提取</li><li>错误重试</li><li>WebUI管理</li><li>代码简洁</li><li>JavaScript渲染</li></ul><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p>cmd管理员身份运行，执行命令<code>pip install pyspider</code></p><h3 id="3、运行"><a href="#3、运行" class="headerlink" title="3、运行"></a>3、运行</h3><p>cmd输入命令<code>pyspider</code>。浏览器输入地址：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a></p><p>ps:需要安装过phantomjs</p><h3 id="4、实战"><a href="#4、实战" class="headerlink" title="4、实战"></a>4、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># Created on 2018-06-07 22:43:08</span></span><br><span class="line"><span class="comment"># Project: TripAdvisor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    client = pymongo.MongoClient(<span class="string">'localhost'</span>)</span><br><span class="line">    db = client[<span class="string">'trip'</span>]</span><br><span class="line"><span class="meta">    @every(minutes=24 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.crawl(<span class="string">'http://www.tripadvisor.cn/Attractions-g186338-Activities-c47-t163-London_England.html#ATTRACTION_LIST'</span>, callback=self.index_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(age=10 * 24 * 60 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'.listing_title &gt; a'</span>).items():</span><br><span class="line">            self.crawl(each.attr.href, callback=self.detail_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line">            next = response.doc(<span class="string">'.pagination .nav.next'</span>).attr.href</span><br><span class="line">            self.crawl(next, callback=self.index_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(priority=2)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        name = response.doc(<span class="string">'.heading_title'</span>).text()</span><br><span class="line">        rating = response.doc(<span class="string">'div &gt; .more'</span>).text()</span><br><span class="line">        adress = response.doc(<span class="string">'.location &gt; .address'</span>).text()</span><br><span class="line">        phone = response.doc(<span class="string">'.phone &gt; div'</span>).text()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"url"</span>:response.url,</span><br><span class="line">            <span class="string">"name"</span>:name,</span><br><span class="line">            <span class="string">"rating"</span>:rating,</span><br><span class="line">            <span class="string">"adress"</span>:adress,</span><br><span class="line">            <span class="string">"phone"</span>:phone</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.db[<span class="string">'london'</span>].insert(result):</span><br><span class="line">            print(<span class="string">'save to mongodb:'</span>,result)</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Redis+Flask维护动态Cookies池</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Redis-Flask%E7%BB%B4%E6%8A%A4%E5%8A%A8%E6%80%81Cookies%E6%B1%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Redis-Flask维护动态Cookies池/</id>
    <published>2018-07-03T05:00:00.000Z</published>
    <updated>2018-08-15T12:27:01.678Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、Cookies池详解"><a href="#一、Cookies池详解" class="headerlink" title="一、Cookies池详解"></a>一、Cookies池详解</h2><h3 id="1、为什么要维护cookie池"><a href="#1、为什么要维护cookie池" class="headerlink" title="1、为什么要维护cookie池"></a>1、为什么要维护cookie池</h3><p>有的网站需要登录后才能爬取，如新浪微博</p><p>爬取过程中如果频率过高会导致封号，那么如果想要获得非常多的数据，则需要非常多的账号</p><h3 id="2、cookie池的要求"><a href="#2、cookie池的要求" class="headerlink" title="2、cookie池的要求"></a>2、cookie池的要求</h3><ul><li>自动登录更新</li><li>定时验证筛选</li><li>提供外部接口（可将池架在远程的服务器上，实现远程部署</li></ul><h3 id="3、cookie池的架构"><a href="#3、cookie池的架构" class="headerlink" title="3、cookie池的架构"></a>3、cookie池的架构</h3><p>账号队列 ===》 生成器 ===》 cookies队列（对外提供API） ===》 定时检测器</p><h3 id="4、cookie池的实现"><a href="#4、cookie池的实现" class="headerlink" title="4、cookie池的实现"></a>4、cookie池的实现</h3><p>需要先将一定量的账号密码之类的cookie存进Redis数据库，然后利用python调用并维护。</p><p>关于cookies池的维护，有以下开源项目案例可供参考：<a href="https://github.com/Germey/CookiesPool" target="_blank" rel="noopener">https://github.com/Germey/CookiesPool</a></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用代理处理反爬抓取微信文章</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E5%A4%84%E7%90%86%E5%8F%8D%E7%88%AC%E6%8A%93%E5%8F%96%E5%BE%AE%E4%BF%A1%E6%96%87%E7%AB%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用代理处理反爬抓取微信文章/</id>
    <published>2018-07-03T04:00:00.000Z</published>
    <updated>2018-08-15T13:05:59.205Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、流程框架"><a href="#1、流程框架" class="headerlink" title="1、流程框架"></a>1、流程框架</h3><p>(1)抓取索引页内容</p><p>利用requests请求目标站点，得到索引网页HTML代码，返回结果。</p><p>(2)代理设置</p><p>如果遇到302状态码，则证明IP被封，切换代理重试。</p><p>(3)分析详情页内容</p><p>请求详情页，分析得到标题，正文等内容。</p><p>(4)将数据保存到数据库</p><p>将结构化数据保存到MongoDB。</p><h3 id="2、实战"><a href="#2、实战" class="headerlink" title="2、实战"></a>2、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ConnectionError</span><br><span class="line"><span class="keyword">from</span> lxml.etree <span class="keyword">import</span> XMLSyntaxError</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> wxconfig <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据库连接</span></span><br><span class="line">client = pymongo.MongoClient(MONGO_URI)</span><br><span class="line">db = client[MONGO_DB]</span><br><span class="line"><span class="comment"># 爬取url</span></span><br><span class="line">base_url = <span class="string">'http://weixin.sogou.com/weixin?'</span></span><br><span class="line"><span class="comment"># 请求头，需要带上cookie，不然只能访问10页数据</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'SUID=F6177C7B3220910A000000058E4D679; SUV=1491392122762346; ABTEST=1|1491392129|v1; SNUID=0DED8681FBFEB69230E6BF3DFB2F8D6B; ld=OZllllllll2Yi2balllllV06C77lllllWTZgdkllll9lllllxv7ll5@@@@@@@@@@; LSTMV=189%2C31; LCLKINT=1805; weixinIndexVisited=1; SUIR=0DED8681FBFEB69230E6BF3DFB2F8D6B; JSESSIONID=aaa-BcHIDk9xYdr4odFSv; PHPSESSID=afohijek3ju93ab6l0eqeph902; sct=21; IPLOC=CN; ppinf=5|1491580643|1492790243|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZToyNzolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOER8Y3J0OjEwOjE0OTE1ODA2NDN8cmVmbmljazoyNzolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOER8dXNlcmlkOjQ0Om85dDJsdUJfZWVYOGRqSjRKN0xhNlBta0RJODRAd2VpeGluLnNvaHUuY29tfA; pprdig=j7ojfJRegMrYrl96LmzUhNq-RujAWyuXT_H3xZba8nNtaj7NKA5d0ORq-yoqedkBg4USxLzmbUMnIVsCUjFciRnHDPJ6TyNrurEdWT_LvHsQIKkygfLJH-U2MJvhwtHuW09enCEzcDAA_GdjwX6_-_fqTJuv9w9Gsw4rF9xfGf4; sgid=; ppmdig=1491580643000000d6ae8b0ebe76bbd1844c993d1ff47cea'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">    <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 开始时是否启用代理</span></span><br><span class="line">proxy = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 代理获取函数，这里使用类flask的接口</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(PROXY_POOL_URL)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="comment"># 没有异常直接结束</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url, count=<span class="number">1</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'Crawling'</span>, url)</span><br><span class="line">    print(<span class="string">'Try Count'</span>, count)</span><br><span class="line">    <span class="keyword">global</span> proxy</span><br><span class="line">    <span class="comment">#设置访问深度</span></span><br><span class="line">    <span class="keyword">if</span> count &gt;= MAX_COUNT:</span><br><span class="line">        print(<span class="string">'Tried Too Many Counts'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 判断启用代理和有代理</span></span><br><span class="line">        <span class="keyword">if</span> proxy:</span><br><span class="line">            <span class="comment"># 给代理池取出的数据加上协议头</span></span><br><span class="line">            proxies = &#123;</span><br><span class="line">                <span class="string">'http'</span>:<span class="string">'http://'</span> + proxy</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment"># 需要设置allow_redirects=False,禁止requests自动处理重定向</span></span><br><span class="line">            response = requests.get(url, headers=headers, proxies=proxies, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            response = requests.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># 如果请求成功返回    </span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">302</span>:</span><br><span class="line">            <span class="comment"># 出现302说明IP被封，需要更换代理</span></span><br><span class="line">            print(<span class="string">'302'</span>)</span><br><span class="line">            <span class="comment"># 获取一个新代理</span></span><br><span class="line">            proxy = get_proxy()</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                print(<span class="string">'Using Proxy'</span>, proxy)</span><br><span class="line">                 <span class="comment"># 获取到代理ip重新获取网页</span></span><br><span class="line">                <span class="keyword">return</span> get_html(url)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'Get Proxy Failed'</span>)</span><br><span class="line">                <span class="comment"># 没有代理了，直接退出</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'Error Occurred'</span>, e.args)</span><br><span class="line">        proxy = get_proxy()</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> get_html(url, count)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(keyword,page)</span>:</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">'query'</span>:keyword,</span><br><span class="line">        <span class="string">'type'</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="string">'page'</span>:page</span><br><span class="line">    &#125;</span><br><span class="line">    queries = urlencode(data)</span><br><span class="line">    url = base_url + queries</span><br><span class="line">    html = get_html(url)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> item.attr(<span class="string">'href'</span>)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_detail</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        doc = pq(html)</span><br><span class="line">        title = doc(<span class="string">'.rich_media_title'</span>).text()</span><br><span class="line">        content = doc(<span class="string">'.rich_media_content'</span>).text()</span><br><span class="line">        date = doc(<span class="string">'#post-date'</span>).text()</span><br><span class="line">        nickname = doc(<span class="string">'#js_profile_qrcode &gt; div &gt; strong'</span>).text()</span><br><span class="line">        wechat = doc(<span class="string">'#js_profile_qrcode &gt; div &gt; p:nth-child(3) &gt; span'</span>).text()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: title,</span><br><span class="line">            <span class="string">'content'</span>: content,</span><br><span class="line">            <span class="string">'date'</span>: date,</span><br><span class="line">            <span class="string">'nickname'</span>: nickname,</span><br><span class="line">            <span class="string">'wechat'</span>: wechat</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">except</span> XMLSyntaxError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> db[<span class="string">'articles'</span>].update(&#123;<span class="string">'title'</span>:data[<span class="string">'title'</span>]&#125;,&#123;<span class="string">'$set'</span>:data&#125;,<span class="keyword">True</span>):</span><br><span class="line">        print(<span class="string">'Saved to Mongo'</span>, data[<span class="string">'title'</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'Saved to Mongo Failed'</span>, data[<span class="string">'title'</span>])</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 设置访问的页数和次数</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 传入访问的关键字</span></span><br><span class="line">        html = get_index(KEYWORD, page)</span><br><span class="line">        <span class="comment"># 获取到html文件，获取链接</span></span><br><span class="line">        <span class="keyword">if</span> html:</span><br><span class="line">            article_urls = parse_index(html)</span><br><span class="line">            <span class="keyword">for</span> article_url <span class="keyword">in</span> article_urls:</span><br><span class="line">                article_html = get_detail(article_url)</span><br><span class="line">                <span class="keyword">if</span> article_html:</span><br><span class="line">                    <span class="comment"># 解析文章内容</span></span><br><span class="line">                    article_data = parse_detail(article_html)</span><br><span class="line">                    print(article_data)</span><br><span class="line">                    <span class="comment"># 保存到数据库</span></span><br><span class="line">                    <span class="keyword">if</span> article_data:</span><br><span class="line">                        save_to_mongo(article_data)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>wxconfig.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line">PROXY_POOL_URL = <span class="string">'http://127.0.0.1:5000/get'</span></span><br><span class="line">KEYWORD = <span class="string">'风景'</span></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'weixin'</span></span><br><span class="line">MAX_COUNT = <span class="number">5</span></span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Redis+Flask维护动态代理池</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Redis-Flask%E7%BB%B4%E6%8A%A4%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%B1%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Redis-Flask维护动态代理池/</id>
    <published>2018-07-03T03:00:00.000Z</published>
    <updated>2018-08-15T13:06:45.676Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、代理池详解"><a href="#一、代理池详解" class="headerlink" title="一、代理池详解"></a>一、代理池详解</h2><h3 id="1、代理池的维护"><a href="#1、代理池的维护" class="headerlink" title="1、代理池的维护"></a>1、代理池的维护</h3><p>目前有很多网站提供免费代理，而且种类齐全，比如各个地区、各个匿名级别的都有，不过质量实在不敢恭维，毕竟都是免费公开的，可能一个代理无数个人在用也说不定。所以我们需要做的是大量抓取这些免费代理，然后筛选出其中可用的代理存储起来供我们使用，不可用的进行剔除。</p><h3 id="2、获取代理途径"><a href="#2、获取代理途径" class="headerlink" title="2、获取代理途径"></a>2、获取代理途径</h3><p>维护一个代理池第一步就是要找到提供免费代理的站点，例如PROXY360。可以看到网页里提供了一些免费代理列表，包括服务器地址、端口、代理种类、地区、更新时间等等信息。</p><p>当前我们需要的就是代理服务器和端口信息，将其爬取下来即可。</p><h3 id="3、代理池的要求"><a href="#3、代理池的要求" class="headerlink" title="3、代理池的要求"></a>3、代理池的要求</h3><ul><li>多占抓取，异步检测</li><li>定时筛选，持续更新</li><li>提供接口，易于获取</li></ul><h3 id="4、代理池的架构"><a href="#4、代理池的架构" class="headerlink" title="4、代理池的架构"></a>4、代理池的架构</h3><p>Internet  ===》  代理获取器  ===》  代理筛选器  ===》  代理调度器（对外提供API）  ===》  定时检测器</p><h3 id="5、维护代理"><a href="#5、维护代理" class="headerlink" title="5、维护代理"></a>5、维护代理</h3><p>那么爬取下代理之后怎样保存呢？</p><p>首先我们需要确保的目标是可以边取边存，另外还需要定时检查队列中不可用的代理将其剔除，所以需要易于存取。</p><p>另外怎样区分哪些是最新的可用的，哪些是旧的，如果用修改时间来标注是可以的，不过更简单的方法就是维护一个队列，只从一端存入，例如右端，这样就能确保最新的代理在队列右端，而在左端则是存入时间较长的代理，如果要取一个可用代理，从队列右端取一个就好了。</p><p>那么对于队列的左端，不能让它一直老化下去，还需要做的操作就是定时从队列左端取出代理，然后进行检测，如果可用，重新将其加入右端。</p><p>通过以上操作，就保证了代理一直是最新可用的。</p><p>所以目前来看，既能高效处理，又可以做到队列动态维护，合适的方法就是利用Redis数据库的队列。</p><p>可以定义一个类来维护一个Redis队列，比如get方法是批量从左端取出代理，put方法是从右端放入可用代理，pop方法是从右端取出最新可用代理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> proxypool.error <span class="keyword">import</span> PoolEmptyError</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> HOST, PORT</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=HOST, port=PORT)</span>:</span></span><br><span class="line">        self._db = redis.Redis(host, port)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, count=<span class="number">1</span>)</span>:</span></span><br><span class="line">        proxies = self._db.lrange(<span class="string">"proxies"</span>, <span class="number">0</span>, count - <span class="number">1</span>)</span><br><span class="line">        self._db.ltrim(<span class="string">"proxies"</span>, count, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> proxies</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self, proxy)</span>:</span></span><br><span class="line">        self._db.rpush(<span class="string">"proxies"</span>, proxy)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self._db.rpop(<span class="string">"proxies"</span>).decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">raise</span> PoolEmptyError</span><br></pre></td></tr></table></figure><h3 id="6、检测代理"><a href="#6、检测代理" class="headerlink" title="6、检测代理"></a>6、检测代理</h3><p>那么如何来检测代理是否可用？可以使用这个代理来请求某个站点，比如百度，如果获得正常的返回结果，那证明代理可用，否则代理不可用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conn = RedisClient()</span><br><span class="line">proxies = &#123;<span class="string">'http'</span>: proxy&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>, proxies=proxies)</span><br><span class="line"><span class="keyword">if</span> r.status_code == <span class="number">200</span>:</span><br><span class="line">    conn.put(proxy)</span><br></pre></td></tr></table></figure><h3 id="7、获取可用代理"><a href="#7、获取可用代理" class="headerlink" title="7、获取可用代理"></a>7、获取可用代理</h3><p>现在我们维护了一个代理池，那么这个代理池需要是可以公用的。</p><p>比如现在有多个爬虫项目都需要用到代理，而代理池的维护作为另外的一个项目，他们之间如果要建立连接，最恰当的方式就是接口。</p><p>所以可以利用Web服务器来实现一个接口，其他的项目通过请求这个接口得到内容获取到一个可用代理，这样保证了代理池的通用性。</p><p>所以要实现这个还需要一个Web服务器，例如Flask，Tornado等等。</p><p>例如使用Flask，定义一个路由，然后调用的RedisClient的pop方法，返回结果即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    conn = RedisClient()</span><br><span class="line">    <span class="keyword">return</span> conn.pop()</span><br></pre></td></tr></table></figure><p>这样一来，整个程序运行起来后，浏览器输入localhost:5000/get，请求网页就可以看到一个可用代理了。</p><h3 id="8、使用代理"><a href="#8、使用代理" class="headerlink" title="8、使用代理"></a>8、使用代理</h3><p>使用代理时只需要请求这个站点，就可以拿到可使用的代理了。</p><p>可以定义一个简单的方法，返回网页内容即代理，然后在爬取方法里设置代理使用即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    r = requests.get(<span class="string">'http://127.0.0.1:5000/get'</span>)</span><br><span class="line">    <span class="keyword">return</span> r.text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url, proxy)</span>:</span></span><br><span class="line">    proxies = &#123;<span class="string">'http'</span>: get_proxy()&#125;</span><br><span class="line">    r = requests.get(url, proxies=proxies)</span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><h3 id="9、github项目源代码"><a href="#9、github项目源代码" class="headerlink" title="9、github项目源代码"></a>9、github项目源代码</h3><p><a href="https://github.com/Germey/ProxyPool" target="_blank" rel="noopener">https://github.com/Germey/ProxyPool</a></p><p>PS:项目requirements.txt文件中缺少一个依赖库fake-useragent，需要手动pip安装。</p><p>还有如果报错’TypeError: expected string or bytes-like object’,找到对应的代码，是一个爬取代理网站的方法，删除或屏蔽掉就可以了。</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Selenium模拟浏览器抓取淘宝商品美食信息</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Selenium%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8A%93%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E7%BE%8E%E9%A3%9F%E4%BF%A1%E6%81%AF/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Selenium模拟浏览器抓取淘宝商品美食信息/</id>
    <published>2018-07-03T02:00:00.000Z</published>
    <updated>2018-08-15T10:09:18.837Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h3><p>selenium、pyquery、re、pymongo</p><h3 id="2、流程"><a href="#2、流程" class="headerlink" title="2、流程"></a>2、流程</h3><p>(1)目标站点分析:<br>用浏览器打开淘宝首页输入‘美食’，打开审查元素，分析我们要的商品信息都在Element里哪个分段。</p><p>(2)搜索关键字:<br>利用Selenium驱动浏览器搜索关键字，得到查询后的商品列表。</p><p>(3)分析页码并翻页:<br>得到商品页码数，模拟翻页，得到后续页面的商品列表。</p><p>(4)分析提取商品内容:<br>利用PyQuery分析源码，解析得到商品列表。</p><p>(5)储存到MongoDB:<br>将商品列表信息储存到数据库MongoDB。</p><h3 id="3、实战"><a href="#3、实战" class="headerlink" title="3、实战"></a>3、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="string">'''使用PhantomJS无界面浏览器，注意需要设置下窗口大小，否则有的元素获取不到会报错'''</span></span><br><span class="line"><span class="comment">#browser = webdriver.PhantomJS()</span></span><br><span class="line"><span class="comment">#browser.set_window_size(1400,900)</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''可以给PhantomJS设置些选项，从而增加效率。比如：不加载图片、设置缓存等'''</span></span><br><span class="line"><span class="comment">#SERVICE_ARGS = ['--load-images=false', '--disk-cache=true']</span></span><br><span class="line"><span class="comment">#browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)</span></span><br><span class="line"><span class="comment">#browser.set_window_size(1400,900)</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''chrome也可以设置无界面模式，Mac和Linux需要chrome59版本，Windows需要chrome60版本'''</span></span><br><span class="line"><span class="comment">#chrome_options = webdriver.ChromeOptions()</span></span><br><span class="line"><span class="comment">#chrome_options.add_argument('--headless')</span></span><br><span class="line"><span class="comment">#browser = webdriver.Chrome(chrome_options=chrome_options)</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">wait = WebDriverWait(browser,<span class="number">10</span>)</span><br><span class="line">client = pymongo.MongoClient(MONGO_URL) <span class="comment">#声明mongodb客户端，MONGO_URL从配置文件config.py获取</span></span><br><span class="line">db = client[MONGO_DB] <span class="comment">#定义数据库，MONGO_DB从配置文件config.py获取。注意中括号[]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">()</span>:</span> <span class="comment">#首页搜索关键字</span></span><br><span class="line">    print(<span class="string">'正在搜索'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        browser.get(<span class="string">'http://www.taobao.com'</span>)</span><br><span class="line">        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#q'</span>))) <span class="comment">#等待搜索框加载</span></span><br><span class="line">        submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#J_TSearchForm &gt; div.search-button &gt; button.btn-search'</span>))) <span class="comment">#等待搜索按钮加载</span></span><br><span class="line">        input.send_keys(KEYWORD) <span class="comment">#输入搜索关键字，KEYWORD从配置文件config.py获取</span></span><br><span class="line">        submit.click()</span><br><span class="line">        total = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total'</span>))) <span class="comment">#获取总页数</span></span><br><span class="line">        get_products()</span><br><span class="line">        <span class="keyword">return</span> total.text <span class="comment">#返回总页数</span></span><br><span class="line">    <span class="keyword">except</span> TimeoutException: <span class="comment">#捕捉browser超时异常</span></span><br><span class="line">        <span class="keyword">return</span> search() <span class="comment">#超时异常后，重新进行搜索即可</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_page</span><span class="params">(page_number)</span>:</span> <span class="comment">#输入页码，进行翻页</span></span><br><span class="line">    print(<span class="string">'正在翻页'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'</span>))) <span class="comment">#等待页码输入框</span></span><br><span class="line">        submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'</span>))) <span class="comment">#等待提交按钮</span></span><br><span class="line">        input.clear() <span class="comment">#输入前先清除内容</span></span><br><span class="line">        input.send_keys(page_number)</span><br><span class="line">        submit.click()</span><br><span class="line">        wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span'</span>),str(page_number))) <span class="comment">#等待高亮页码数值显示跳转的页码，确定跳转完成</span></span><br><span class="line">        get_products()</span><br><span class="line">    <span class="keyword">except</span> TimeoutException:</span><br><span class="line">        <span class="keyword">return</span> next_page(page_number) <span class="comment">#超时异常后，重新进行翻页即可</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_products</span><span class="params">()</span>:</span> <span class="comment">#获取当页商品数据</span></span><br><span class="line">    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-itemlist .items .item'</span>)))</span><br><span class="line">    html = browser.page_source <span class="comment">#获取当前页面源码</span></span><br><span class="line">    doc = pq(html) <span class="comment">#声明pyquery对象</span></span><br><span class="line">    items = doc(<span class="string">'#mainsrp-itemlist .items .item'</span>).items() <span class="comment"># 获取当前页所有商品对象</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items: <span class="comment">#遍历所有商品</span></span><br><span class="line">        product = &#123;</span><br><span class="line">            <span class="string">'image'</span>:item.find(<span class="string">'.pic .img'</span>).attr(<span class="string">'src'</span>),</span><br><span class="line">            <span class="string">'price'</span>:item.find(<span class="string">'.price'</span>).text(),</span><br><span class="line">            <span class="string">'deal'</span>:item.find(<span class="string">'.deal-cnt'</span>).text()[:<span class="number">-3</span>],</span><br><span class="line">            <span class="string">'title'</span>:item.find(<span class="string">'.title'</span>).text(),</span><br><span class="line">            <span class="string">'shop'</span>:item.find(<span class="string">'.shop'</span>).text(),</span><br><span class="line">            <span class="string">'location'</span>:item.find(<span class="string">'.location'</span>).text()</span><br><span class="line">        &#125;</span><br><span class="line">        save_to_mongo(product)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(result)</span>:</span> <span class="comment">#保存到mongodb数据库</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> db[MONGO_TABLE].insert(result): <span class="comment">#判断数据插入到数据表中，MONGO_TABLE从配置文件config.py中获取</span></span><br><span class="line">            print(<span class="string">'存储到MongoDB成功'</span>,result)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'存储到MongoDB失败'</span>,result)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        total = search()</span><br><span class="line">        total = int(re.compile(<span class="string">'(\d+)'</span>).search(total).group(<span class="number">1</span>)) <span class="comment">#正则匹配对象，搜索总页数字符串，结果索引1，转为int型就是总页数了</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,total+<span class="number">1</span>): <span class="comment">#从第二页开始循环翻页</span></span><br><span class="line">            next_page(i)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'浏览器出错啦'</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        print(<span class="string">'爬取完成'</span>)</span><br><span class="line">        browser.close() <span class="comment">#完成后一定关闭浏览器</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>config.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#config.py</span></span><br><span class="line"></span><br><span class="line">KEYWORD = <span class="string">'美食'</span></span><br><span class="line"></span><br><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'taobao'</span></span><br><span class="line">MONGO_TABLE = <span class="string">'product'</span></span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>分析Ajax爬取今日头条街拍美图</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E5%88%86%E6%9E%90Ajax%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E8%A1%97%E6%8B%8D%E7%BE%8E%E5%9B%BE/"/>
    <id>http://pythonfood.github.io/2018/07/03/分析Ajax爬取今日头条街拍美图/</id>
    <published>2018-07-03T02:00:00.000Z</published>
    <updated>2018-08-15T03:36:41.354Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h3><p>请确保已经安装好requests库，使用的编辑器是PyCharm。</p><h3 id="2、抓取分析"><a href="#2、抓取分析" class="headerlink" title="2、抓取分析"></a>2、抓取分析</h3><p>(1)抓取前分析抓取对象：打开今日头条首页<a href="https://www.toutiao.com/" target="_blank" rel="noopener">https://www.toutiao.com/</a></p><p>(2)右上角输入“街拍”，搜索进入街拍。</p><p>(3)这时打开开发者工具，查看所有的网络请求。首先，打开第一个网络请求，这个请求的URL就是当前的链接<a href="http://www.toutiao.com/search/?keyword=街拍" target="_blank" rel="noopener">http://www.toutiao.com/search/?keyword=街拍</a> ，打开Preview选项卡查看Response Body。如果页面中的内容是根据第一个请求得到的结果渲染出来的，那么第一个请求的源代码中必然会包含页面结果中的文字。</p><p>(4)实际没有结果中的文字，可以初步判断这些内容是由Ajax加载，然后用JavaScript渲染出来的。接下来，我们可以切换到XHR过滤选项卡，查看一下有没有Ajax请求。不出所料，此处出现了一个比较常规的Ajax请求，看看它的结果是否包含了页面中的相关数据。   </p><p>(5)点击data字段展开，发现这里有许多条数据。点击第一条展开，可以发现有一个title字段，它的值正好就是页面中第一条数据的标题。再检查一下其他数据，也正好是一一对应的。</p><p>(6)我们的目的是要抓取其中的美图，这里一组图就对应前面data字段中的一条数据。每条数据还有一个image_detail字段，它是列表形式，这其中就包含了组图的所有图片列表,因此，我们只需要将列表中的url字段提取出来并下载下来就好了。每一组图都建立一个文件夹，文件夹的名称就为组图的标题。</p><p>(7)接下来，就可以直接用Python来模拟这个Ajax请求，然后提取出相关美图链接并下载。但是在这之前，我们还需要分析一下URL的规律。切换回Headers选项卡，观察一下它的请求URL和Headers信息。</p><p>(8)可以看到，这是一个GET请求，请求URL的参数有offset、format、keyword、autoload、count和cur_tab。我们需要找出这些参数的规律，因为这样才可以方便地用程序构造出来。</p><p>(9)接下来，可以滑动页面，多加载一些新结果。在加载的同时可以发现，Network中又出现了许多Ajax请求，这里观察一下后续链接的参数，发现变化的参数只有offset，其他参数都没有变化，而且第二次请求的offset值为20，第三次为40，第四次为60，所以可以发现规律，这个offset值就是偏移量，进而可以推断出count参数就是一次性获取的数据条数。因此，我们可以用offset参数来控制数据分页。这样一来，我们就可以通过接口批量获取数据了，然后将数据解析，将图片下载下来即可。</p><p><strong>PS：崔老师教程的第(6)步中查找的字段image_detail，实际上没有发现，这里使用image_list代替。</strong></p><h3 id="3、实战演练"><a href="#3、实战演练" class="headerlink" title="3、实战演练"></a>3、实战演练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> multiprocessing.pool <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载单个Ajax请求的结果。其中唯一变化的参数就是offset。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(offset)</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'autoload'</span>:<span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'count'</span>:<span class="string">'20'</span>,</span><br><span class="line">        <span class="string">'cur_tab'</span>:<span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'format'</span>:<span class="string">'json'</span>,</span><br><span class="line">        <span class="string">'keyword'</span>:<span class="string">'街拍'</span>,</span><br><span class="line">        <span class="string">'offset'</span>:offset</span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://www.toutiao.com/search_content/?'</span>+ urlencode(params) <span class="comment">#urlencode()方法构造请求的GET参数</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.json() <span class="comment"># 返回json格式数据</span></span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        print(<span class="string">'请求索引页出错'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#提取每条数据的image_list字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_images</span><span class="params">(json)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> json.get(<span class="string">'data'</span>): <span class="comment">#json中data存在</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json.get(<span class="string">'data'</span>):</span><br><span class="line">            title = item.get(<span class="string">'title'</span>)</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'image_list'</span>): <span class="comment">#item中image_list存在</span></span><br><span class="line">                images = item.get(<span class="string">'image_list'</span>)</span><br><span class="line">                <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">                    <span class="keyword">yield</span>&#123; <span class="comment">#构造生成器</span></span><br><span class="line">                        <span class="string">'image'</span>:<span class="string">'http:'</span>+ image[<span class="string">'url'</span>],</span><br><span class="line">                        <span class="string">'title'</span>:title</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">保存图片，其中item就是前面get_images()方法返回的一个字典。</span></span><br><span class="line"><span class="string">首先根据item的title来创建文件夹。</span></span><br><span class="line"><span class="string">然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。</span></span><br><span class="line"><span class="string">图片的名称可以使用其内容的MD5值，这样可以去除重复。                    </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_image</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(item.get(<span class="string">'title'</span>)): <span class="comment">#判断路径不存在</span></span><br><span class="line">        os.mkdir(item.get(<span class="string">'title'</span>)) <span class="comment">#用title创建文件夹</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(item.get(<span class="string">'image'</span>)) <span class="comment">#请求图片地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'</span>.format(item.get(<span class="string">'title'</span>),md5(response.content).hexdigest(),<span class="string">'jpg'</span>) <span class="comment">#定义图片路径，图片命名用图片内容MD5值16进制表示</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path): <span class="comment">#判断路径不存在</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f: </span><br><span class="line">                    f.write(response.content) <span class="comment">#写入二进制文件</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'Already Downloaded'</span>, file_path)</span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        print(<span class="string">'Failed to Save Image'</span>)</span><br><span class="line">    </span><br><span class="line">                    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    json = get_page(offset)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> get_images(json):</span><br><span class="line">        print(item)</span><br><span class="line">        save_image(item)</span><br><span class="line">        </span><br><span class="line">group_start = <span class="number">1</span></span><br><span class="line">group_end = <span class="number">5</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    groups = ([i*<span class="number">20</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(group_start,group_end+<span class="number">1</span>)]) <span class="comment">#构造一个offset数组</span></span><br><span class="line">    pool.map(main,groups) <span class="comment">#多进程进程池，调用其map()方法实现多进程下载</span></span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
