<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PythonFood</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://pythonfood.github.io/"/>
  <updated>2018-08-16T05:02:51.753Z</updated>
  <id>http://pythonfood.github.io/</id>
  
  <author>
    <name>Python Food</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Scrapy中Download Middleware的用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADDownload-Middleware%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Download-Middleware的用法/</id>
    <published>2018-07-05T07:00:00.000Z</published>
    <updated>2018-08-16T05:02:51.753Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、Downloader-Middleware中文文档0-25-0"><a href="#1、Downloader-Middleware中文文档0-25-0" class="headerlink" title="1、Downloader Middleware中文文档0.25.0"></a>1、Downloader Middleware中文文档0.25.0</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/downloader-middleware.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/downloader-middleware.html</a></p><h3 id="2、Downloader-Middleware下载器中间件"><a href="#2、Downloader-Middleware下载器中间件" class="headerlink" title="2、Downloader Middleware下载器中间件"></a>2、Downloader Middleware下载器中间件</h3><p>下载器中间件是介于Scrapy的request/response处理的钩子框架。 是用于全局修改Scrapy request和response的一个轻量、底层的系统。</p><h3 id="3、激活下载器中间件"><a href="#3、激活下载器中间件" class="headerlink" title="3、激活下载器中间件"></a>3、激活下载器中间件</h3><ul><li>settings中设置DOWNLOADER_MIDDLEWARES选项。 设置一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &apos;myproject.middlewares.CustomDownloaderMiddleware&apos;: 543,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>DOWNLOADER_MIDDLEWARES设置会与Scrapy定义的DOWNLOADER_MIDDLEWARES_BASE 设置合并(但不是覆盖)， 而后根据顺序(order)进行排序，最后得到启用中间件的有序列表: 第一个中间件是最靠近引擎的，最后一个中间件是最靠近下载器的。</p></li><li><p>如果想禁止内置的(在 DOWNLOADER_MIDDLEWARES_BASE 中设置并默认启用的)中间件， 必须在项目的 DOWNLOADER_MIDDLEWARES 设置中定义该中间件，并将其值赋为 None 。 例如，关闭user-agent中间件:  </p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &apos;myproject.middlewares.CustomDownloaderMiddleware&apos;: 543,</span><br><span class="line">    &apos;scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware&apos;: None,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>获得内置的DOWNLOADER_MIDDLEWARES_BASE中间件，命令行输入命令<code>scrapy settings --get=DOWNLOADER_MIDDLEWARES_BASE</code>，回显例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;: 100, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;: 300, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;: 350,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;: 400,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: 500,</span><br><span class="line"> &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;: 550, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;: 560, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;: 580, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 590, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;: 600, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;: 700, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;: 750, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;: 850, </span><br><span class="line"> &quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;: 900&#125;</span><br></pre></td></tr></table></figure><h3 id="4、编写下载器中间件"><a href="#4、编写下载器中间件" class="headerlink" title="4、编写下载器中间件"></a>4、编写下载器中间件</h3><ul><li><p>process_request(request, spider)</p></li><li><p>process_response(request, response, spider)</p></li><li><p>process_exception(request, exception, spider)</p></li></ul><p>详见官方文档</p><h3 id="5、其他内置的MIDDLEWARE"><a href="#5、其他内置的MIDDLEWARE" class="headerlink" title="5、其他内置的MIDDLEWARE"></a>5、其他内置的MIDDLEWARE</h3><p>详见官方文档</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中Item Pipeline的用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADItem-Pipeline%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Item-Pipeline的用法/</id>
    <published>2018-07-05T06:00:00.000Z</published>
    <updated>2018-08-16T05:05:49.875Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、Item-Pipeline中文文档0-24-0"><a href="#1、Item-Pipeline中文文档0-24-0" class="headerlink" title="1、Item Pipeline中文文档0.24.0"></a>1、Item Pipeline中文文档0.24.0</h3><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html</a></p><h3 id="2、Item-Pipeline的一些典型应用"><a href="#2、Item-Pipeline的一些典型应用" class="headerlink" title="2、Item Pipeline的一些典型应用"></a>2、Item Pipeline的一些典型应用</h3><ul><li><p>清理HTML数据</p></li><li><p>验证爬取的数据(检查item包含某些字段)</p></li><li><p>查重(并丢弃)</p></li><li><p>将爬取结果保存到数据库中</p></li></ul><h3 id="3、Item-Pipeline"><a href="#3、Item-Pipeline" class="headerlink" title="3、Item Pipeline"></a>3、Item Pipeline</h3><ul><li><p>process_item() 每个item pipeline组件都需要调用该方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem 异常。</p></li><li><p>open_spider() 当spider被开启时，这个方法被调用。</p></li><li><p>close_spider() 当spider被关闭时，这个方法被调用。</p></li><li><p>from_crawler() 可以获取项目settings中的一些配置信息。</p></li></ul><h3 id="4、Item-Pipeline样例"><a href="#4、Item-Pipeline样例" class="headerlink" title="4、Item Pipeline样例"></a>4、Item Pipeline样例</h3><ul><li>将item写入JSON文件：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">class JsonWriterPipeline(object):</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.file = open(&apos;items.jl&apos;, &apos;w&apos;)</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item)) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>将item写入MongoDB数据库：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">import pymongo</span><br><span class="line"></span><br><span class="line">class MongoPipeline(object):</span><br><span class="line"></span><br><span class="line">    collection_name = &apos;scrapy_items&apos;</span><br><span class="line"></span><br><span class="line">    def __init__(self, mongo_uri, mongo_db):</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>截取item网页截图：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">import hashlib</span><br><span class="line">from urllib.parse import quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ScreenshotPipeline(object):</span><br><span class="line">    &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span><br><span class="line">    every Scrapy item.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    SPLASH_URL = &quot;http://localhost:8050/render.png?url=&#123;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        encoded_item_url = quote(item[&quot;url&quot;])</span><br><span class="line">        screenshot_url = self.SPLASH_URL.format(encoded_item_url)</span><br><span class="line">        request = scrapy.Request(screenshot_url)</span><br><span class="line">        dfd = spider.crawler.engine.download(request, spider)</span><br><span class="line">        dfd.addBoth(self.return_item, item)</span><br><span class="line">        return dfd</span><br><span class="line"></span><br><span class="line">    def return_item(self, response, item):</span><br><span class="line">        if response.status != 200:</span><br><span class="line">            # Error happened, return item.</span><br><span class="line">            return item</span><br><span class="line"></span><br><span class="line">        # Save screenshot to file, filename will be hash of url.</span><br><span class="line">        url = item[&quot;url&quot;]</span><br><span class="line">        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()</span><br><span class="line">        filename = &quot;&#123;&#125;.png&quot;.format(url_hash)</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line"></span><br><span class="line">        # Store filename in item.</span><br><span class="line">        item[&quot;screenshot_filename&quot;] = filename</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure><ul><li>去重：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line">class DuplicatesPipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.ids_seen = set()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        if item[&apos;id&apos;] in self.ids_seen:</span><br><span class="line">            raise DropItem(&quot;Duplicate item found: %s&quot; % item)</span><br><span class="line">        else:</span><br><span class="line">            self.ids_seen.add(item[&apos;id&apos;])</span><br><span class="line">            return item</span><br></pre></td></tr></table></figure><h3 id="5、启用一个Item-Pipeline组件："><a href="#5、启用一个Item-Pipeline组件：" class="headerlink" title="5、启用一个Item Pipeline组件："></a>5、启用一个Item Pipeline组件：</h3><p>在settings的ITEM_PIPELINES选项里填写：pipline名称:数字优先级。通常将这些数字定义在0-1000范围内，数字越小优先级越高。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    &apos;myproject.pipelines.PricePipeline&apos;: 300,</span><br><span class="line">    &apos;myproject.pipelines.JsonWriterPipeline&apos;: 800,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中Spiders用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%ADSpiders%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中Spiders用法/</id>
    <published>2018-07-05T05:00:00.000Z</published>
    <updated>2018-08-16T05:05:00.989Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、scrapy中文文档"><a href="#1、scrapy中文文档" class="headerlink" title="1、scrapy中文文档"></a>1、scrapy中文文档</h3><p>文档地址：<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest</a></p><p>其中spider部分：<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/spiders.html</a></p><h3 id="2、Spider中文文档0-25-0里未翻译的部分简介"><a href="#2、Spider中文文档0-25-0里未翻译的部分简介" class="headerlink" title="2、Spider中文文档0.25.0里未翻译的部分简介"></a>2、Spider中文文档0.25.0里未翻译的部分简介</h3><ul><li>custom_settings 类里面的一个特定设置，可以覆盖settings里面的全局设置。以字典形式覆盖即可,例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">     DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">       &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</span><br><span class="line">       &apos;Accept-Language&apos;: &apos;en&apos;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>crawler 是Crawler类的一个实例对象，扩展访问scrapy核心组件和挂载scrapy的唯一途径。</p></li><li><p>settings 是Settings类的一个实例对象，可以控制包括核心插件、pipeline、其他spider组件的相应的配置。</p></li><li><p>from_crawler() 是一个类方法，在类里面定义这个方法，可以通过这个类拿到全局的settings的一些设置。例如：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, mongo_uri, mongo_db, *args, **kwargs):</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line">@classmethod</span><br><span class="line">def from_crawler(cls, crawler): </span><br><span class="line">        return cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</span><br><span class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DB&apos;)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><h3 id="3、Spider英文文档1-5-0中增加部分简介"><a href="#3、Spider英文文档1-5-0中增加部分简介" class="headerlink" title="3、Spider英文文档1.5.0中增加部分简介"></a>3、Spider英文文档1.5.0中增加部分简介</h3><ul><li>logger 可以直接调用logger输出日志，例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.logger.info(response.status)</span><br></pre></td></tr></table></figure><ul><li>Spider arguments 可以通过命令行<code>scrapy crawl myspider -a category=electronics</code>，指定category传入一些相应的额外参数。例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class MySpider(scrapy.Spider):</span><br><span class="line">    name = &apos;myspider&apos;</span><br><span class="line"></span><br><span class="line">    def __init__(self, category=None, *args, **kwargs):</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [&apos;http://www.example.com/categories/%s&apos; % category]</span><br><span class="line">        # ...</span><br></pre></td></tr></table></figure><h3 id="4、特定的spider"><a href="#4、特定的spider" class="headerlink" title="4、特定的spider"></a>4、特定的spider</h3><p>后面还提供了一些特定的spider，应用于一些定制化的需求。例如：CrawlSpider、XMLFeedSpider、CSVFeedSpider、SitemapSpider。</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy中选择器用法</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E4%B8%AD%E9%80%89%E6%8B%A9%E5%99%A8%E7%94%A8%E6%B3%95/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy中选择器用法/</id>
    <published>2018-07-05T04:00:00.000Z</published>
    <updated>2018-08-16T05:02:11.340Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、选择器是scrapy内置的一个selector对象"><a href="#1、选择器是scrapy内置的一个selector对象" class="headerlink" title="1、选择器是scrapy内置的一个selector对象"></a>1、选择器是scrapy内置的一个selector对象</h3><p>测试的网址：<a href="https://doc.scrapy.org/en/latest/_static/selectors-sample1.html" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/_static/selectors-sample1.html</a></p><h3 id="2、命令行模式测试"><a href="#2、命令行模式测试" class="headerlink" title="2、命令行模式测试"></a>2、命令行模式测试</h3><p><code>scrapy shell https://doc.scrapy.org/en/latest/_static/selectors-sample1.html</code>  # 进入交互模式</p><p><code>response.selector</code>  # scrapy内置的选择器的类</p><p><code>response.selector.xpath(&#39;//title/text()&#39;)</code>  # xpath()返回一个Selector的list</p><p><code>response.selector.css(&#39;title::text&#39;)</code>  # css()返回一个Selector的list</p><p><code>response.selector.xpath(&#39;//title/text()&#39;).extract_first()</code>  # extract_first()可以获取具体的内容</p><p><code>response.selector.css(&#39;title::text&#39;).extract()</code>  # extract可以获取具体内容的列表</p><p><code>response.xpath(&#39;//title/text()&#39;</code>  # 直接用.xpath简写模式，可以省去.selector</p><p><code>response.css(&#39;title::text&#39;)</code>  # .css简写模式</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;)</code>  # 1.首先用xpath定位到这个div标签</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img&#39;)</code>  # 2.然后用css获取img标签的list</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;)</code>  # 3.接着用::attr获取src属性的列表</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;).extract()</code>  # 4.最后用extract获取具体的属性值列表</p><p><code>response.xpath(&#39;//div[@id=&quot;images&quot;]&#39;).css(&#39;img::attr(src)&#39;).extract_first(defaut=&#39;&#39;)</code>  #用default设置默认值，查找不到的话用default代替</p><p><code>response.xpath(&#39;//a/@href&#39;).extract()</code>  # xpath获取属性值</p><p><code>response.css(&#39;a::attr(href)&#39;).extract()</code>  # css获取属性值</p><p><code>response.xpath(&#39;//a/text()&#39;).extract()</code>  # xpath获取内容值</p><p><code>response.css(&#39;a::text&#39;).extract()</code>  # css获取内容值</p><p><code>response.xpath(&#39;//a[contains(@href,&quot;image&quot;)]/@href&#39;).extract()</code>  # xpath查找所有a标签下href属性名称包含”image”的超链接值</p><p><code>response.css(&#39;a[href*=image]::attr(href)&#39;).extract()</code>  #  css查找所有a标签下href属性名称包含”image”的超链接值</p><p><code>response.xpath(&#39;//a[contains(@href,&quot;image&quot;)]/img/@src&#39;).extract()</code>  # xpath查找所有a标签(href属性名称包含”image”)下img标签下src属性值</p><p><code>response.css(&#39;a[href*=image] img::attr(src)&#39;).extract()</code>  #  css查找所有a标签(href属性名称包含”image”)下img标签下src属性值</p><p><code>response.css(&#39;a::text&#39;).re(&#39;Name\:(.*)&#39;)</code>  # 通过re正则匹配，返回的是（.*）里的内容，‘Name\:’是匹配的起始位置</p><p><code>response.css(&#39;a::text&#39;).re_first(&#39;Name\:(.*)&#39;).strip()</code>  # rere_first返回正则匹配的第一个值，strip()去除前后的空格          </p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy命令行详解</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%AF%A6%E8%A7%A3/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy命令行详解/</id>
    <published>2018-07-05T03:00:00.000Z</published>
    <updated>2018-08-16T02:58:55.552Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、命令行详解"><a href="#一、命令行详解" class="headerlink" title="一、命令行详解"></a>一、命令行详解</h2><h3 id="1、Scrapy官方文档"><a href="#1、Scrapy官方文档" class="headerlink" title="1、Scrapy官方文档"></a>1、Scrapy官方文档</h3><p><a href="https://doc.scrapy.org/en/latest/" target="_blank" rel="noopener">https://doc.scrapy.org/en/latest/</a></p><h3 id="2、命令行详解"><a href="#2、命令行详解" class="headerlink" title="2、命令行详解"></a>2、命令行详解</h3><p><code>scrapy startproject testproject</code>  # 新建项目</p><p><code>cd testproject</code></p><p><code>scrapy genspider baidu www.baidu.com</code>  # 生成spider</p><p><code>cd testproject</code></p><p><code>cd spiders</code></p><p><code>scrapy genspider -l</code>  # 列出所有的模板</p><p><code>scrapy genspider -t crawl zhihu www.zhihu.com</code>  # 指定crwal模板，生成spider</p><p><code>scrapy crawl zhihu</code>  # 运行爬虫</p><p><code>scrapy crawl baidu</code></p><p><code>scrapy check</code>  # 检查代码错误</p><p><code>scrapy list</code>  # 列出项目所有的spider</p><p><code>scrapy edit zhihu</code>  # 在命令行下编辑spider</p><p><code>cd ../../../</code></p><p><code>scrapy fetch http://www.baidu.com</code>  # download一个网页，返回网页源代码</p><p><code>scrapy fetch --nolog http://www.baidu.com</code>  # 返回网页源代码时，不会显示日志信息</p><p><code>scrapy fetch --headers http://www.baidu.com</code>  # 返回网页源代码时，返回headers信息    </p><p><code>scrapy fetch --no-redirect http://www.baidu.com</code>  # 请求网页时，不能重定向</p><p><code>scrapy view http://www.baidu.com</code>  # 请求网页，生成document下载下来，并用浏览器自动打开</p><p><code>scrapy view http://www.taobao.com</code>  # 会发现淘宝的网页很多内容不显示，因为都是用ajax加载的</p><p><code>scrapy shell https://www.baidu.com</code>  # 进入命令行交互模式</p><ul><li><p><code>request</code>  # 交互模式下进行操作</p></li><li><p><code>reponse.text</code>  # 交互模式下进行操作</p></li><li><p><code>reponse.headers</code>  # 交互模式下进行操作</p></li><li><p><code>response.css(&#39;title::text&#39;).extract_first()</code>  # 交互模式下进行操作</p></li><li><p><code>view(response)</code>  # 交互模式下进行操作</p></li><li><p><code>exit()</code>  # 退出交互模式</p></li></ul><p><code>cd D:\PycharmProjects\quotetutroial</code></p><p><code>scrapy parse http://quotes.toscrape.com --callback parse</code>  # parse方法，传入url，指定参数，看下解析结果</p><p><code>scrapy settings --get MONGO_URI</code>  # settings方法，获取一些配置信息</p><p><code>scrapy settings -h</code>  # 获取一些帮助信息</p><p><code>scrapy settings --getbool=ROBOTSTXT_OBEY</code>  # 验证下是否遵循ROBOTSTXT_OBEY这个规则</p><p><code>scrapy runspider quotes.py</code>  # 运行spider，注意文件名带.py</p><p><code>scrapy version</code>  # 输出scrapy版本</p><p><code>scrapy version -v</code>  # 输出一些依赖库的版本</p><p><code>scrapy bench</code>  # 测试下当前的爬行速度</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架基本使用</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy框架基本使用/</id>
    <published>2018-07-05T02:00:00.000Z</published>
    <updated>2018-08-16T02:58:41.579Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、基本使用"><a href="#一、基本使用" class="headerlink" title="一、基本使用"></a>一、基本使用</h2><h3 id="1、目标站点"><a href="#1、目标站点" class="headerlink" title="1、目标站点"></a>1、目标站点</h3><p><a href="http://quotes.toscrape.com/" target="_blank" rel="noopener">http://quotes.toscrape.com/</a></p><h3 id="2、流程框架"><a href="#2、流程框架" class="headerlink" title="2、流程框架"></a>2、流程框架</h3><p>(1)抓取第一页</p><p>请求第一页URL并得到源代码，进行下一步分析。</p><p>(2)获取内容和下一页链接</p><p>分析源代码，获取首页内容，获取下一页链接等待进一步爬取。</p><p>(3)翻页爬取</p><p>请求下一页信息，分析内容并请求再下一页链接。</p><p>(4)保存爬取结果</p><p>将爬取结果保存为特定格式如文本、数据库。</p><h3 id="3、爬虫实战"><a href="#3、爬虫实战" class="headerlink" title="3、爬虫实战"></a>3、爬虫实战</h3><p>命令行输入：</p><p>新建工程：<code>scrapy startproject quotetutroial</code></p><p>进入目录：<code>cd quotetutroial</code></p><p>生成爬虫：<code>scrapy genspider quotes quotes.toscrape.com</code></p><blockquote><p>项目结构：</p><ul><li><p>scrapy.cfg 配置文件</p></li><li><p>items.py 保存数据的数据结构</p></li><li><p>middlewares.py 爬取过程中定义的一些中间件</p></li><li><p>pipelines.py 项目管道，可以输出一些items</p></li><li><p>settings.py 定义的一些配置信息</p></li><li><p>spider/quotes.py 主要的运行代码</p></li></ul></blockquote><p>运行爬虫：<code>scrapy crawl quotes</code></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架安装</title>
    <link href="http://pythonfood.github.io/2018/07/05/Scrapy%E6%A1%86%E6%9E%B6%E5%AE%89%E8%A3%85/"/>
    <id>http://pythonfood.github.io/2018/07/05/Scrapy框架安装/</id>
    <published>2018-07-05T01:00:00.000Z</published>
    <updated>2018-08-16T02:58:28.473Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、框架安装"><a href="#一、框架安装" class="headerlink" title="一、框架安装"></a>一、框架安装</h2><h3 id="1、Scrapy安装"><a href="#1、Scrapy安装" class="headerlink" title="1、Scrapy安装"></a>1、Scrapy安装</h3><p>scrapy依赖的库比较多，至少需要依赖库有pywin32、Twisted、pyOpenSSL、lxml。而在不同平台环境又各不相同，所以安装之前最好确保依赖库安装好，尤其是windows。</p><h3 id="2、windows"><a href="#2、windows" class="headerlink" title="2、windows"></a>2、windows</h3><p>最好的安装方式是通过wheel文件来安装。首先需要安装wheel库：</p><p><code>pip install wheel</code></p><p>(1)安装lxml</p><p>进入<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\lxml‑3.7.2‑cp36‑cp36m‑win32.whl</code></p><p>(2)安装OpenSSL</p><p>进入<a href="https://pypi.python.org/pypi/pyOpenSSL#downloads" target="_blank" rel="noopener">https://pypi.python.org/pypi/pyOpenSSL#downloads</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\pyOpenSSL-16.2.0-py2.py3-none-any.whl</code></p><p>(3)安装Twisted</p><p>进入<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a> ，找到与python版本和系统版本对应的whl文件，下载后通过pip安装：</p><p><code>pip install C:\Users\tester\Downloads\Twisted‑17.1.0‑cp36‑cp36m‑win32.whl</code></p><p>(4)安装pywin32</p><p>进入<a href="https://sourceforge.net/projects/pywin32/files/pywin32/Build%20220/" target="_blank" rel="noopener">https://sourceforge.net/projects/pywin32/files/pywin32/Build%20220/</a> ，找到与python版本和系统版本对应的安装包，运行安装即可。</p><p>(5)安装Scrapy</p><p>最后安装Scrapy，pip安装即可：</p><p><code>pip install Scrapy</code></p><h3 id="3、Anaconda安装"><a href="#3、Anaconda安装" class="headerlink" title="3、Anaconda安装"></a>3、Anaconda安装</h3><p>可以轻松的使用conda命令安装Scrapy：</p><p><code>conda install Scrapy</code></p><h3 id="4、安装验证"><a href="#4、安装验证" class="headerlink" title="4、安装验证"></a>4、安装验证</h3><p>安装完，cmd输入命令<code>scarpy</code>可以验证。</p><p>进一步创建一个工程：</p><p><code>scrapy startproject hello</code></p><p><code>cd hello</code></p><p><code>scrapy genspider baidu www.baidu.com</code></p><p><code>scrapy crawl baidu</code></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Pyspider架构概述及用法详解</title>
    <link href="http://pythonfood.github.io/2018/07/04/Pyspider%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0%E5%8F%8A%E7%94%A8%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
    <id>http://pythonfood.github.io/2018/07/04/Pyspider架构概述及用法详解/</id>
    <published>2018-07-04T02:00:00.000Z</published>
    <updated>2018-08-16T02:27:23.585Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、官方详解"><a href="#一、官方详解" class="headerlink" title="一、官方详解"></a>一、官方详解</h2><h3 id="1、官方文档"><a href="#1、官方文档" class="headerlink" title="1、官方文档"></a>1、官方文档</h3><p><a href="http://docs.pyspider.org/en/latest/" target="_blank" rel="noopener">http://docs.pyspider.org/en/latest/</a></p><h3 id="2、中文翻译"><a href="#2、中文翻译" class="headerlink" title="2、中文翻译"></a>2、中文翻译</h3><p><a href="http://www.pyspider.cn/book/pyspider/pyspider-Quickstart-2.html" target="_blank" rel="noopener">http://www.pyspider.cn/book/pyspider/pyspider-Quickstart-2.html</a></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>PySpider框架基本使用及抓取TripAdvisor实战</title>
    <link href="http://pythonfood.github.io/2018/07/04/PySpider%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%8A%93%E5%8F%96TripAdvisor%E5%AE%9E%E6%88%98/"/>
    <id>http://pythonfood.github.io/2018/07/04/PySpider框架基本使用及抓取TripAdvisor实战/</id>
    <published>2018-07-04T01:00:00.000Z</published>
    <updated>2018-08-16T02:26:36.285Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、PySpider框架"><a href="#1、PySpider框架" class="headerlink" title="1、PySpider框架"></a>1、PySpider框架</h3><ul><li>去重处理</li><li>结果监控</li><li>多进程处理</li><li>PyQuery提取</li><li>错误重试</li><li>WebUI管理</li><li>代码简洁</li><li>JavaScript渲染</li></ul><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p>cmd管理员身份运行，执行命令<code>pip install pyspider</code></p><h3 id="3、运行"><a href="#3、运行" class="headerlink" title="3、运行"></a>3、运行</h3><p>cmd输入命令<code>pyspider</code>。浏览器输入地址：<a href="http://localhost:5000/" target="_blank" rel="noopener">http://localhost:5000/</a></p><p>ps:需要安装过phantomjs</p><h3 id="4、实战"><a href="#4、实战" class="headerlink" title="4、实战"></a>4、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- encoding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># Created on 2018-06-07 22:43:08</span></span><br><span class="line"><span class="comment"># Project: TripAdvisor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspider.libs.base_handler <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Handler</span><span class="params">(BaseHandler)</span>:</span></span><br><span class="line">    crawl_config = &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    client = pymongo.MongoClient(<span class="string">'localhost'</span>)</span><br><span class="line">    db = client[<span class="string">'trip'</span>]</span><br><span class="line"><span class="meta">    @every(minutes=24 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_start</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.crawl(<span class="string">'http://www.tripadvisor.cn/Attractions-g186338-Activities-c47-t163-London_England.html#ATTRACTION_LIST'</span>, callback=self.index_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(age=10 * 24 * 60 * 60)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> response.doc(<span class="string">'.listing_title &gt; a'</span>).items():</span><br><span class="line">            self.crawl(each.attr.href, callback=self.detail_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line">            next = response.doc(<span class="string">'.pagination .nav.next'</span>).attr.href</span><br><span class="line">            self.crawl(next, callback=self.index_page, validate_cert=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @config(priority=2)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail_page</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        name = response.doc(<span class="string">'.heading_title'</span>).text()</span><br><span class="line">        rating = response.doc(<span class="string">'div &gt; .more'</span>).text()</span><br><span class="line">        adress = response.doc(<span class="string">'.location &gt; .address'</span>).text()</span><br><span class="line">        phone = response.doc(<span class="string">'.phone &gt; div'</span>).text()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"url"</span>:response.url,</span><br><span class="line">            <span class="string">"name"</span>:name,</span><br><span class="line">            <span class="string">"rating"</span>:rating,</span><br><span class="line">            <span class="string">"adress"</span>:adress,</span><br><span class="line">            <span class="string">"phone"</span>:phone</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_result</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            self.save_to_mongo(result)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(self,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.db[<span class="string">'london'</span>].insert(result):</span><br><span class="line">            print(<span class="string">'save to mongodb:'</span>,result)</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Redis+Flask维护动态Cookies池</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Redis-Flask%E7%BB%B4%E6%8A%A4%E5%8A%A8%E6%80%81Cookies%E6%B1%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Redis-Flask维护动态Cookies池/</id>
    <published>2018-07-03T05:00:00.000Z</published>
    <updated>2018-08-15T12:27:01.678Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、Cookies池详解"><a href="#一、Cookies池详解" class="headerlink" title="一、Cookies池详解"></a>一、Cookies池详解</h2><h3 id="1、为什么要维护cookie池"><a href="#1、为什么要维护cookie池" class="headerlink" title="1、为什么要维护cookie池"></a>1、为什么要维护cookie池</h3><p>有的网站需要登录后才能爬取，如新浪微博</p><p>爬取过程中如果频率过高会导致封号，那么如果想要获得非常多的数据，则需要非常多的账号</p><h3 id="2、cookie池的要求"><a href="#2、cookie池的要求" class="headerlink" title="2、cookie池的要求"></a>2、cookie池的要求</h3><ul><li>自动登录更新</li><li>定时验证筛选</li><li>提供外部接口（可将池架在远程的服务器上，实现远程部署</li></ul><h3 id="3、cookie池的架构"><a href="#3、cookie池的架构" class="headerlink" title="3、cookie池的架构"></a>3、cookie池的架构</h3><p>账号队列 ===》 生成器 ===》 cookies队列（对外提供API） ===》 定时检测器</p><h3 id="4、cookie池的实现"><a href="#4、cookie池的实现" class="headerlink" title="4、cookie池的实现"></a>4、cookie池的实现</h3><p>需要先将一定量的账号密码之类的cookie存进Redis数据库，然后利用python调用并维护。</p><p>关于cookies池的维护，有以下开源项目案例可供参考：<a href="https://github.com/Germey/CookiesPool" target="_blank" rel="noopener">https://github.com/Germey/CookiesPool</a></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用代理处理反爬抓取微信文章</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E5%A4%84%E7%90%86%E5%8F%8D%E7%88%AC%E6%8A%93%E5%8F%96%E5%BE%AE%E4%BF%A1%E6%96%87%E7%AB%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用代理处理反爬抓取微信文章/</id>
    <published>2018-07-03T04:00:00.000Z</published>
    <updated>2018-08-15T13:05:59.205Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、流程框架"><a href="#1、流程框架" class="headerlink" title="1、流程框架"></a>1、流程框架</h3><p>(1)抓取索引页内容</p><p>利用requests请求目标站点，得到索引网页HTML代码，返回结果。</p><p>(2)代理设置</p><p>如果遇到302状态码，则证明IP被封，切换代理重试。</p><p>(3)分析详情页内容</p><p>请求详情页，分析得到标题，正文等内容。</p><p>(4)将数据保存到数据库</p><p>将结构化数据保存到MongoDB。</p><h3 id="2、实战"><a href="#2、实战" class="headerlink" title="2、实战"></a>2、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ConnectionError</span><br><span class="line"><span class="keyword">from</span> lxml.etree <span class="keyword">import</span> XMLSyntaxError</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> wxconfig <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据库连接</span></span><br><span class="line">client = pymongo.MongoClient(MONGO_URI)</span><br><span class="line">db = client[MONGO_DB]</span><br><span class="line"><span class="comment"># 爬取url</span></span><br><span class="line">base_url = <span class="string">'http://weixin.sogou.com/weixin?'</span></span><br><span class="line"><span class="comment"># 请求头，需要带上cookie，不然只能访问10页数据</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'Cookie'</span>: <span class="string">'SUID=F6177C7B3220910A000000058E4D679; SUV=1491392122762346; ABTEST=1|1491392129|v1; SNUID=0DED8681FBFEB69230E6BF3DFB2F8D6B; ld=OZllllllll2Yi2balllllV06C77lllllWTZgdkllll9lllllxv7ll5@@@@@@@@@@; LSTMV=189%2C31; LCLKINT=1805; weixinIndexVisited=1; SUIR=0DED8681FBFEB69230E6BF3DFB2F8D6B; JSESSIONID=aaa-BcHIDk9xYdr4odFSv; PHPSESSID=afohijek3ju93ab6l0eqeph902; sct=21; IPLOC=CN; ppinf=5|1491580643|1492790243|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZToyNzolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOER8Y3J0OjEwOjE0OTE1ODA2NDN8cmVmbmljazoyNzolRTUlQjQlOTQlRTUlQkElODYlRTYlODklOER8dXNlcmlkOjQ0Om85dDJsdUJfZWVYOGRqSjRKN0xhNlBta0RJODRAd2VpeGluLnNvaHUuY29tfA; pprdig=j7ojfJRegMrYrl96LmzUhNq-RujAWyuXT_H3xZba8nNtaj7NKA5d0ORq-yoqedkBg4USxLzmbUMnIVsCUjFciRnHDPJ6TyNrurEdWT_LvHsQIKkygfLJH-U2MJvhwtHuW09enCEzcDAA_GdjwX6_-_fqTJuv9w9Gsw4rF9xfGf4; sgid=; ppmdig=1491580643000000d6ae8b0ebe76bbd1844c993d1ff47cea'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'weixin.sogou.com'</span>,</span><br><span class="line">    <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 开始时是否启用代理</span></span><br><span class="line">proxy = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 代理获取函数，这里使用类flask的接口</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(PROXY_POOL_URL)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="comment"># 没有异常直接结束</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url, count=<span class="number">1</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'Crawling'</span>, url)</span><br><span class="line">    print(<span class="string">'Try Count'</span>, count)</span><br><span class="line">    <span class="keyword">global</span> proxy</span><br><span class="line">    <span class="comment">#设置访问深度</span></span><br><span class="line">    <span class="keyword">if</span> count &gt;= MAX_COUNT:</span><br><span class="line">        print(<span class="string">'Tried Too Many Counts'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 判断启用代理和有代理</span></span><br><span class="line">        <span class="keyword">if</span> proxy:</span><br><span class="line">            <span class="comment"># 给代理池取出的数据加上协议头</span></span><br><span class="line">            proxies = &#123;</span><br><span class="line">                <span class="string">'http'</span>:<span class="string">'http://'</span> + proxy</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment"># 需要设置allow_redirects=False,禁止requests自动处理重定向</span></span><br><span class="line">            response = requests.get(url, headers=headers, proxies=proxies, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            response = requests.get(url, headers=headers, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line">        <span class="comment"># 如果请求成功返回    </span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">302</span>:</span><br><span class="line">            <span class="comment"># 出现302说明IP被封，需要更换代理</span></span><br><span class="line">            print(<span class="string">'302'</span>)</span><br><span class="line">            <span class="comment"># 获取一个新代理</span></span><br><span class="line">            proxy = get_proxy()</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                print(<span class="string">'Using Proxy'</span>, proxy)</span><br><span class="line">                 <span class="comment"># 获取到代理ip重新获取网页</span></span><br><span class="line">                <span class="keyword">return</span> get_html(url)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'Get Proxy Failed'</span>)</span><br><span class="line">                <span class="comment"># 没有代理了，直接退出</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">'Error Occurred'</span>, e.args)</span><br><span class="line">        proxy = get_proxy()</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> get_html(url, count)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_index</span><span class="params">(keyword,page)</span>:</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">'query'</span>:keyword,</span><br><span class="line">        <span class="string">'type'</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="string">'page'</span>:page</span><br><span class="line">    &#125;</span><br><span class="line">    queries = urlencode(data)</span><br><span class="line">    url = base_url + queries</span><br><span class="line">    html = get_html(url)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_index</span><span class="params">(html)</span>:</span></span><br><span class="line">    doc = pq(html)</span><br><span class="line">    items = doc(<span class="string">'.news-box .news-list li .txt-box h3 a'</span>).items()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span> item.attr(<span class="string">'href'</span>)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_detail</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.text</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">except</span> ConnectionError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(html)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        doc = pq(html)</span><br><span class="line">        title = doc(<span class="string">'.rich_media_title'</span>).text()</span><br><span class="line">        content = doc(<span class="string">'.rich_media_content'</span>).text()</span><br><span class="line">        date = doc(<span class="string">'#post-date'</span>).text()</span><br><span class="line">        nickname = doc(<span class="string">'#js_profile_qrcode &gt; div &gt; strong'</span>).text()</span><br><span class="line">        wechat = doc(<span class="string">'#js_profile_qrcode &gt; div &gt; p:nth-child(3) &gt; span'</span>).text()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: title,</span><br><span class="line">            <span class="string">'content'</span>: content,</span><br><span class="line">            <span class="string">'date'</span>: date,</span><br><span class="line">            <span class="string">'nickname'</span>: nickname,</span><br><span class="line">            <span class="string">'wechat'</span>: wechat</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">except</span> XMLSyntaxError:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> db[<span class="string">'articles'</span>].update(&#123;<span class="string">'title'</span>:data[<span class="string">'title'</span>]&#125;,&#123;<span class="string">'$set'</span>:data&#125;,<span class="keyword">True</span>):</span><br><span class="line">        print(<span class="string">'Saved to Mongo'</span>, data[<span class="string">'title'</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'Saved to Mongo Failed'</span>, data[<span class="string">'title'</span>])</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 设置访问的页数和次数</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 传入访问的关键字</span></span><br><span class="line">        html = get_index(KEYWORD, page)</span><br><span class="line">        <span class="comment"># 获取到html文件，获取链接</span></span><br><span class="line">        <span class="keyword">if</span> html:</span><br><span class="line">            article_urls = parse_index(html)</span><br><span class="line">            <span class="keyword">for</span> article_url <span class="keyword">in</span> article_urls:</span><br><span class="line">                article_html = get_detail(article_url)</span><br><span class="line">                <span class="keyword">if</span> article_html:</span><br><span class="line">                    <span class="comment"># 解析文章内容</span></span><br><span class="line">                    article_data = parse_detail(article_html)</span><br><span class="line">                    print(article_data)</span><br><span class="line">                    <span class="comment"># 保存到数据库</span></span><br><span class="line">                    <span class="keyword">if</span> article_data:</span><br><span class="line">                        save_to_mongo(article_data)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>wxconfig.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line">PROXY_POOL_URL = <span class="string">'http://127.0.0.1:5000/get'</span></span><br><span class="line">KEYWORD = <span class="string">'风景'</span></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'weixin'</span></span><br><span class="line">MAX_COUNT = <span class="number">5</span></span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Redis+Flask维护动态代理池</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Redis-Flask%E7%BB%B4%E6%8A%A4%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%B1%A0/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Redis-Flask维护动态代理池/</id>
    <published>2018-07-03T03:00:00.000Z</published>
    <updated>2018-08-15T13:06:45.676Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、代理池详解"><a href="#一、代理池详解" class="headerlink" title="一、代理池详解"></a>一、代理池详解</h2><h3 id="1、代理池的维护"><a href="#1、代理池的维护" class="headerlink" title="1、代理池的维护"></a>1、代理池的维护</h3><p>目前有很多网站提供免费代理，而且种类齐全，比如各个地区、各个匿名级别的都有，不过质量实在不敢恭维，毕竟都是免费公开的，可能一个代理无数个人在用也说不定。所以我们需要做的是大量抓取这些免费代理，然后筛选出其中可用的代理存储起来供我们使用，不可用的进行剔除。</p><h3 id="2、获取代理途径"><a href="#2、获取代理途径" class="headerlink" title="2、获取代理途径"></a>2、获取代理途径</h3><p>维护一个代理池第一步就是要找到提供免费代理的站点，例如PROXY360。可以看到网页里提供了一些免费代理列表，包括服务器地址、端口、代理种类、地区、更新时间等等信息。</p><p>当前我们需要的就是代理服务器和端口信息，将其爬取下来即可。</p><h3 id="3、代理池的要求"><a href="#3、代理池的要求" class="headerlink" title="3、代理池的要求"></a>3、代理池的要求</h3><ul><li>多占抓取，异步检测</li><li>定时筛选，持续更新</li><li>提供接口，易于获取</li></ul><h3 id="4、代理池的架构"><a href="#4、代理池的架构" class="headerlink" title="4、代理池的架构"></a>4、代理池的架构</h3><p>Internet  ===》  代理获取器  ===》  代理筛选器  ===》  代理调度器（对外提供API）  ===》  定时检测器</p><h3 id="5、维护代理"><a href="#5、维护代理" class="headerlink" title="5、维护代理"></a>5、维护代理</h3><p>那么爬取下代理之后怎样保存呢？</p><p>首先我们需要确保的目标是可以边取边存，另外还需要定时检查队列中不可用的代理将其剔除，所以需要易于存取。</p><p>另外怎样区分哪些是最新的可用的，哪些是旧的，如果用修改时间来标注是可以的，不过更简单的方法就是维护一个队列，只从一端存入，例如右端，这样就能确保最新的代理在队列右端，而在左端则是存入时间较长的代理，如果要取一个可用代理，从队列右端取一个就好了。</p><p>那么对于队列的左端，不能让它一直老化下去，还需要做的操作就是定时从队列左端取出代理，然后进行检测，如果可用，重新将其加入右端。</p><p>通过以上操作，就保证了代理一直是最新可用的。</p><p>所以目前来看，既能高效处理，又可以做到队列动态维护，合适的方法就是利用Redis数据库的队列。</p><p>可以定义一个类来维护一个Redis队列，比如get方法是批量从左端取出代理，put方法是从右端放入可用代理，pop方法是从右端取出最新可用代理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">from</span> proxypool.error <span class="keyword">import</span> PoolEmptyError</span><br><span class="line"><span class="keyword">from</span> proxypool.setting <span class="keyword">import</span> HOST, PORT</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisClient</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, host=HOST, port=PORT)</span>:</span></span><br><span class="line">        self._db = redis.Redis(host, port)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, count=<span class="number">1</span>)</span>:</span></span><br><span class="line">        proxies = self._db.lrange(<span class="string">"proxies"</span>, <span class="number">0</span>, count - <span class="number">1</span>)</span><br><span class="line">        self._db.ltrim(<span class="string">"proxies"</span>, count, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> proxies</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span><span class="params">(self, proxy)</span>:</span></span><br><span class="line">        self._db.rpush(<span class="string">"proxies"</span>, proxy)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self._db.rpop(<span class="string">"proxies"</span>).decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">raise</span> PoolEmptyError</span><br></pre></td></tr></table></figure><h3 id="6、检测代理"><a href="#6、检测代理" class="headerlink" title="6、检测代理"></a>6、检测代理</h3><p>那么如何来检测代理是否可用？可以使用这个代理来请求某个站点，比如百度，如果获得正常的返回结果，那证明代理可用，否则代理不可用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conn = RedisClient()</span><br><span class="line">proxies = &#123;<span class="string">'http'</span>: proxy&#125;</span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com'</span>, proxies=proxies)</span><br><span class="line"><span class="keyword">if</span> r.status_code == <span class="number">200</span>:</span><br><span class="line">    conn.put(proxy)</span><br></pre></td></tr></table></figure><h3 id="7、获取可用代理"><a href="#7、获取可用代理" class="headerlink" title="7、获取可用代理"></a>7、获取可用代理</h3><p>现在我们维护了一个代理池，那么这个代理池需要是可以公用的。</p><p>比如现在有多个爬虫项目都需要用到代理，而代理池的维护作为另外的一个项目，他们之间如果要建立连接，最恰当的方式就是接口。</p><p>所以可以利用Web服务器来实现一个接口，其他的项目通过请求这个接口得到内容获取到一个可用代理，这样保证了代理池的通用性。</p><p>所以要实现这个还需要一个Web服务器，例如Flask，Tornado等等。</p><p>例如使用Flask，定义一个路由，然后调用的RedisClient的pop方法，返回结果即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@app.route('/')</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    conn = RedisClient()</span><br><span class="line">    <span class="keyword">return</span> conn.pop()</span><br></pre></td></tr></table></figure><p>这样一来，整个程序运行起来后，浏览器输入localhost:5000/get，请求网页就可以看到一个可用代理了。</p><h3 id="8、使用代理"><a href="#8、使用代理" class="headerlink" title="8、使用代理"></a>8、使用代理</h3><p>使用代理时只需要请求这个站点，就可以拿到可使用的代理了。</p><p>可以定义一个简单的方法，返回网页内容即代理，然后在爬取方法里设置代理使用即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_proxy</span><span class="params">()</span>:</span></span><br><span class="line">    r = requests.get(<span class="string">'http://127.0.0.1:5000/get'</span>)</span><br><span class="line">    <span class="keyword">return</span> r.text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl</span><span class="params">(url, proxy)</span>:</span></span><br><span class="line">    proxies = &#123;<span class="string">'http'</span>: get_proxy()&#125;</span><br><span class="line">    r = requests.get(url, proxies=proxies)</span><br><span class="line">    <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><h3 id="9、github项目源代码"><a href="#9、github项目源代码" class="headerlink" title="9、github项目源代码"></a>9、github项目源代码</h3><p><a href="https://github.com/Germey/ProxyPool" target="_blank" rel="noopener">https://github.com/Germey/ProxyPool</a></p><p>PS:项目requirements.txt文件中缺少一个依赖库fake-useragent，需要手动pip安装。</p><p>还有如果报错’TypeError: expected string or bytes-like object’,找到对应的代码，是一个爬取代理网站的方法，删除或屏蔽掉就可以了。</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>分析Ajax爬取今日头条街拍美图</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E5%88%86%E6%9E%90Ajax%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1%E8%A1%97%E6%8B%8D%E7%BE%8E%E5%9B%BE/"/>
    <id>http://pythonfood.github.io/2018/07/03/分析Ajax爬取今日头条街拍美图/</id>
    <published>2018-07-03T02:00:00.000Z</published>
    <updated>2018-08-15T03:36:41.354Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h3><p>请确保已经安装好requests库，使用的编辑器是PyCharm。</p><h3 id="2、抓取分析"><a href="#2、抓取分析" class="headerlink" title="2、抓取分析"></a>2、抓取分析</h3><p>(1)抓取前分析抓取对象：打开今日头条首页<a href="https://www.toutiao.com/" target="_blank" rel="noopener">https://www.toutiao.com/</a></p><p>(2)右上角输入“街拍”，搜索进入街拍。</p><p>(3)这时打开开发者工具，查看所有的网络请求。首先，打开第一个网络请求，这个请求的URL就是当前的链接<a href="http://www.toutiao.com/search/?keyword=街拍" target="_blank" rel="noopener">http://www.toutiao.com/search/?keyword=街拍</a> ，打开Preview选项卡查看Response Body。如果页面中的内容是根据第一个请求得到的结果渲染出来的，那么第一个请求的源代码中必然会包含页面结果中的文字。</p><p>(4)实际没有结果中的文字，可以初步判断这些内容是由Ajax加载，然后用JavaScript渲染出来的。接下来，我们可以切换到XHR过滤选项卡，查看一下有没有Ajax请求。不出所料，此处出现了一个比较常规的Ajax请求，看看它的结果是否包含了页面中的相关数据。   </p><p>(5)点击data字段展开，发现这里有许多条数据。点击第一条展开，可以发现有一个title字段，它的值正好就是页面中第一条数据的标题。再检查一下其他数据，也正好是一一对应的。</p><p>(6)我们的目的是要抓取其中的美图，这里一组图就对应前面data字段中的一条数据。每条数据还有一个image_detail字段，它是列表形式，这其中就包含了组图的所有图片列表,因此，我们只需要将列表中的url字段提取出来并下载下来就好了。每一组图都建立一个文件夹，文件夹的名称就为组图的标题。</p><p>(7)接下来，就可以直接用Python来模拟这个Ajax请求，然后提取出相关美图链接并下载。但是在这之前，我们还需要分析一下URL的规律。切换回Headers选项卡，观察一下它的请求URL和Headers信息。</p><p>(8)可以看到，这是一个GET请求，请求URL的参数有offset、format、keyword、autoload、count和cur_tab。我们需要找出这些参数的规律，因为这样才可以方便地用程序构造出来。</p><p>(9)接下来，可以滑动页面，多加载一些新结果。在加载的同时可以发现，Network中又出现了许多Ajax请求，这里观察一下后续链接的参数，发现变化的参数只有offset，其他参数都没有变化，而且第二次请求的offset值为20，第三次为40，第四次为60，所以可以发现规律，这个offset值就是偏移量，进而可以推断出count参数就是一次性获取的数据条数。因此，我们可以用offset参数来控制数据分页。这样一来，我们就可以通过接口批量获取数据了，然后将数据解析，将图片下载下来即可。</p><p><strong>PS：崔老师教程的第(6)步中查找的字段image_detail，实际上没有发现，这里使用image_list代替。</strong></p><h3 id="3、实战演练"><a href="#3、实战演练" class="headerlink" title="3、实战演练"></a>3、实战演练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"><span class="keyword">from</span> multiprocessing.pool <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载单个Ajax请求的结果。其中唯一变化的参数就是offset。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page</span><span class="params">(offset)</span>:</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'autoload'</span>:<span class="string">'true'</span>,</span><br><span class="line">        <span class="string">'count'</span>:<span class="string">'20'</span>,</span><br><span class="line">        <span class="string">'cur_tab'</span>:<span class="string">'1'</span>,</span><br><span class="line">        <span class="string">'format'</span>:<span class="string">'json'</span>,</span><br><span class="line">        <span class="string">'keyword'</span>:<span class="string">'街拍'</span>,</span><br><span class="line">        <span class="string">'offset'</span>:offset</span><br><span class="line">    &#125;</span><br><span class="line">    url = <span class="string">'http://www.toutiao.com/search_content/?'</span>+ urlencode(params) <span class="comment">#urlencode()方法构造请求的GET参数</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url)</span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> response.json() <span class="comment"># 返回json格式数据</span></span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        print(<span class="string">'请求索引页出错'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#提取每条数据的image_list字段中的每一张图片链接，将图片链接和图片所属的标题一并返回，此时可以构造一个生成器。    </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_images</span><span class="params">(json)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> json.get(<span class="string">'data'</span>): <span class="comment">#json中data存在</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json.get(<span class="string">'data'</span>):</span><br><span class="line">            title = item.get(<span class="string">'title'</span>)</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'image_list'</span>): <span class="comment">#item中image_list存在</span></span><br><span class="line">                images = item.get(<span class="string">'image_list'</span>)</span><br><span class="line">                <span class="keyword">for</span> image <span class="keyword">in</span> images:</span><br><span class="line">                    <span class="keyword">yield</span>&#123; <span class="comment">#构造生成器</span></span><br><span class="line">                        <span class="string">'image'</span>:<span class="string">'http:'</span>+ image[<span class="string">'url'</span>],</span><br><span class="line">                        <span class="string">'title'</span>:title</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">保存图片，其中item就是前面get_images()方法返回的一个字典。</span></span><br><span class="line"><span class="string">首先根据item的title来创建文件夹。</span></span><br><span class="line"><span class="string">然后请求这个图片链接，获取图片的二进制数据，以二进制的形式写入文件。</span></span><br><span class="line"><span class="string">图片的名称可以使用其内容的MD5值，这样可以去除重复。                    </span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_image</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(item.get(<span class="string">'title'</span>)): <span class="comment">#判断路径不存在</span></span><br><span class="line">        os.mkdir(item.get(<span class="string">'title'</span>)) <span class="comment">#用title创建文件夹</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(item.get(<span class="string">'image'</span>)) <span class="comment">#请求图片地址</span></span><br><span class="line">        <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">            file_path = <span class="string">'&#123;0&#125;/&#123;1&#125;.&#123;2&#125;'</span>.format(item.get(<span class="string">'title'</span>),md5(response.content).hexdigest(),<span class="string">'jpg'</span>) <span class="comment">#定义图片路径，图片命名用图片内容MD5值16进制表示</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file_path): <span class="comment">#判断路径不存在</span></span><br><span class="line">                <span class="keyword">with</span> open(file_path,<span class="string">'wb'</span>) <span class="keyword">as</span> f: </span><br><span class="line">                    f.write(response.content) <span class="comment">#写入二进制文件</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">'Already Downloaded'</span>, file_path)</span><br><span class="line">    <span class="keyword">except</span> requests.ConnectionError:</span><br><span class="line">        print(<span class="string">'Failed to Save Image'</span>)</span><br><span class="line">    </span><br><span class="line">                    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    json = get_page(offset)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> get_images(json):</span><br><span class="line">        print(item)</span><br><span class="line">        save_image(item)</span><br><span class="line">        </span><br><span class="line">group_start = <span class="number">1</span></span><br><span class="line">group_end = <span class="number">5</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pool = Pool()</span><br><span class="line">    groups = ([i*<span class="number">20</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(group_start,group_end+<span class="number">1</span>)]) <span class="comment">#构造一个offset数组</span></span><br><span class="line">    pool.map(main,groups) <span class="comment">#多进程进程池，调用其map()方法实现多进程下载</span></span><br><span class="line">    pool.close()</span><br><span class="line">    pool.join()</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用Selenium模拟浏览器抓取淘宝商品美食信息</title>
    <link href="http://pythonfood.github.io/2018/07/03/%E4%BD%BF%E7%94%A8Selenium%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8A%93%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E7%BE%8E%E9%A3%9F%E4%BF%A1%E6%81%AF/"/>
    <id>http://pythonfood.github.io/2018/07/03/使用Selenium模拟浏览器抓取淘宝商品美食信息/</id>
    <published>2018-07-03T02:00:00.000Z</published>
    <updated>2018-08-15T10:09:18.837Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h3><p>selenium、pyquery、re、pymongo</p><h3 id="2、流程"><a href="#2、流程" class="headerlink" title="2、流程"></a>2、流程</h3><p>(1)目标站点分析:<br>用浏览器打开淘宝首页输入‘美食’，打开审查元素，分析我们要的商品信息都在Element里哪个分段。</p><p>(2)搜索关键字:<br>利用Selenium驱动浏览器搜索关键字，得到查询后的商品列表。</p><p>(3)分析页码并翻页:<br>得到商品页码数，模拟翻页，得到后续页面的商品列表。</p><p>(4)分析提取商品内容:<br>利用PyQuery分析源码，解析得到商品列表。</p><p>(5)储存到MongoDB:<br>将商品列表信息储存到数据库MongoDB。</p><h3 id="3、实战"><a href="#3、实战" class="headerlink" title="3、实战"></a>3、实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="string">'''使用PhantomJS无界面浏览器，注意需要设置下窗口大小，否则有的元素获取不到会报错'''</span></span><br><span class="line"><span class="comment">#browser = webdriver.PhantomJS()</span></span><br><span class="line"><span class="comment">#browser.set_window_size(1400,900)</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''可以给PhantomJS设置些选项，从而增加效率。比如：不加载图片、设置缓存等'''</span></span><br><span class="line"><span class="comment">#SERVICE_ARGS = ['--load-images=false', '--disk-cache=true']</span></span><br><span class="line"><span class="comment">#browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)</span></span><br><span class="line"><span class="comment">#browser.set_window_size(1400,900)</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''chrome也可以设置无界面模式，Mac和Linux需要chrome59版本，Windows需要chrome60版本'''</span></span><br><span class="line"><span class="comment">#chrome_options = webdriver.ChromeOptions()</span></span><br><span class="line"><span class="comment">#chrome_options.add_argument('--headless')</span></span><br><span class="line"><span class="comment">#browser = webdriver.Chrome(chrome_options=chrome_options)</span></span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">wait = WebDriverWait(browser,<span class="number">10</span>)</span><br><span class="line">client = pymongo.MongoClient(MONGO_URL) <span class="comment">#声明mongodb客户端，MONGO_URL从配置文件config.py获取</span></span><br><span class="line">db = client[MONGO_DB] <span class="comment">#定义数据库，MONGO_DB从配置文件config.py获取。注意中括号[]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">()</span>:</span> <span class="comment">#首页搜索关键字</span></span><br><span class="line">    print(<span class="string">'正在搜索'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        browser.get(<span class="string">'http://www.taobao.com'</span>)</span><br><span class="line">        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#q'</span>))) <span class="comment">#等待搜索框加载</span></span><br><span class="line">        submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#J_TSearchForm &gt; div.search-button &gt; button.btn-search'</span>))) <span class="comment">#等待搜索按钮加载</span></span><br><span class="line">        input.send_keys(KEYWORD) <span class="comment">#输入搜索关键字，KEYWORD从配置文件config.py获取</span></span><br><span class="line">        submit.click()</span><br><span class="line">        total = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.total'</span>))) <span class="comment">#获取总页数</span></span><br><span class="line">        get_products()</span><br><span class="line">        <span class="keyword">return</span> total.text <span class="comment">#返回总页数</span></span><br><span class="line">    <span class="keyword">except</span> TimeoutException: <span class="comment">#捕捉browser超时异常</span></span><br><span class="line">        <span class="keyword">return</span> search() <span class="comment">#超时异常后，重新进行搜索即可</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_page</span><span class="params">(page_number)</span>:</span> <span class="comment">#输入页码，进行翻页</span></span><br><span class="line">    print(<span class="string">'正在翻页'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; input'</span>))) <span class="comment">#等待页码输入框</span></span><br><span class="line">        submit = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; div.form &gt; span.btn.J_Submit'</span>))) <span class="comment">#等待提交按钮</span></span><br><span class="line">        input.clear() <span class="comment">#输入前先清除内容</span></span><br><span class="line">        input.send_keys(page_number)</span><br><span class="line">        submit.click()</span><br><span class="line">        wait.until(EC.text_to_be_present_in_element((By.CSS_SELECTOR,<span class="string">'#mainsrp-pager &gt; div &gt; div &gt; div &gt; ul &gt; li.item.active &gt; span'</span>),str(page_number))) <span class="comment">#等待高亮页码数值显示跳转的页码，确定跳转完成</span></span><br><span class="line">        get_products()</span><br><span class="line">    <span class="keyword">except</span> TimeoutException:</span><br><span class="line">        <span class="keyword">return</span> next_page(page_number) <span class="comment">#超时异常后，重新进行翻页即可</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_products</span><span class="params">()</span>:</span> <span class="comment">#获取当页商品数据</span></span><br><span class="line">    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,<span class="string">'#mainsrp-itemlist .items .item'</span>)))</span><br><span class="line">    html = browser.page_source <span class="comment">#获取当前页面源码</span></span><br><span class="line">    doc = pq(html) <span class="comment">#声明pyquery对象</span></span><br><span class="line">    items = doc(<span class="string">'#mainsrp-itemlist .items .item'</span>).items() <span class="comment"># 获取当前页所有商品对象</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items: <span class="comment">#遍历所有商品</span></span><br><span class="line">        product = &#123;</span><br><span class="line">            <span class="string">'image'</span>:item.find(<span class="string">'.pic .img'</span>).attr(<span class="string">'src'</span>),</span><br><span class="line">            <span class="string">'price'</span>:item.find(<span class="string">'.price'</span>).text(),</span><br><span class="line">            <span class="string">'deal'</span>:item.find(<span class="string">'.deal-cnt'</span>).text()[:<span class="number">-3</span>],</span><br><span class="line">            <span class="string">'title'</span>:item.find(<span class="string">'.title'</span>).text(),</span><br><span class="line">            <span class="string">'shop'</span>:item.find(<span class="string">'.shop'</span>).text(),</span><br><span class="line">            <span class="string">'location'</span>:item.find(<span class="string">'.location'</span>).text()</span><br><span class="line">        &#125;</span><br><span class="line">        save_to_mongo(product)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_mongo</span><span class="params">(result)</span>:</span> <span class="comment">#保存到mongodb数据库</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> db[MONGO_TABLE].insert(result): <span class="comment">#判断数据插入到数据表中，MONGO_TABLE从配置文件config.py中获取</span></span><br><span class="line">            print(<span class="string">'存储到MongoDB成功'</span>,result)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'存储到MongoDB失败'</span>,result)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        total = search()</span><br><span class="line">        total = int(re.compile(<span class="string">'(\d+)'</span>).search(total).group(<span class="number">1</span>)) <span class="comment">#正则匹配对象，搜索总页数字符串，结果索引1，转为int型就是总页数了</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,total+<span class="number">1</span>): <span class="comment">#从第二页开始循环翻页</span></span><br><span class="line">            next_page(i)</span><br><span class="line">    <span class="keyword">except</span> Exception:</span><br><span class="line">        print(<span class="string">'浏览器出错啦'</span>)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        print(<span class="string">'爬取完成'</span>)</span><br><span class="line">        browser.close() <span class="comment">#完成后一定关闭浏览器</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>config.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#config.py</span></span><br><span class="line"></span><br><span class="line">KEYWORD = <span class="string">'美食'</span></span><br><span class="line"></span><br><span class="line">MONGO_URL = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DB = <span class="string">'taobao'</span></span><br><span class="line">MONGO_TABLE = <span class="string">'product'</span></span><br></pre></td></tr></table></figure></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Requests+正则表达式爬取猫眼电影TOP100</title>
    <link href="http://pythonfood.github.io/2018/07/03/Requests-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%88%AC%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1TOP100/"/>
    <id>http://pythonfood.github.io/2018/07/03/Requests-正则表达式爬取猫眼电影TOP100/</id>
    <published>2018-07-03T01:00:00.000Z</published>
    <updated>2018-08-15T03:30:35.985Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、爬取实战"><a href="#一、爬取实战" class="headerlink" title="一、爬取实战"></a>一、爬取实战</h2><h3 id="1、目标站点分析"><a href="#1、目标站点分析" class="headerlink" title="1、目标站点分析"></a>1、目标站点分析</h3><p>我们需要抓取的目标站点为<a href="http://maoyan.com/board/4" target="_blank" rel="noopener">http://maoyan.com/board/4</a> ，打开之后便可以查看到榜单信息。排名第一的电影是霸王别姬，页面中显示的有效信息有影片名称、主演、上映时间、上映地区、评分、图片等信息。</p><p>将网页滚动到最下方，可以发现有分页的列表，直接点击第2页，观察页面的URL和内容发生了怎样的变化，可以发现页面的URL变成<a href="http://maoyan.com/board/4?offset=10" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=10</a> ，比之前的URL多了一个参数，那就是offset=10，而目前显示的结果是排行11~20名的电影，初步推断这是一个偏移量的参数。再点击下一页，发现页面的URL变成了<a href="http://maoyan.com/board/4?offset=20" target="_blank" rel="noopener">http://maoyan.com/board/4?offset=20</a> ，参数offset变成了20，而显示的结果是排行21~30的电影。</p><p>由此可以总结出规律，offset代表偏移量值，如果偏移量为n，则显示的电影序号就是n+1到n+10，每页显示10个。所以，如果想获取TOP100电影，只需要分开请求10次，而10次的offset参数分别设置为0、10、20、…90即可，这样获取不同的页面之后，再用正则表达式提取出相关信息，就可以得到TOP100的所有电影信息了。</p><h3 id="2、流程框架"><a href="#2、流程框架" class="headerlink" title="2、流程框架"></a>2、流程框架</h3><p>(1)抓取单页内容： 利用requests请求目标站点得到单个网页的HTML代码，返回结果。</p><p>(2)正则表达式分析： 根据HTML代码分析得到电影名称、主演、上映时间、评分、图片链接等信息。</p><p>(3)保存至文件： 通过文件的形式将结果保存，每一部电影一个结果一行Json字符串。</p><p>(4)开启循环及多线程： 对多项内容遍历，开启多线程提高抓取速度。</p><h3 id="3、爬虫实战"><a href="#3、爬虫实战" class="headerlink" title="3、爬虫实战"></a>3、爬虫实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0Safari/537.36'</span>&#125; <span class="comment">#设定自己的headers</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_page</span><span class="params">(url)</span>:</span></span><br><span class="line">    response = requests.get(url,headers=headers) <span class="comment">#添加头部信息,不然会被禁</span></span><br><span class="line">    <span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">return</span> response.text</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_one_page</span><span class="params">(html)</span>:</span></span><br><span class="line">    <span class="comment">#匹配过程保证起始符和终止符存在</span></span><br><span class="line">    pattern = re.compile(<span class="string">'&lt;dd&gt;.*?board-index.*?&gt;(\d+)&lt;/i&gt;.*?data-src="(.*?)".*?name"&gt;&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?star"&gt;(.*?)&lt;/p&gt;.*?releasetime"&gt;(.*?)&lt;/p&gt;.*?integer"&gt;(.*?)&lt;/i&gt;.*?fraction"&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;'</span>,re.S)</span><br><span class="line">    items = re.findall(pattern,html)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="keyword">yield</span>&#123;</span><br><span class="line">            <span class="string">'index'</span>:item[<span class="number">0</span>],</span><br><span class="line">            <span class="string">'image'</span>:item[<span class="number">1</span>],</span><br><span class="line">            <span class="string">'title'</span>:item[<span class="number">2</span>],</span><br><span class="line">            <span class="string">'actor'</span>:item[<span class="number">3</span>].strip()[<span class="number">3</span>:],</span><br><span class="line">            <span class="string">'time'</span>:item[<span class="number">4</span>].strip()[<span class="number">5</span>:],</span><br><span class="line">            <span class="string">'score'</span>:item[<span class="number">5</span>]+item[<span class="number">6</span>]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_to_file</span><span class="params">(content)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'猫眼电影TOP100.txt'</span>,<span class="string">'a'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(json.dumps(content,ensure_ascii=<span class="keyword">False</span>) + <span class="string">'\n'</span>)</span><br><span class="line">        f.close</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(offset)</span>:</span></span><br><span class="line">    url = <span class="string">'http://maoyan.com/board/4?offset'</span> + str(offset) <span class="comment">#url设置偏移量</span></span><br><span class="line">    html = get_one_page(url)</span><br><span class="line">    <span class="comment">#print(html)</span></span><br><span class="line">    items = parse_one_page(html)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        print(item)</span><br><span class="line">        write_to_file(item)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):  </span><br><span class="line">        main(offset=i * <span class="number">10</span>)  </span><br><span class="line">    <span class="comment">#pool = Pool() # 使用进程池，提高抓取效率（使用多进程需要屏蔽for循环）</span></span><br><span class="line">    <span class="comment">#pool.map(main,[i*10 for i in range(10)])</span></span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫-selenium库</title>
    <link href="http://pythonfood.github.io/2018/07/02/%E7%88%AC%E8%99%AB-selenium%E5%BA%93/"/>
    <id>http://pythonfood.github.io/2018/07/02/爬虫-selenium库/</id>
    <published>2018-07-02T07:00:00.000Z</published>
    <updated>2018-08-14T09:52:56.689Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、Selenium库详解"><a href="#一、Selenium库详解" class="headerlink" title="一、Selenium库详解"></a>一、Selenium库详解</h2><h3 id="1、什么是Selenium"><a href="#1、什么是Selenium" class="headerlink" title="1、什么是Selenium"></a>1、什么是Selenium</h3><p>Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面的源代码，做到可见即可爬。对于一些JavaScript动态渲染的页面来说，此种抓取方式非常有效。</p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p><code>pip install selenium</code></p><h3 id="3、基本使用"><a href="#3、基本使用" class="headerlink" title="3、基本使用"></a>3、基本使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    input = browser.find_element_by_id(<span class="string">'kw'</span>)</span><br><span class="line">    input.send_keys(<span class="string">'python'</span>)</span><br><span class="line">    input.send_keys(Keys.ENTER)</span><br><span class="line">    wait = WebDriverWait(browser,<span class="number">10</span>)</span><br><span class="line">    wait.until(EC.presence_of_element_located((By.ID,<span class="string">'content_left'</span>)))</span><br><span class="line">    print(browser.current_url)</span><br><span class="line">    print(browser.get_cookies())</span><br><span class="line">    print(browser.page_source)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre></td></tr></table></figure><h3 id="4、声明浏览器对象"><a href="#4、声明浏览器对象" class="headerlink" title="4、声明浏览器对象"></a>4、声明浏览器对象</h3><p>Selenium支持非常多的浏览器，如Chrome、Firefox、Edge等，还有Android、BlackBerry等手机端的浏览器。另外，也支持无界面浏览器PhantomJS。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"> </span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser = webdriver.Firefox()</span><br><span class="line">browser = webdriver.Edge()</span><br><span class="line">browser = webdriver.PhantomJS()</span><br><span class="line">browser = webdriver.Safari()</span><br></pre></td></tr></table></figure><h3 id="5、访问页面"><a href="#5、访问页面" class="headerlink" title="5、访问页面"></a>5、访问页面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">print(browser.page_source)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="6、查找节点"><a href="#6、查找节点" class="headerlink" title="6、查找节点"></a>6、查找节点</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#单个节点</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input1 = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input2 = browser.find_element_by_css_selector(<span class="string">'#q'</span>)</span><br><span class="line">input3 = browser.find_element_by_xpath(<span class="string">'//*[@id="q"]'</span>)</span><br><span class="line">print(input1,input2,input3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Selenium还提供了查找元素通用方法find_element()，它需要传入两个参数：查找方式By和值。</span></span><br><span class="line"><span class="comment">#这种查找方式的功能和上面列举的查找函数完全一致，不过参数更加灵活。</span></span><br><span class="line">input1_same = browser.find_element(By.ID,<span class="string">'q'</span>) <span class="comment">#等价于find_element_by_id('q')</span></span><br><span class="line">print(input1_same)</span><br><span class="line"></span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><p>所有获取单个节点的方法：</p><ul><li>find_element_by_id</li><li>find_element_by_name</li><li>find_element_by_xpath</li><li>find_element_by_link_text</li><li>find_element_by_partial_link_text</li><li>find_element_by_tag_name</li><li>find_element_by_class_name</li><li>find_element_by_css_selector</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多个节点</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">lis = browser.find_elements_by_css_selector(<span class="string">'.service-bd li'</span>)</span><br><span class="line">print(lis)</span><br><span class="line"></span><br><span class="line"><span class="comment">#也可以直接用find_elements()方法来选择</span></span><br><span class="line">lis_same = browser.find_elements(By.CSS_SELECTOR,<span class="string">'.service-bd li'</span>) <span class="comment">#等价于find_elements_by_css_selector('.service-bd li')</span></span><br><span class="line">print(lis_same)</span><br><span class="line"></span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><p>所有获取多个节点的方法：</p><ul><li>find_elements_by_id</li><li>find_elements_by_name</li><li>find_elements_by_xpath</li><li>find_elements_by_link_text</li><li>find_elements_by_partial_link_text</li><li>find_elements_by_tag_name</li><li>find_elements_by_class_name</li><li>find_elements_by_css_selector</li></ul><h3 id="7、元素交互"><a href="#7、元素交互" class="headerlink" title="7、元素交互"></a>7、元素交互</h3><p>官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">input = browser.find_element_by_id(<span class="string">'q'</span>)</span><br><span class="line">input.send_keys(<span class="string">'iphone'</span>)</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line">input.clear()</span><br><span class="line">input.send_keys(<span class="string">'ipad'</span>)</span><br><span class="line">button = browser.find_element_by_class_name(<span class="string">'btn-search'</span>)</span><br><span class="line">button.click()</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="8、交互动作-动作链"><a href="#8、交互动作-动作链" class="headerlink" title="8、交互动作(动作链)"></a>8、交互动作(动作链)</h3><p>还有一些操作，它们没有特定的执行对象，比如鼠标拖曳、键盘按键等，这些动作用另一种方式来执行，那就是动作链。</p><p>将动作附加到动作链中串行执行。</p><p>官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ActionChains</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span>)</span><br><span class="line">time.sleep(<span class="number">5</span>)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>)</span><br><span class="line">source = browser.find_element_by_css_selector(<span class="string">'#draggable'</span>)</span><br><span class="line">target = browser.find_element_by_css_selector(<span class="string">'#droppable'</span>)</span><br><span class="line">actions = ActionChains(browser) <span class="comment">#声明动作链对象</span></span><br><span class="line">actions.drag_and_drop(source,target) <span class="comment">#定义拖放动作</span></span><br><span class="line">actions.perform() <span class="comment">#执行动作链</span></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="9、执行JavaScript"><a href="#9、执行JavaScript" class="headerlink" title="9、执行JavaScript"></a>9、执行JavaScript</h3><p>对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时使用execute_script()方法即可实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.scrollTo(0,document.body.scrollHeight)'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'alert("To Bottom")'</span>)</span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="10、获取元素信息"><a href="#10、获取元素信息" class="headerlink" title="10、获取元素信息"></a>10、获取元素信息</h3><p>Selenium已经提供了选择节点的方法，返回的是WebElement类型，那么它也有相关的方法和属性来直接提取节点信息，如属性、文本等。这样的话，我们就可以不用通过解析源代码来提取信息了，非常方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">logo = browser.find_element_by_id(<span class="string">'zh-top-link-logo'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.get_attribute(<span class="string">'class'</span>)) <span class="comment">#获取class属性</span></span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取文本值</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input)</span><br><span class="line">print(input.text) <span class="comment">#获取文本值</span></span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取id、位置、标签名和大小</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input)</span><br><span class="line">print(input.id) <span class="comment">#获取节点id</span></span><br><span class="line">print(input.location) <span class="comment">#获取该节点在页面中的相对位置</span></span><br><span class="line">print(input.tag_name) <span class="comment">#获取标签名称</span></span><br><span class="line">print(input.size) <span class="comment">#获取节点的大小，也就是宽高</span></span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="11、切换Frame"><a href="#11、切换Frame" class="headerlink" title="11、切换Frame"></a>11、切换Frame</h3><p>网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。Selenium打开页面后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame()方法来切换Frame。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> NoSuchElementException</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'</span>)</span><br><span class="line">browser.switch_to.frame(<span class="string">'iframeResult'</span>) <span class="comment">#切换到子frame</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'NO LOGO'</span>)</span><br><span class="line">browser.switch_to.parent_frame() <span class="comment">#切换回父frame</span></span><br><span class="line">logo = browser.find_element_by_class_name(<span class="string">'logo'</span>)</span><br><span class="line">print(logo)</span><br><span class="line">print(logo.text)</span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="12、延时等待"><a href="#12、延时等待" class="headerlink" title="12、延时等待"></a>12、延时等待</h3><p>这里等待的方式有两种：一种是隐式等待，一种是显式等待。</p><p>隐式等待，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。隐式等待的效果其实并没有那么好，因为我们只规定了一个固定时间，而页面的加载时间会受到网络条件的影响。</p><p>显式等待，它指定要查找的节点，然后指定一个最长等待时间。如果在规定时间内加载出来了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#隐式等待</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.implicitly_wait(<span class="number">10</span>) <span class="comment">#隐式等待10秒</span></span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">input = browser.find_element_by_class_name(<span class="string">'zu-top-add-question'</span>)</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示等待</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">wait = WebDriverWait(browser,<span class="number">10</span>) <span class="comment">#声明WebDriverWait对象，指定最长等待时间</span></span><br><span class="line">input = wait.until(EC.presence_of_element_located((By.ID,<span class="string">'q'</span>))) <span class="comment">#调用它的until()方法，传入要等待条件</span></span><br><span class="line">button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,<span class="string">'.btn-search'</span>)))</span><br><span class="line">print(input,button)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><p>等待条件及其含义：</p><ul><li><code>title_is</code>：标题是某内容</li><li><code>title_contains</code>：标题包含某内容</li><li><code>presence_of_element_located</code>：节点加载出来，传入定位元组，如(By.ID, ‘p’)</li><li><code>visibility_of_element_located</code>：节点可见，传入定位元组</li><li><code>visibility_of</code>：可见，传入节点对象</li><li><code>presence_of_all_elements_located</code>：所有节点加载出来</li><li><code>text_to_be_present_in_element</code>：某个节点文本包含某文字</li><li><code>text_to_be_present_in_element_value</code>：某个节点值包含某文字</li><li><code>frame_to_be_available_and_switch_to_it</code>：加载并切换</li><li><code>invisibility_of_element_located</code>：节点不可见</li><li><code>element_to_be_clickable</code>：节点可点击</li><li><code>staleness_of</code>：判断一个节点是否仍在DOM，可判断页面是否已经刷新</li><li><code>element_to_be_selected</code>：节点可选择，传节点对象</li><li><code>element_located_to_be_selected</code>：节点可选择，传入定位元组</li><li><code>element_selection_state_to_be</code>：传入节点对象以及状态，相等返回True，否则返回False</li><li><code>element_located_selection_state_to_be</code>：传入定位元组以及状态，相等返回True，否则返回False</li><li><code>alert_is_present</code>：是否出现警告</li></ul><p>等待条件的参数及用法参考官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.support.expected_conditions</a></p><h3 id="13、前进后退"><a href="#13、前进后退" class="headerlink" title="13、前进后退"></a>13、前进后退</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com/'</span>)</span><br><span class="line">browser.get(<span class="string">'https://www.python.org/'</span>)</span><br><span class="line"></span><br><span class="line">browser.back() <span class="comment">#后退</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">browser.forward() <span class="comment">#前进</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="14、Cookies"><a href="#14、Cookies" class="headerlink" title="14、Cookies"></a>14、Cookies</h3><p>Selenium可以方便地对Cookies进行操作，例如获取、添加、删除Cookies等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"> </span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.zhihu.com/explore'</span>)</span><br><span class="line">print(browser.get_cookies()) <span class="comment">#获取cookies</span></span><br><span class="line">browser.add_cookie(&#123;<span class="string">'name'</span>: <span class="string">'name'</span>, <span class="string">'domain'</span>: <span class="string">'www.zhihu.com'</span>, <span class="string">'value'</span>: <span class="string">'germey'</span>&#125;) <span class="comment">#添加cookie</span></span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.delete_all_cookies() <span class="comment">#删除全部cookies</span></span><br><span class="line">print(browser.get_cookies())</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="15、选项卡管理"><a href="#15、选项卡管理" class="headerlink" title="15、选项卡管理"></a>15、选项卡管理</h3><p>在访问网页的时候，会开启一个个选项卡。Selenium也可以对选项卡进行操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time </span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.execute_script(<span class="string">'window.open()'</span>) <span class="comment">#借助javascript开启新的选项卡</span></span><br><span class="line">print(browser.window_handles) <span class="comment">#window_handles获取当前所有选项卡句柄</span></span><br><span class="line">browser.switch_to_window(browser.window_handles[<span class="number">1</span>]) <span class="comment">#切换到第二个选项卡</span></span><br><span class="line">browser.get(<span class="string">'https://www.taobao.com'</span>)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">browser.switch_to_window(browser.window_handles[<span class="number">0</span>]) <span class="comment">#切换回第一个选项卡</span></span><br><span class="line">browser.get(<span class="string">'https://python.org'</span>)</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line">browser.close()</span><br></pre></td></tr></table></figure><h3 id="16、异常处理"><a href="#16、异常处理" class="headerlink" title="16、异常处理"></a>16、异常处理</h3><p>官方文档：<a href="http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions" target="_blank" rel="noopener">http://selenium-python.readthedocs.io/api.html#module-selenium.common.exceptions</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"> </span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">browser.find_element_by_id(<span class="string">'hello'</span>) <span class="comment">#选择一个并不存在的节点，此时就会遇到异常。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException,NoSuchElementException</span><br><span class="line"></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">except</span> TimeoutException:</span><br><span class="line">    print(<span class="string">'Time Out'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    browser.find_element_by_id(<span class="string">'hello'</span>)</span><br><span class="line"><span class="keyword">except</span> NoSuchElementException:</span><br><span class="line">    print(<span class="string">'No Element'</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    browser.close()</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫-pyquery库</title>
    <link href="http://pythonfood.github.io/2018/07/02/%E7%88%AC%E8%99%AB-pyquery%E5%BA%93/"/>
    <id>http://pythonfood.github.io/2018/07/02/爬虫-pyquery库/</id>
    <published>2018-07-02T06:00:00.000Z</published>
    <updated>2018-08-14T09:47:15.801Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、PyQuery库详解"><a href="#一、PyQuery库详解" class="headerlink" title="一、PyQuery库详解"></a>一、PyQuery库详解</h2><h3 id="1、什么是PyQuery库"><a href="#1、什么是PyQuery库" class="headerlink" title="1、什么是PyQuery库"></a>1、什么是PyQuery库</h3><p>强大而灵活的网页解析库。如果你觉得正则写起来太麻烦，如果你觉得BeautifulSoup语法太难记，如果你熟悉jQuery的语法，那么PyQuery就是你的绝佳选择！！！</p><p><a href="http://pyquery.readthedocs.io" target="_blank" rel="noopener">http://pyquery.readthedocs.io</a></p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p><code>pip install pyquery</code></p><h3 id="3、初始化"><a href="#3、初始化" class="headerlink" title="3、初始化"></a>3、初始化</h3><p>初始化pyquery的时候，也需要传入HTML文本来初始化一个PyQuery对象。它的初始化方式有多种，比如直接传入字符串，传入URL，传入文件名，等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#三种初始化方式</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.字符串初始化</span></span><br><span class="line">html = <span class="string">''' </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">    &lt;ul&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;first item&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;&gt;/li </span></span><br><span class="line"><span class="string">        &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;/ul&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">doc1 = pq(html)</span><br><span class="line">print(doc1(<span class="string">'li'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.URL初始化 </span></span><br><span class="line">doc2 = pq(url=<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(doc2(<span class="string">'head'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.文件初始化</span></span><br><span class="line">doc3 = pq(filename=<span class="string">'demo.html'</span>)</span><br><span class="line">print(doc3(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure><h3 id="4、CSS选择器"><a href="#4、CSS选择器" class="headerlink" title="4、CSS选择器"></a>4、CSS选择器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">''' </span></span><br><span class="line"><span class="string">&lt;div id="container"&gt; </span></span><br><span class="line"><span class="string">    &lt;ul class="list"&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;first item&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;&gt;/li </span></span><br><span class="line"><span class="string">        &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;/ul&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">doc = pq(html)</span><br><span class="line">print(doc(<span class="string">'#container .list li'</span>)) <span class="comment">#空格代表嵌套关系,依次传入了id、class、标签</span></span><br><span class="line">print(type(doc(<span class="string">'#container .list li'</span>)))</span><br></pre></td></tr></table></figure><h3 id="5、查找元素"><a href="#5、查找元素" class="headerlink" title="5、查找元素"></a>5、查找元素</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#子元素</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div id="container"&gt; </span></span><br><span class="line"><span class="string">    &lt;ul class="list"&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;first item&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;&gt;/li </span></span><br><span class="line"><span class="string">        &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;/ul&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">doc = pq(html)</span><br><span class="line"><span class="comment"># 查找子孙节点需要用到find()方法，此时传入的参数是CSS选择器</span></span><br><span class="line">items = doc.find(<span class="string">'.list'</span>)</span><br><span class="line">print(items)</span><br><span class="line">print(type(items))</span><br><span class="line">lis = doc.find(<span class="string">'li'</span>)</span><br><span class="line">print(lis)</span><br><span class="line">print(type(lis))</span><br><span class="line"></span><br><span class="line">child1 = items.children() <span class="comment">#children()方法查找子节点</span></span><br><span class="line">print(child1)</span><br><span class="line">print(type(child1))</span><br><span class="line"></span><br><span class="line">child2 = items.children(<span class="string">'.active'</span>) <span class="comment">#筛选子节点中符合条件的节点</span></span><br><span class="line">print(child2)</span><br><span class="line">print(type(child2))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#父元素</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line">items = doc(<span class="string">'.list'</span>) <span class="comment">#首先用.list选取class为list的节点</span></span><br><span class="line"></span><br><span class="line">container = items.parent() <span class="comment">#parent()得到父节点</span></span><br><span class="line">print(container)</span><br><span class="line">print(type(container))</span><br><span class="line"></span><br><span class="line">parents = items.parents() <span class="comment">#parents()得到祖先节点</span></span><br><span class="line">print(parents)</span><br><span class="line">print(type(parents))</span><br><span class="line">parent = items.parents(<span class="string">'.wrap'</span>) <span class="comment">#筛选祖先节点中符合条件的节点</span></span><br><span class="line">print(parent)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#兄弟元素</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(<span class="string">'.list .item-0.active'</span>) <span class="comment">#注意：.item-0和.active之间没有空格，表示这两个class并存的节点</span></span><br><span class="line"></span><br><span class="line">print(li.siblings()) <span class="comment">#siblings()获取兄弟节点</span></span><br><span class="line"></span><br><span class="line">print(li.siblings(<span class="string">'.active'</span>)) <span class="comment">#筛选兄弟节点中符合条件的节点</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#遍历</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">print(li) <span class="comment">#单个节点可以直接打印输出</span></span><br><span class="line">print(str(li)) <span class="comment">#也可以直接转成字符串</span></span><br><span class="line"></span><br><span class="line">lis = doc(<span class="string">'li'</span>).items() <span class="comment">#多个节点的结果，需要items()遍历来获取</span></span><br><span class="line">print(type(lis))</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> lis:</span><br><span class="line">    print(li,type(li))</span><br></pre></td></tr></table></figure><h3 id="6、获取信息"><a href="#6、获取信息" class="headerlink" title="6、获取信息"></a>6、获取信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt; </span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0"&gt;first item&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line"></span><br><span class="line">print(a,type(a))</span><br><span class="line">print(a.attr(<span class="string">'href'</span>)) <span class="comment">#调用attr()方法获取属性值</span></span><br><span class="line">print(a.attr.href) <span class="comment">#调用attr属性来获取属性值，这两种方式都可以</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取文本</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt; </span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;  </span></span><br><span class="line"><span class="string">            &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">doc = pq(html)</span><br><span class="line"></span><br><span class="line">a = doc(<span class="string">'.item-0.active a'</span>)</span><br><span class="line">print(a,type(a))</span><br><span class="line">print(a.text()) <span class="comment">#text()获取节点内部的纯文本</span></span><br><span class="line"></span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">print(li)</span><br><span class="line">print(li.html()) <span class="comment">#html()获取节点内部的HTML文本</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意，如果选中的结果是多个节点</span></span><br><span class="line">lis = doc(<span class="string">'li'</span>)</span><br><span class="line">print(lis.html()) <span class="comment">#html()方法返回的是第一个li节点的内部HTML文本</span></span><br><span class="line">print(lis.text()) <span class="comment">#text()则返回所有的li节点内部的纯文本，中间用空格分割开</span></span><br><span class="line">print(type(lis.text())) <span class="comment">#text()返回结果是一个字符串</span></span><br></pre></td></tr></table></figure><h3 id="7、DOM操作"><a href="#7、DOM操作" class="headerlink" title="7、DOM操作"></a>7、DOM操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt; </span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0"&gt;first item&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="boid"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">doc = pq(html)</span><br><span class="line"></span><br><span class="line">li = doc(<span class="string">'.item-0.active'</span>)</span><br><span class="line">print(li)</span><br><span class="line">li.removeClass(<span class="string">'active'</span>) <span class="comment">#removeClass()移除class属性</span></span><br><span class="line">print(li)</span><br><span class="line">li.addClass(<span class="string">'active'</span>) <span class="comment">#addClass()增加class属性</span></span><br><span class="line">print(li)</span><br><span class="line"></span><br><span class="line">li.attr(<span class="string">'name'</span>,<span class="string">'link'</span>) <span class="comment">#attr(属性名，属性值)来修改属性，如果只传入第一个参数的属性名，则是获取这个属性值</span></span><br><span class="line">print(li)</span><br><span class="line"></span><br><span class="line">li.text(<span class="string">'changed item'</span>) <span class="comment">#text(纯文本)来修改节点内纯文本，如果不传参数则是获取节点内纯文本</span></span><br><span class="line">print(li)</span><br><span class="line"></span><br><span class="line">li.html(<span class="string">'&lt;span&gt;changed item&lt;/span&gt;'</span>) <span class="comment">#html(html文本)来修改节点内html文本，如果不传参数则是获取节点内html文本</span></span><br><span class="line">print(li)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># remove()移除</span></span><br><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    Hello, World</span></span><br><span class="line"><span class="string">    &lt;p&gt;This is a paragraph.&lt;/p&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">doc = pq(html)</span><br><span class="line"></span><br><span class="line">wrap = doc(<span class="string">'.wrap'</span>)</span><br><span class="line">print(wrap.text()) <span class="comment">#获取的所有文本。现在想提取'Hello, World'这个字符串，而不要p节点内部的字符串，需要怎样操作呢？</span></span><br><span class="line"></span><br><span class="line">wrap.find(<span class="string">'p'</span>).remove() <span class="comment">#remove()方法移除节点</span></span><br><span class="line">print(wrap.text()) <span class="comment">#此时wrap内部就只剩下'Hello, World'这句话了</span></span><br></pre></td></tr></table></figure><p>还有很多节点操作的方法：append()、empty()和prepend()等，和jQuery的用法完全一致。官方文档：<a href="http://pyquery.readthedocs.io/en/latest/api.html。" target="_blank" rel="noopener">http://pyquery.readthedocs.io/en/latest/api.html。</a></p><h3 id="8、伪类选择器"><a href="#8、伪类选择器" class="headerlink" title="8、伪类选择器"></a>8、伪类选择器</h3><p>CSS选择器之所以强大，还有一个很重要的原因，那就是它支持多种多样的伪类选择器，例如选择第一个节点、最后一个节点、奇偶数节点、包含某一文本的节点等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyquery <span class="keyword">import</span> PyQuery <span class="keyword">as</span> pq</span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="wrap"&gt;</span></span><br><span class="line"><span class="string">    &lt;div id="container"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list"&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;first item&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1"&gt;&lt;a href="link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0 active"&gt;&lt;a href="link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-1 active"&gt;&lt;a href="link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">             &lt;li class="item-0"&gt;&lt;a href="link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;/ul&gt;</span></span><br><span class="line"><span class="string">     &lt;/div&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">doc = pq(html)</span><br><span class="line"></span><br><span class="line">li = doc(<span class="string">'li:first-child'</span>) <span class="comment">#第一个</span></span><br><span class="line">print(li)</span><br><span class="line">li1 = doc(<span class="string">'li:last-child'</span>) <span class="comment">#最后一个</span></span><br><span class="line">print(li1)</span><br><span class="line">li2 = doc(<span class="string">'li:nth-child(2)'</span>) <span class="comment">#指定缩写顺序，第二个</span></span><br><span class="line">print(li2)</span><br><span class="line">li3 = doc(<span class="string">'li:gt(2)'</span>) <span class="comment">#大于2的</span></span><br><span class="line">print(li3)</span><br><span class="line">li4 = doc(<span class="string">'li:nth-child(2n)'</span>) <span class="comment">#偶数</span></span><br><span class="line">print(li4)</span><br><span class="line">li5 = doc(<span class="string">'li:contains(second)'</span>) <span class="comment">#包含second</span></span><br><span class="line">print(li5)</span><br></pre></td></tr></table></figure><p>关于CSS选择器的更多用法，可以参考<a href="http://www.w3school.com.cn/css/index.asp" target="_blank" rel="noopener">http://www.w3school.com.cn/css/index.asp</a></p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫-beautifulsoup库</title>
    <link href="http://pythonfood.github.io/2018/07/02/%E7%88%AC%E8%99%AB-beautifulsoup%E5%BA%93/"/>
    <id>http://pythonfood.github.io/2018/07/02/爬虫-beautifulsoup库/</id>
    <published>2018-07-02T05:00:00.000Z</published>
    <updated>2018-08-14T09:43:09.814Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、BeautifulSoup库详解"><a href="#一、BeautifulSoup库详解" class="headerlink" title="一、BeautifulSoup库详解"></a>一、BeautifulSoup库详解</h2><h3 id="1、什么是BeautifulSoup"><a href="#1、什么是BeautifulSoup" class="headerlink" title="1、什么是BeautifulSoup"></a>1、什么是BeautifulSoup</h3><p>Beautiful Soup就是Python的一个HTML或XML的解析库，可以用它来方便地从网页中提取数据。利用它不用编写正则表达式即可方便实现网页信息的提取。</p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p><code>pip install beautifulsoup4</code></p><h3 id="3、解析器"><a href="#3、解析器" class="headerlink" title="3、解析器"></a>3、解析器</h3><table><thead><tr><th style="text-align:left">解析器</th><th style="text-align:left">使用方法</th><th style="text-align:left">优势</th><th style="text-align:left">劣势</th></tr></thead><tbody><tr><td style="text-align:left">Python标准库</td><td style="text-align:left">BeautifulSoup(markup, “html.parser”)</td><td style="text-align:left">Python的内置标准库、执行速度适中、文档容错能力强</td><td style="text-align:left">Python 2.7.3及Python 3.2.2之前的版本文档容错能力差</td></tr><tr><td style="text-align:left">lxml HTML解析器</td><td style="text-align:left">BeautifulSoup(markup, “lxml”)</td><td style="text-align:left">速度快、文档容错能力强</td><td style="text-align:left">需要安装C语言库</td></tr><tr><td style="text-align:left">lxml XML解析器</td><td style="text-align:left">BeautifulSoup(markup, “xml”)</td><td style="text-align:left">速度快、唯一支持XML的解析器</td><td style="text-align:left">需要安装C语言库</td></tr><tr><td style="text-align:left">html5lib</td><td style="text-align:left">BeautifulSoup(markup, “html5lib”)</td><td style="text-align:left">最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档</td><td style="text-align:left">速度慢、不依赖外部扩展</td></tr></tbody></table><h3 id="4、基本用法"><a href="#4、基本用法" class="headerlink" title="4、基本用法"></a>4、基本用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">''''' </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt; </span></span><br><span class="line"><span class="string">&lt;body&gt; </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt; </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt; </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt; </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt; </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title.string) <span class="comment">#打印标题</span></span><br><span class="line">print(soup.prettify()) <span class="comment">#格式化代码</span></span><br></pre></td></tr></table></figure><h3 id="5、标签选择器"><a href="#5、标签选择器" class="headerlink" title="5、标签选择器"></a>5、标签选择器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择元素</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''  </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt;  </span></span><br><span class="line"><span class="string">&lt;body&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were  </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt;  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt;  </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;  </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title)</span><br><span class="line">print(type(soup.title))</span><br><span class="line">print(soup.head)</span><br><span class="line">print(soup.p) <span class="comment">#只能返回第一个标签</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取名称</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''  </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt;  </span></span><br><span class="line"><span class="string">&lt;body&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were  </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt;  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt;  </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;  </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.title.name) <span class="comment">#打印标签名称</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''  </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt;  </span></span><br><span class="line"><span class="string">&lt;body&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were  </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt;  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt;  </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;  </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.p.attrs[<span class="string">'name'</span>])</span><br><span class="line">print(soup.p[<span class="string">'name'</span>]) <span class="comment">#两种方式都可以获取标签属性</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取内容</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''  </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt;  </span></span><br><span class="line"><span class="string">&lt;body&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were  </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt;  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt;  </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;  </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.p.string)</span><br></pre></td></tr></table></figure><h3 id="6、嵌套选择"><a href="#6、嵌套选择" class="headerlink" title="6、嵌套选择"></a>6、嵌套选择</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''  </span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Domouse's story&lt;/title&gt;&lt;/head&gt;  </span></span><br><span class="line"><span class="string">&lt;body&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="title"name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were little sisters;and their names were  </span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie"class="sister"id="link1"&gt;&lt;!--Elsie--&gt;&lt;/a&gt;  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/lacle"class="sister"id="link2"&gt;Lacle&lt;/a&gt;and  </span></span><br><span class="line"><span class="string">&lt;a hred="http://example.com/tilie"class="sister"id="link3"&gt;Tillie&lt;/a&gt;  </span></span><br><span class="line"><span class="string">and they lived at bottom of a well.&lt;/p&gt;  </span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;  </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.head.title.string)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#子节点和子孙节点</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;html&gt; </span></span><br><span class="line"><span class="string">    &lt;head&gt; </span></span><br><span class="line"><span class="string">        &lt;title&gt;The Domouse's story&lt;/title&gt; </span></span><br><span class="line"><span class="string">    &lt;/head&gt; </span></span><br><span class="line"><span class="string">    &lt;body&gt; </span></span><br><span class="line"><span class="string">    &lt;p class="story"&gt; </span></span><br><span class="line"><span class="string">        Once upon a time there were little sisters;and their names were </span></span><br><span class="line"><span class="string">        &lt;a href="http://example.com/elsie" class="sister"id="link1"&gt; </span></span><br><span class="line"><span class="string">        &lt;span&gt;Elsle&lt;/span&gt; </span></span><br><span class="line"><span class="string">        &lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/lacle"class="sister" id="link2"&gt;Lacle&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/tilie"class="sister" id="link3"&gt;Tillie&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and they lived at bottom of a well. </span></span><br><span class="line"><span class="string">        &lt;/p&gt; </span></span><br><span class="line"><span class="string">        &lt;p class="story"&gt;...&lt;/p&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.p.contents) <span class="comment">#子节点以列表形式返回</span></span><br><span class="line"></span><br><span class="line">print(soup.p.children) <span class="comment">#不同之处：children实际上是一个迭代器，需要用循环的方式才能将内容取出</span></span><br><span class="line"><span class="keyword">for</span> i,child <span class="keyword">in</span> enumerate(soup.p.children):</span><br><span class="line">    print(i,child)</span><br><span class="line"></span><br><span class="line">print(soup.p.descendants) <span class="comment">#获取所有的子孙节点，也是一个迭代器</span></span><br><span class="line"><span class="keyword">for</span> l,child1 <span class="keyword">in</span> enumerate(soup.p.descendants):</span><br><span class="line">    print(l,child1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#父节点和祖先节点</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;html&gt; </span></span><br><span class="line"><span class="string">    &lt;head&gt; </span></span><br><span class="line"><span class="string">        &lt;title&gt;The Domouse's story&lt;/title&gt; </span></span><br><span class="line"><span class="string">    &lt;/head&gt; </span></span><br><span class="line"><span class="string">    &lt;body&gt; </span></span><br><span class="line"><span class="string">    &lt;p class="story"&gt; </span></span><br><span class="line"><span class="string">        Once upon a time there were little sisters;and their names were </span></span><br><span class="line"><span class="string">        &lt;a href="http://example.com/elsie" class="sister"id="link1"&gt; </span></span><br><span class="line"><span class="string">        &lt;span&gt;Elsle&lt;/span&gt; </span></span><br><span class="line"><span class="string">        &lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/lacle"class="sister" id="link2"&gt;Lacle&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/tilie"class="sister" id="link3"&gt;Tillie&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and they lived at bottom of a well. </span></span><br><span class="line"><span class="string">        &lt;/p&gt; </span></span><br><span class="line"><span class="string">        &lt;p class="story"&gt;...&lt;/p&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.a.parent) <span class="comment">#返回父标签的整个内容</span></span><br><span class="line"></span><br><span class="line">print(list(enumerate(soup.a.parents))) <span class="comment">#所有祖先节点（包括父标签）</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#兄弟节点</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;html&gt; </span></span><br><span class="line"><span class="string">    &lt;head&gt; </span></span><br><span class="line"><span class="string">        &lt;title&gt;The Domouse's story&lt;/title&gt; </span></span><br><span class="line"><span class="string">    &lt;/head&gt; </span></span><br><span class="line"><span class="string">    &lt;body&gt; </span></span><br><span class="line"><span class="string">    &lt;p class="story"&gt; </span></span><br><span class="line"><span class="string">        Once upon a time there were little sisters;and their names were </span></span><br><span class="line"><span class="string">        &lt;a href="http://example.com/elsie" class="sister"id="link1"&gt; </span></span><br><span class="line"><span class="string">        &lt;span&gt;Elsle&lt;/span&gt; </span></span><br><span class="line"><span class="string">        &lt;/a&gt; </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/lacle"class="sister" id="link2"&gt;Lacle&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and </span></span><br><span class="line"><span class="string">        &lt;a hred="http://example.com/tilie"class="sister" id="link3"&gt;Tillie&lt;/a&gt; </span></span><br><span class="line"><span class="string">        and they lived at bottom of a well. </span></span><br><span class="line"><span class="string">        &lt;/p&gt; </span></span><br><span class="line"><span class="string">        &lt;p class="story"&gt;...&lt;/p&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">print(list(enumerate(soup.a.next_siblings))) <span class="comment">#后面的兄弟节点 </span></span><br><span class="line">print(list(enumerate(soup.a.previous_siblings))) <span class="comment">#前面的兄弟节点</span></span><br></pre></td></tr></table></figure><h3 id="7、标准选择器"><a href="#7、标准选择器" class="headerlink" title="7、标准选择器"></a>7、标准选择器</h3><p><code>find_all(name,attrs,recursive,text,**kargs)</code> </p><p>可根据签名、属性、内容查找文档  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#name</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line">  </span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"name="elements"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find_all(<span class="string">'ul'</span>)) <span class="comment">#返回列表类型</span></span><br><span class="line">print(soup.find_all(<span class="string">'ul'</span>)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.find_all(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.find_all(<span class="string">'li'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#attrs</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line">  </span><br><span class="line">html=<span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list" id="list-1" name="elements"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small" id="list-2"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">"id"</span>:<span class="string">"list-1"</span>&#125;))</span><br><span class="line">print(soup.find_all(attrs=&#123;<span class="string">"name"</span>:<span class="string">"elements"</span>&#125;))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#常用属性</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line">  </span><br><span class="line">html=<span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list" id="list-1" name="elements"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small" id="list-2"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find_all(id=<span class="string">'list-1'</span>))</span><br><span class="line">print(soup.find_all(class_=<span class="string">'element'</span>)) <span class="comment">#注意：由于class在Python里是一个关键字，所以后面需要加一个下划线</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#text</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line">  </span><br><span class="line">html=<span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt;</span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list" id="list-1" name="elements"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small" id="list-2"&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find_all(text=<span class="string">'Foo'</span>))</span><br></pre></td></tr></table></figure><p><code>find(name，attrs，recursive，text，**kwargs)</code></p><p>find返回单个元素，find_all返回所有元素 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line"></span><br><span class="line">html = <span class="string">''''' </span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"name="elelments"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>)</span><br><span class="line">print(soup.find(<span class="string">'ul'</span>))</span><br><span class="line">print(type(soup.find(<span class="string">'ul'</span>)))</span><br><span class="line">print(soup.find(<span class="string">'page'</span>))</span><br></pre></td></tr></table></figure><p>其他常用方法：</p><ul><li><code>find_parents()</code>：返回所有祖先节点和find_parent()返回直接父节点</li><li><code>find_next_siblings()</code>：返回所有兄弟节点和find_next_sibling()返回后面第一个兄弟节点</li><li><code>find_previous_siblings()</code>：返回前面的所有兄弟节点和find_previous_sibling()：返回前面第一个兄弟节点</li><li><code>find_all_next()</code>：返回节点后所有符合条件的节点和find_next()：返回第一个符合条件的节点</li><li><code>find_all_previous()</code>：返回节点前所有符合条件的节点和find_previous()：返回节点前第一个符合条件的节点</li></ul><h3 id="8、CSS选择器"><a href="#8、CSS选择器" class="headerlink" title="8、CSS选择器"></a>8、CSS选择器</h3><p>通过select()直接传入CSS选择器即可完成选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup </span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"name="elelments"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>) </span><br><span class="line">print(soup.select(<span class="string">'.panel .panel-heading'</span>)) <span class="comment">#传入css选择器</span></span><br><span class="line">print(soup.select(<span class="string">'ul li'</span>)) <span class="comment"># 传入标签</span></span><br><span class="line">print(soup.select(<span class="string">'#list-2 .element'</span>)) <span class="comment">#传入id</span></span><br><span class="line">print(type(soup.select(<span class="string">'ul'</span>)[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#嵌套选择</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup </span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"name="elelments"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>) </span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul.select(<span class="string">'li'</span>))</span><br><span class="line"><span class="comment">#可以直接传入选择器实现嵌套比这种方式更方便</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取属性</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup </span><br><span class="line"></span><br><span class="line">html = <span class="string">'''</span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"name="elelments"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>) </span><br><span class="line"><span class="keyword">for</span> ul <span class="keyword">in</span> soup.select(<span class="string">'ul'</span>):</span><br><span class="line">    print(ul[<span class="string">'id'</span>]) <span class="comment">#直接传入中括号和属性名</span></span><br><span class="line">    print(ul.attrs[<span class="string">'id'</span>]) <span class="comment">#通过attrs属性获取属性值,两种方式都可以</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取内容</span></span><br><span class="line">html = <span class="string">''''' </span></span><br><span class="line"><span class="string">&lt;div class="panel"&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-heading"&gt; </span></span><br><span class="line"><span class="string">        &lt;h4&gt;Hello&lt;/h4&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">    &lt;div class="panel-body"name="elelments"&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list"Id="list-1"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Jay&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">        &lt;ul class="list list-small"Id="list-2"&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Foo&lt;/li&gt; </span></span><br><span class="line"><span class="string">            &lt;li class="element"&gt;Bar&lt;/li&gt; </span></span><br><span class="line"><span class="string">        &lt;/ul&gt; </span></span><br><span class="line"><span class="string">    &lt;/div&gt; </span></span><br><span class="line"><span class="string">&lt;div&gt; </span></span><br><span class="line"><span class="string">'''</span>  </span><br><span class="line">soup = BeautifulSoup(html,<span class="string">'lxml'</span>) </span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> soup.select(<span class="string">'li'</span>):</span><br><span class="line">    print(li.string) <span class="comment">#string属性获取文本</span></span><br><span class="line">    print(li.get_text()) <span class="comment">#get_text()方法获取文本，两种都行</span></span><br></pre></td></tr></table></figure><p>总结：</p><p>(1)推荐使用’lxml’解析库，必要时使用html.parser</p><p>(2)标签选择器筛选功能但速度快</p><p>(3)建议使用find()，find_all()查询匹配单个结果或者多个结果</p><p>(4)如果对CSS选择器熟悉建议选用select()</p><p>(5)记住常用的获取属性和文本值得方法</p><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫-re库</title>
    <link href="http://pythonfood.github.io/2018/07/02/%E7%88%AC%E8%99%AB-re%E5%BA%93/"/>
    <id>http://pythonfood.github.io/2018/07/02/爬虫-re库/</id>
    <published>2018-07-02T04:00:00.000Z</published>
    <updated>2018-08-14T09:29:30.055Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、Re库详解"><a href="#一、Re库详解" class="headerlink" title="一、Re库详解"></a>一、Re库详解</h2><h3 id="1、什么是正则表达式"><a href="#1、什么是正则表达式" class="headerlink" title="1、什么是正则表达式"></a>1、什么是正则表达式</h3><p>正则表达式对子符串操作的一种逻辑公式，就是事先定义好的一些特定字符、及这些特定字符的组合，组成一个‘规则字符串’，这个‘规则字符串’用来表达对字符串的一种过滤逻辑。</p><h3 id="2、样例展示"><a href="#2、样例展示" class="headerlink" title="2、样例展示"></a>2、样例展示</h3><p>开源中国提供的正则表达式测试工具：<a href="http://tool.oschina.net/regex/" target="_blank" rel="noopener">http://tool.oschina.net/regex/</a> 。输入待匹配的文本，然后选择常用的正则表达式，就可以得出相应的匹配结果了。</p><h3 id="3、常用的匹配规则"><a href="#3、常用的匹配规则" class="headerlink" title="3、常用的匹配规则"></a>3、常用的匹配规则</h3><table><thead><tr><th style="text-align:left">模式</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">\w</td><td style="text-align:left">匹配字母、数字及下划线</td></tr><tr><td style="text-align:left">\W</td><td style="text-align:left">匹配不是字母、数字及下划线的字符</td></tr><tr><td style="text-align:left">\s</td><td style="text-align:left">匹配任意空白字符，等价于[\t\n\r\f]</td></tr><tr><td style="text-align:left">\S</td><td style="text-align:left">匹配任意非空字符</td></tr><tr><td style="text-align:left">\d</td><td style="text-align:left">匹配任意数字，等价于[0-9]</td></tr><tr><td style="text-align:left">\D</td><td style="text-align:left">匹配任意非数字的字符</td></tr><tr><td style="text-align:left">\A</td><td style="text-align:left">匹配字符串开头</td></tr><tr><td style="text-align:left">\Z</td><td style="text-align:left">匹配字符串结尾，如果存在换行，只匹配到换行前的结束字符串</td></tr><tr><td style="text-align:left">\z</td><td style="text-align:left">匹配字符串结尾，如果存在换行，同时还会匹配换行符</td></tr><tr><td style="text-align:left">\G</td><td style="text-align:left">匹配最后匹配完成的位置</td></tr><tr><td style="text-align:left">\n</td><td style="text-align:left">匹配一个换行符</td></tr><tr><td style="text-align:left">\t</td><td style="text-align:left">匹配一个制表符</td></tr><tr><td style="text-align:left">^</td><td style="text-align:left">匹配一行字符串的开头</td></tr><tr><td style="text-align:left">$</td><td style="text-align:left">匹配一行字符串的结尾</td></tr><tr><td style="text-align:left">.</td><td style="text-align:left">匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符</td></tr><tr><td style="text-align:left">[…]</td><td style="text-align:left">用来表示一组字符，单独列出，比如[amk]匹配a、m或k</td></tr><tr><td style="text-align:left">[^…]</td><td style="text-align:left">不在[]中的字符，比如[^abc]匹配除了a、b、c之外的字符</td></tr><tr><td style="text-align:left">*</td><td style="text-align:left">匹配0个或多个表达式</td></tr><tr><td style="text-align:left">+</td><td style="text-align:left">匹配1个或多个表达式</td></tr><tr><td style="text-align:left">?</td><td style="text-align:left">匹配0个或1个前面的正则表达式定义的片段，非贪婪方式</td></tr><tr><td style="text-align:left">{n}</td><td style="text-align:left">精确匹配n个前面的表达式</td></tr><tr><td style="text-align:left">{n, m}</td><td style="text-align:left">匹配n到m次由前面正则表达式定义的片段，贪婪方式</td></tr><tr><td style="text-align:left">a&#124;b</td><td style="text-align:left">匹配a或b</td></tr><tr><td style="text-align:left">( )</td><td style="text-align:left">匹配括号内的表达式，也表示一个组</td></tr></tbody></table><h3 id="4、常用的修饰符-匹配模式"><a href="#4、常用的修饰符-匹配模式" class="headerlink" title="4、常用的修饰符(匹配模式)"></a>4、常用的修饰符(匹配模式)</h3><table><thead><tr><th style="text-align:left">修饰符</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">re.I</td><td style="text-align:left">使匹配对大小写不敏感</td></tr><tr><td style="text-align:left">re.L</td><td style="text-align:left">做本地化识别（locale-aware）匹配</td></tr><tr><td style="text-align:left">re.M</td><td style="text-align:left">多行匹配，影响^和$</td></tr><tr><td style="text-align:left">re.S</td><td style="text-align:left">使.匹配包括换行在内的所有字符</td></tr><tr><td style="text-align:left">re.U</td><td style="text-align:left">根据Unicode字符集解析字符。这个标志影响\w、\W、 \b和\B</td></tr><tr><td style="text-align:left">re.X</td><td style="text-align:left">该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解</td></tr></tbody></table><h3 id="5、re-match"><a href="#5、re-match" class="headerlink" title="5、re.match"></a>5、re.match</h3><p>尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回None</p><p><code>re.match(pattern,string,flags=0)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#常规匹配</span></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 123 4567 World_This is a Regex Demo'</span>  </span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(<span class="string">'^Hello\s\d\d\d\s\d&#123;4&#125;.*Demo$'</span>,content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#泛匹配</span></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 123 4567 World_This is a Regex Demo'</span>  </span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(<span class="string">'^Hello.*Demo$'</span>,content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#匹配目标</span></span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">print(len(content))</span><br><span class="line"><span class="comment">#可以使用()括号将想提取的子字符串括起来。()实际上标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，调用group()方法传入分组的索引即可获取提取的结果。</span></span><br><span class="line">result = re.match(<span class="string">'^Hello\s(\d+)\sWorld.*Demo$'</span>,content) </span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.group(<span class="number">1</span>)) <span class="comment"># 将第一括号括起来的内容打印出来，依次可推group(2)</span></span><br><span class="line">print(result.span())</span><br><span class="line">print(result.groups())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#贪婪匹配</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(<span class="string">'^Hel.*(\d+).*Demo$'</span>,content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#在贪婪匹配下，.*会匹配尽可能多的字符。正则表达式中.*后面\d+至少一个数字，但并没有指定具体多少个数字，因此，.*就尽可能匹配多的字符，这里就把123456匹配了，给\d+留下一个可满足条件的数字7。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#非贪婪匹配</span></span><br><span class="line">content = <span class="string">'Hello 1234567 World_This is a Regex Demo'</span></span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(<span class="string">'Hel.*?(\d+).*Demo$'</span>,content) <span class="comment">#非贪婪匹配的写法是.*?</span></span><br><span class="line">print(result)</span><br><span class="line">print(result.group(<span class="number">1</span>))</span><br><span class="line"><span class="comment">#非贪婪匹配就是尽可能匹配少的字符。当.*?匹配到Hello后面的空白字符时，再往后的字符就是数字了，而\d+恰好可以匹配，那么这里.*?就不再进行匹配，交给\d+去匹配后面的数字。</span></span><br><span class="line"><span class="comment">#这里需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。需要用贪婪匹配.*</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#匹配模式(修饰符)</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'''Hello 1234567 World_This  </span></span><br><span class="line"><span class="string">is a Regex Demo'''</span>  </span><br><span class="line">result = re.match(<span class="string">'He.*?(\d+).*?Demo$'</span>,content) <span class="comment">#.匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了</span></span><br><span class="line">print(result)</span><br><span class="line">result1 = re.match(<span class="string">'He.*?(\d+).*?Demo$'</span>,content,re.S) <span class="comment"># 只需加一个修饰符re.S,使.匹配包括换行在内的所有字符</span></span><br><span class="line">print(result1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转义匹配</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">content = <span class="string">'prince is $5.00'</span></span><br><span class="line">result = re.match(<span class="string">'prince is $5.00'</span>,content)</span><br><span class="line">print(result)</span><br><span class="line">result1 = re.match(<span class="string">'prince is \$5\.00'</span>,content) <span class="comment">#遇到用于正则匹配模式的特殊字符时，在前面加反斜线转义一下</span></span><br><span class="line">print(result1)</span><br></pre></td></tr></table></figure><p>总结：尽量使用泛匹配，使用括号得到匹配目标，尽量使用非贪婪模式、有换行符就用re.S</p><h3 id="6、re-search"><a href="#6、re-search" class="headerlink" title="6、re.search"></a>6、re.search</h3><p>match()方法是从字符串的开头开始匹配的，一旦开头不匹配，那么整个匹配就失败了.</p><p>这里有另外一个方法search()会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，就返回None。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">content = <span class="string">'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'</span></span><br><span class="line">result = re.match(<span class="string">'Hello.*?(\d+).*?Demo'</span>, content)</span><br><span class="line">print(result)</span><br><span class="line">result1 = re.search(<span class="string">'Hello.*?(\d+).*?Demo'</span>, content)</span><br><span class="line">print(result1)</span><br></pre></td></tr></table></figure><p>总结：为了匹配方便，能用search()就不用match()。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匹配练习</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''&lt;div id="songs-list"&gt;</span></span><br><span class="line"><span class="string">    &lt;h2 class="title"&gt;经典老歌&lt;/h2&gt;</span></span><br><span class="line"><span class="string">    &lt;p class="introduction"&gt;</span></span><br><span class="line"><span class="string">        经典老歌列表</span></span><br><span class="line"><span class="string">    &lt;/p&gt;</span></span><br><span class="line"><span class="string">    &lt;ul id="list" class="list-group"&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="2"&gt;一路上有你&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="7"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/2.mp3" singer="任贤齐"&gt;沧海一声笑&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="4" class="active"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/3.mp3" singer="齐秦"&gt;往事随风&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="6"&gt;&lt;a href="/4.mp3" singer="beyond"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="5"&gt;&lt;a href="/5.mp3" singer="陈慧琳"&gt;记事本&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="5"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/6.mp3" singer="邓丽君"&gt;&lt;i class="fa fa-user"&gt;&lt;/i&gt;但愿人长久&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">    &lt;/ul&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;'''</span></span><br><span class="line"></span><br><span class="line">result = re.search(<span class="string">'&lt;li.*?active.*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>,html,re.S)</span><br><span class="line">print(result.groups())</span><br><span class="line">print(result.group(<span class="number">1</span>),result.group(<span class="number">2</span>))</span><br></pre></td></tr></table></figure><h3 id="7、re-findall"><a href="#7、re-findall" class="headerlink" title="7、re.findall"></a>7、re.findall</h3><p>搜索字符，以列表的形式返回全部匹配的字符串。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">'''&lt;div id="songs-list"&gt;</span></span><br><span class="line"><span class="string">    &lt;h2 class="title"&gt;经典老歌&lt;/h2&gt;</span></span><br><span class="line"><span class="string">    &lt;p class="introduction"&gt;</span></span><br><span class="line"><span class="string">        经典老歌列表</span></span><br><span class="line"><span class="string">    &lt;/p&gt;</span></span><br><span class="line"><span class="string">    &lt;ul id="list" class="list-group"&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="2"&gt;一路上有你&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="7"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/2.mp3" singer="任贤齐"&gt;沧海一声笑&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="4" class="active"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/3.mp3" singer="齐秦"&gt;往事随风&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="6"&gt;&lt;a href="/4.mp3" singer="beyond"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="5"&gt;&lt;a href="/5.mp3" singer="陈慧琳"&gt;记事本&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">        &lt;li data-view="5"&gt;</span></span><br><span class="line"><span class="string">            &lt;a href="/6.mp3" singer="邓丽君"&gt;但愿人长久&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/li&gt;</span></span><br><span class="line"><span class="string">    &lt;/ul&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;'''</span></span><br><span class="line"></span><br><span class="line">results = re.findall(<span class="string">'&lt;li.*?href="/(.*?)".*?singer="(.*?)"&gt;(.*?)&lt;/a&gt;'</span>,html,re.S)</span><br><span class="line">print(results)</span><br><span class="line">print(type(results))</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result[<span class="number">0</span>],result[<span class="number">1</span>],result[<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">results1 = re.findall(<span class="string">'&lt;li.*?&gt;\s*?(&lt;/a.*?&gt;)?(\w+)(&lt;/a&gt;)?\s*?&lt;/li&gt;'</span>,html,re.S)</span><br><span class="line">print(results1)</span><br><span class="line"><span class="keyword">for</span> result1 <span class="keyword">in</span> results1:</span><br><span class="line">    print(result1[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="8、re-sub"><a href="#8、re-sub" class="headerlink" title="8、re.sub"></a>8、re.sub</h3><p>替换字符串中每一个匹配的字符串后返回替换后的字符串。</p><p><code>re.sub(正则表达式，要替换成的字符串，原字符串)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = <span class="string">'Extra stings Hello 1234567 World_This is a Regex Demo Extra stings'</span></span><br><span class="line"></span><br><span class="line">result = re.sub(<span class="string">'\d+'</span>,<span class="string">''</span>,content) <span class="comment">#去掉数字</span></span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">result1 = re.sub(<span class="string">'\d+'</span>,<span class="string">'Replacement'</span>,content) <span class="comment">#数字替换为字符</span></span><br><span class="line">print(result1)</span><br><span class="line"></span><br><span class="line">result2 = re.sub(<span class="string">'(\d+)'</span>,<span class="string">r'\1 8910'</span>,content) <span class="comment">#如果要替换的字符串包含自己本身用\1表示，正则表达式需要用()</span></span><br><span class="line">print(result2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">html = <span class="string">'''''&lt;div id="songs-list"&gt; </span></span><br><span class="line"><span class="string">&lt;h2 class="title"&gt;经典老歌&lt;/h2&gt; </span></span><br><span class="line"><span class="string">&lt;p class="introduction"&gt; </span></span><br><span class="line"><span class="string">    经典老歌列表 </span></span><br><span class="line"><span class="string">&lt;/p&gt; </span></span><br><span class="line"><span class="string">&lt;ul id="list"class="list-group"&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="2"&gt;一路上有你&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="7"&gt; </span></span><br><span class="line"><span class="string">        &lt;a href="/2.mp3"singer="任贤齐"&gt;沧海一声笑&lt;/a&gt; </span></span><br><span class="line"><span class="string">    &lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="4"class="active"&gt; </span></span><br><span class="line"><span class="string">        &lt;a href="/3.mp3"singer="齐秦"&gt;往事随风&lt;/a&gt; </span></span><br><span class="line"><span class="string">    &lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="6"&gt;&lt;a href="/4.mp3"singer="begoud"&gt;光辉岁月&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="5"&gt;&lt;a href="/5.mp3"singer="陈慧琳"&gt;记事本&lt;/a&gt;&lt;/li&gt; </span></span><br><span class="line"><span class="string">    &lt;li data-view="5"&gt; </span></span><br><span class="line"><span class="string">        &lt;a href="/6.mp3"singer="邓丽君"&gt;但愿人长久&lt;/a&gt; </span></span><br><span class="line"><span class="string">    &lt;/li&gt; </span></span><br><span class="line"><span class="string">&lt;/ul&gt; </span></span><br><span class="line"><span class="string">&lt;/div&gt;'''</span>  </span><br><span class="line"></span><br><span class="line">html = re.sub(<span class="string">'&lt;a.*?&gt;|&lt;/a&gt;'</span>,<span class="string">''</span>,html) <span class="comment">#首先去掉a标签</span></span><br><span class="line">print(html)</span><br><span class="line">results = re.findall(<span class="string">'&lt;li.*?&gt;(.*?)&lt;/li&gt;'</span>,html,re.S) <span class="comment">#然后再查找所有歌名</span></span><br><span class="line">print(results)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result.strip())</span><br></pre></td></tr></table></figure><h3 id="9、re-compile"><a href="#9、re-compile" class="headerlink" title="9、re.compile"></a>9、re.compile</h3><p>将正则字符串编译成正则表达式对象，以便在后面的匹配中复用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = <span class="string">'''Hello 1234545 World_This  </span></span><br><span class="line"><span class="string">is a Regex Demo'''</span>  </span><br><span class="line"></span><br><span class="line">pattern = re.compile(<span class="string">'Hello.*Demo'</span>,re.S) <span class="comment">#编译正则表达式对象</span></span><br><span class="line">result = re.match(pattern,content) <span class="comment">#正则对象复用</span></span><br><span class="line">print(result)</span><br><span class="line">result1 = re.match(<span class="string">'Hello.*Demo'</span>,content,re.S)</span><br><span class="line">print(result1)</span><br></pre></td></tr></table></figure><h3 id="10、实战练习"><a href="#10、实战练习" class="headerlink" title="10、实战练习"></a>10、实战练习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"></span><br><span class="line">content = requests.get(<span class="string">'https://book.douban.com/'</span>).text <span class="comment">#获取网页源代码</span></span><br><span class="line"><span class="comment">#print(content)</span></span><br><span class="line">pattern = re.compile(<span class="string">'&lt;li.*?"cover"&gt;.*?href="(.*?)" title="(.*?)".*?"more-meta".*?"author"&gt;(.*?)&lt;/span&gt;.*?"year"&gt;(.*?)&lt;/span&gt;.*?&lt;/li&gt;'</span>,re.S)</span><br><span class="line">results = re.findall(pattern,content)</span><br><span class="line">print(results)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    url,name,author,time = result</span><br><span class="line">    author = re.sub(<span class="string">'\s'</span>,<span class="string">''</span>,author)</span><br><span class="line">    time = re.sub(<span class="string">'\s'</span>,<span class="string">''</span>,time)</span><br><span class="line">    print(url,name,author,time)</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫-requests库</title>
    <link href="http://pythonfood.github.io/2018/07/02/%E7%88%AC%E8%99%AB-requests%E5%BA%93/"/>
    <id>http://pythonfood.github.io/2018/07/02/爬虫-requests库/</id>
    <published>2018-07-02T03:00:00.000Z</published>
    <updated>2018-08-14T09:14:32.334Z</updated>
    
    <content type="html"><![CDATA[<p>崔庆才老师爬虫的学习笔记。<br><a id="more"></a></p><h2 id="一、Requests库详解"><a href="#一、Requests库详解" class="headerlink" title="一、Requests库详解"></a>一、Requests库详解</h2><h3 id="1、什么是Requests库"><a href="#1、什么是Requests库" class="headerlink" title="1、什么是Requests库"></a>1、什么是Requests库</h3><p>Requests库是用Python编写的，基于urllib，采用Apache2 Licensed开源协议的HTTP库。</p><p>相比urllib库，Requests库更加方便，可以节约我们大量的工作，完全满足HTTP测试需求。</p><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><p><code>pip install request</code></p><h3 id="3、Requests库用法详解"><a href="#3、Requests库用法详解" class="headerlink" title="3、Requests库用法详解"></a>3、Requests库用法详解</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(type(response))</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(type(response.text))</span><br><span class="line">print(response.text)</span><br><span class="line">print(response.cookies)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 各种请求（HTTP测试网站：http://httpbin.org/）</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">requests.post(<span class="string">'http://httpbin.org/post'</span>)</span><br><span class="line">requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</span><br><span class="line">requests.put(<span class="string">'http://httpbin.org/put'</span>)</span><br><span class="line">requests.head(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">requests.options(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本get请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 带参数get请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data=&#123;</span><br><span class="line">    <span class="string">'name'</span>:<span class="string">'asr'</span>,</span><br><span class="line">    <span class="string">'age'</span>:<span class="string">'12'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">'http://httpbin.org/get'</span>,params=data)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解析json</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">print(response.text)</span><br><span class="line">print(type(response.text))</span><br><span class="line">print(response.json())</span><br><span class="line">print(type(response.json()))</span><br><span class="line">print(json.loads(response.text))</span><br><span class="line">print(type(json.loads(response.text)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取二进制数据</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'https://github.com/favicon.ico'</span>)</span><br><span class="line">print(response.text)</span><br><span class="line">print(type(response.text))</span><br><span class="line">print(response.content) <span class="comment"># 获取二进制数据用response.content</span></span><br><span class="line">print(type(response.content))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(response.content)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加headers</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://www.zhihu.com/explore'</span>)  <span class="comment">#不加headers不能访问</span></span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; rv:60.0) Gecko/20100101 Firefox/60.0'</span>&#125;</span><br><span class="line">response1 = requests.get(<span class="string">'http://www.zhihu.com/explore'</span>,headers=headers)</span><br><span class="line">print(response1.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本post请求</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>:<span class="string">'jk'</span>,</span><br><span class="line">    <span class="string">'age'</span>:<span class="number">18</span></span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; rv:60.0) Gecko/20100101 Firefox/60.0'</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(<span class="string">'http://httpbin.org/post'</span>,data=data,headers=headers)</span><br><span class="line">print(response.json())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># response属性</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://www.douban.com'</span>)</span><br><span class="line">print(type(response.status_code),response.status_code)</span><br><span class="line">print(type(response.headers),response.headers)</span><br><span class="line">print(type(response.cookies),response.cookies)</span><br><span class="line">print(type(response.url),response.url)</span><br><span class="line">print(type(response.history),response.history)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 状态码判断</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response =requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">exit() <span class="keyword">if</span> <span class="keyword">not</span> response.status_code==requests.codes.forbidden <span class="keyword">else</span> print(<span class="string">'403 forbidden'</span>) <span class="comment">#状态码查询对象，可以从下表中查找</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 信息性状态码</span></span><br><span class="line"><span class="number">100</span>: (<span class="string">'continue'</span>,),</span><br><span class="line"><span class="number">101</span>: (<span class="string">'switching_protocols'</span>,),</span><br><span class="line"><span class="number">102</span>: (<span class="string">'processing'</span>,),</span><br><span class="line"><span class="number">103</span>: (<span class="string">'checkpoint'</span>,),</span><br><span class="line"><span class="number">122</span>: (<span class="string">'uri_too_long'</span>, <span class="string">'request_uri_too_long'</span>),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功状态码</span></span><br><span class="line"><span class="number">200</span>: (<span class="string">'ok'</span>, <span class="string">'okay'</span>, <span class="string">'all_ok'</span>, <span class="string">'all_okay'</span>, <span class="string">'all_good'</span>, <span class="string">'\\o/'</span>, <span class="string">'✓'</span>),</span><br><span class="line"><span class="number">201</span>: (<span class="string">'created'</span>,),</span><br><span class="line"><span class="number">202</span>: (<span class="string">'accepted'</span>,),</span><br><span class="line"><span class="number">203</span>: (<span class="string">'non_authoritative_info'</span>, <span class="string">'non_authoritative_information'</span>),</span><br><span class="line"><span class="number">204</span>: (<span class="string">'no_content'</span>,),</span><br><span class="line"><span class="number">205</span>: (<span class="string">'reset_content'</span>, <span class="string">'reset'</span>),</span><br><span class="line"><span class="number">206</span>: (<span class="string">'partial_content'</span>, <span class="string">'partial'</span>),</span><br><span class="line"><span class="number">207</span>: (<span class="string">'multi_status'</span>, <span class="string">'multiple_status'</span>, <span class="string">'multi_stati'</span>, <span class="string">'multiple_stati'</span>),</span><br><span class="line"><span class="number">208</span>: (<span class="string">'already_reported'</span>,),</span><br><span class="line"><span class="number">226</span>: (<span class="string">'im_used'</span>,),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重定向状态码</span></span><br><span class="line"><span class="number">300</span>: (<span class="string">'multiple_choices'</span>,),</span><br><span class="line"><span class="number">301</span>: (<span class="string">'moved_permanently'</span>, <span class="string">'moved'</span>, <span class="string">'\\o-'</span>),</span><br><span class="line"><span class="number">302</span>: (<span class="string">'found'</span>,),</span><br><span class="line"><span class="number">303</span>: (<span class="string">'see_other'</span>, <span class="string">'other'</span>),</span><br><span class="line"><span class="number">304</span>: (<span class="string">'not_modified'</span>,),</span><br><span class="line"><span class="number">305</span>: (<span class="string">'use_proxy'</span>,),</span><br><span class="line"><span class="number">306</span>: (<span class="string">'switch_proxy'</span>,),</span><br><span class="line"><span class="number">307</span>: (<span class="string">'temporary_redirect'</span>, <span class="string">'temporary_moved'</span>, <span class="string">'temporary'</span>),</span><br><span class="line"><span class="number">308</span>: (<span class="string">'permanent_redirect'</span>,</span><br><span class="line">      <span class="string">'resume_incomplete'</span>, <span class="string">'resume'</span>,), <span class="comment"># These 2 to be removed in 3.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 客户端错误状态码</span></span><br><span class="line"><span class="number">400</span>: (<span class="string">'bad_request'</span>, <span class="string">'bad'</span>),</span><br><span class="line"><span class="number">401</span>: (<span class="string">'unauthorized'</span>,),</span><br><span class="line"><span class="number">402</span>: (<span class="string">'payment_required'</span>, <span class="string">'payment'</span>),</span><br><span class="line"><span class="number">403</span>: (<span class="string">'forbidden'</span>,),</span><br><span class="line"><span class="number">404</span>: (<span class="string">'not_found'</span>, <span class="string">'-o-'</span>),</span><br><span class="line"><span class="number">405</span>: (<span class="string">'method_not_allowed'</span>, <span class="string">'not_allowed'</span>),</span><br><span class="line"><span class="number">406</span>: (<span class="string">'not_acceptable'</span>,),</span><br><span class="line"><span class="number">407</span>: (<span class="string">'proxy_authentication_required'</span>, <span class="string">'proxy_auth'</span>, <span class="string">'proxy_authentication'</span>),</span><br><span class="line"><span class="number">408</span>: (<span class="string">'request_timeout'</span>, <span class="string">'timeout'</span>),</span><br><span class="line"><span class="number">409</span>: (<span class="string">'conflict'</span>,),</span><br><span class="line"><span class="number">410</span>: (<span class="string">'gone'</span>,),</span><br><span class="line"><span class="number">411</span>: (<span class="string">'length_required'</span>,),</span><br><span class="line"><span class="number">412</span>: (<span class="string">'precondition_failed'</span>, <span class="string">'precondition'</span>),</span><br><span class="line"><span class="number">413</span>: (<span class="string">'request_entity_too_large'</span>,),</span><br><span class="line"><span class="number">414</span>: (<span class="string">'request_uri_too_large'</span>,),</span><br><span class="line"><span class="number">415</span>: (<span class="string">'unsupported_media_type'</span>, <span class="string">'unsupported_media'</span>, <span class="string">'media_type'</span>),</span><br><span class="line"><span class="number">416</span>: (<span class="string">'requested_range_not_satisfiable'</span>, <span class="string">'requested_range'</span>, <span class="string">'range_not_satisfiable'</span>),</span><br><span class="line"><span class="number">417</span>: (<span class="string">'expectation_failed'</span>,),</span><br><span class="line"><span class="number">418</span>: (<span class="string">'im_a_teapot'</span>, <span class="string">'teapot'</span>, <span class="string">'i_am_a_teapot'</span>),</span><br><span class="line"><span class="number">421</span>: (<span class="string">'misdirected_request'</span>,),</span><br><span class="line"><span class="number">422</span>: (<span class="string">'unprocessable_entity'</span>, <span class="string">'unprocessable'</span>),</span><br><span class="line"><span class="number">423</span>: (<span class="string">'locked'</span>,),</span><br><span class="line"><span class="number">424</span>: (<span class="string">'failed_dependency'</span>, <span class="string">'dependency'</span>),</span><br><span class="line"><span class="number">425</span>: (<span class="string">'unordered_collection'</span>, <span class="string">'unordered'</span>),</span><br><span class="line"><span class="number">426</span>: (<span class="string">'upgrade_required'</span>, <span class="string">'upgrade'</span>),</span><br><span class="line"><span class="number">428</span>: (<span class="string">'precondition_required'</span>, <span class="string">'precondition'</span>),</span><br><span class="line"><span class="number">429</span>: (<span class="string">'too_many_requests'</span>, <span class="string">'too_many'</span>),</span><br><span class="line"><span class="number">431</span>: (<span class="string">'header_fields_too_large'</span>, <span class="string">'fields_too_large'</span>),</span><br><span class="line"><span class="number">444</span>: (<span class="string">'no_response'</span>, <span class="string">'none'</span>),</span><br><span class="line"><span class="number">449</span>: (<span class="string">'retry_with'</span>, <span class="string">'retry'</span>),</span><br><span class="line"><span class="number">450</span>: (<span class="string">'blocked_by_windows_parental_controls'</span>, <span class="string">'parental_controls'</span>),</span><br><span class="line"><span class="number">451</span>: (<span class="string">'unavailable_for_legal_reasons'</span>, <span class="string">'legal_reasons'</span>),</span><br><span class="line"><span class="number">499</span>: (<span class="string">'client_closed_request'</span>,),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务端错误状态码</span></span><br><span class="line"><span class="number">500</span>: (<span class="string">'internal_server_error'</span>, <span class="string">'server_error'</span>, <span class="string">'/o\\'</span>, <span class="string">'✗'</span>),</span><br><span class="line"><span class="number">501</span>: (<span class="string">'not_implemented'</span>,),</span><br><span class="line"><span class="number">502</span>: (<span class="string">'bad_gateway'</span>,),</span><br><span class="line"><span class="number">503</span>: (<span class="string">'service_unavailable'</span>, <span class="string">'unavailable'</span>),</span><br><span class="line"><span class="number">504</span>: (<span class="string">'gateway_timeout'</span>,),</span><br><span class="line"><span class="number">505</span>: (<span class="string">'http_version_not_supported'</span>, <span class="string">'http_version'</span>),</span><br><span class="line"><span class="number">506</span>: (<span class="string">'variant_also_negotiates'</span>,),</span><br><span class="line"><span class="number">507</span>: (<span class="string">'insufficient_storage'</span>,),</span><br><span class="line"><span class="number">509</span>: (<span class="string">'bandwidth_limit_exceeded'</span>, <span class="string">'bandwidth'</span>),</span><br><span class="line"><span class="number">510</span>: (<span class="string">'not_extended'</span>,),</span><br><span class="line"><span class="number">511</span>: (<span class="string">'network_authentication_required'</span>, <span class="string">'network_auth'</span>, <span class="string">'network_authentication'</span>)</span><br></pre></td></tr></table></figure><h3 id="4、Requests高级操作"><a href="#4、Requests高级操作" class="headerlink" title="4、Requests高级操作"></a>4、Requests高级操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件上传</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;</span><br><span class="line">    <span class="string">'files'</span>:open(<span class="string">'favicon.ico'</span>,<span class="string">'rb'</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(<span class="string">'http://httpbin.org/post'</span>,files=files)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取cookies</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">response = requests.get(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(response.cookies)</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> response.cookies.items():</span><br><span class="line">    print(k +<span class="string">'='</span>+ v)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 会话维持</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>) <span class="comment">#为网站的访问设置cookie </span></span><br><span class="line">response = requests.get(<span class="string">'http://httpbin.org/cookies'</span>)  <span class="comment">#与上面的行为时独立的，所以获取不到任何与cookie相关的信息</span></span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line">s = requests.Session() <span class="comment">#声明Session对象，使用这个对象发起两次GET请求（相当于同一个浏览器发出来的请求）</span></span><br><span class="line">s.get(<span class="string">'http://httpbin.org/cookies/set/number/123456789'</span>)</span><br><span class="line">r = s.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 证书验证</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#直接请求https会报错SLLError</span></span><br><span class="line"><span class="comment">#response = requests.get('https://www.12306.cn')</span></span><br><span class="line"><span class="comment">#print(response.status_code)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加verify=False可以忽略证书验证，但是还是会报警告</span></span><br><span class="line"><span class="comment">#response = requests.get('https://www.12306.cn',verify=False)</span></span><br><span class="line"><span class="comment">#print(response.text)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.引入requests.packages.urllib3设置忽略警告</span></span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>,verify=<span class="keyword">False</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.通过捕获警告到日志的方式忽略警告</span></span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.captureWarnings(<span class="keyword">True</span>)</span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>,verify=<span class="keyword">False</span>)</span><br><span class="line">print(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组</span></span><br><span class="line">response = requests.get(<span class="string">'https://www.12306.cn'</span>,cert=(<span class="string">'/path/server.crt'</span>, <span class="string">'/path/key'</span>))</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 代理设置</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://10.10.1.10:3128"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"http://10.10.1.10:1080"</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若代理需要使用HTTP Basic Auth，可以使用类似http://user:password@host:port这样的语法来设置代理。</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">"http"</span>: <span class="string">"http://user:password@10.10.1.10:3128/"</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除了基本的HTTP代理外，requests还支持SOCKS协议的代理。pip3 install 'requests[socks]'</span></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://user:password@host:port'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://user:password@host:port'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">"https://www.taobao.com"</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超时设置 </span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>,timeout=<span class="number">1</span>)</span><br><span class="line">print(r.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别指定连接（connect）和读取（read）两个阶段超时，可以传入一个元组</span></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>,timeout=(<span class="number">5.11</span>,<span class="number">30</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 想永久等待，可以直接将timeout设置为None,或直接不加参数</span></span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>,timeout=<span class="keyword">None</span>)</span><br><span class="line">r = requests.get(<span class="string">'https://www.taobao.com'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 身份认证</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">r = requests.get(<span class="string">'http://120.27.34.24.9001'</span>,auth=(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br><span class="line">print(r.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment">#上面代码是简写，实际调用的requests.auth.HTTPBasicAuth </span></span><br><span class="line"><span class="keyword">from</span> requests.auth <span class="keyword">import</span> HTTPBasicAuth</span><br><span class="line">r=requests.get(<span class="string">'http://120.27.34.24.9001'</span>,auth=HTTPBasicAuth(<span class="string">'username'</span>, <span class="string">'password'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 异常处理</span></span><br><span class="line"><span class="comment"># http://docs.python-requests.org/en/master/api/#exceptions</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> ReadTimeout,HTTPError,RequestException</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    r = requests.get(<span class="string">'https://www.taobao.com'</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> ReadTimeout:</span><br><span class="line">    print(<span class="string">'ReadTimeout'</span>)</span><br><span class="line"><span class="keyword">except</span> HTTPError:</span><br><span class="line">    print(<span class="string">'HTTPError'</span>)</span><br><span class="line"><span class="keyword">except</span> RequestException:</span><br><span class="line">    print(<span class="string">'RequestException'</span>)</span><br></pre></td></tr></table></figure><h1 id="持续更新…"><a href="#持续更新…" class="headerlink" title="持续更新…"></a><em>持续更新…</em></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;崔庆才老师爬虫的学习笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://pythonfood.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="http://pythonfood.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
</feed>
